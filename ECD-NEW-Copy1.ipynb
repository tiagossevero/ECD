{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aea681-2477-45f5-9ffe-1cb8f6cb1a68",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "#Import libs python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "#Import libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4faf8a1f-48e4-4c57-a162-c839c8e3a940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Generates DBASparkAppSession.\"\"\"\n",
    "    \n",
    "    app_name = \"tsevero_ecd_new\"\n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00777b24-69dc-4a6e-ba6f-4e11cd534937",
   "metadata": {},
   "outputs": [],
   "source": [
    "session.sparkSession.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5dd6bb-3844-436b-9810-cce86e02a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# CONFIGURA√á√ÉO INICIAL - \n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PySpark imports com aliases para evitar conflitos\n",
    "from pyspark.sql.functions import (\n",
    "    col as spark_col, \n",
    "    sum as spark_sum, \n",
    "    avg as spark_avg,\n",
    "    count as spark_count,\n",
    "    when as spark_when,\n",
    "    desc as spark_desc,\n",
    "    asc as spark_asc,\n",
    "    round as spark_round,\n",
    "    concat as spark_concat,\n",
    "    lit as spark_lit,\n",
    "    max as spark_max,\n",
    "    min as spark_min,\n",
    "    stddev as spark_stddev,\n",
    "    countDistinct as spark_countDistinct\n",
    ")\n",
    "from pyspark.sql.types import DoubleType, IntegerType\n",
    "\n",
    "# Configura√ß√µes de visualiza√ß√£o\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (16, 8)\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "# ‚úÖ CORRE√á√ÉO: N√£o usar abs() que conflita com PySpark\n",
    "# pd.set_option('display.float_format', lambda x: f'{x:,.2f}' if abs(x) > 0.01 else f'{x:.6f}')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.precision', 2)\n",
    "\n",
    "# Acesso ao Spark\n",
    "spark = session.sparkSession\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"üîç SISTEMA\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Sess√£o Spark: {spark.sparkContext.appName}\")\n",
    "print(f\"Vers√£o Spark: {spark.version}\")\n",
    "print(f\"Iniciado em: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb6e6ba7-3aec-49df-843f-adaa1e2a4510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adicione estas linhas no in√≠cio da C√âLULA 1 (depois dos imports)\n",
    "import logging\n",
    "\n",
    "# Reduzir logs do Spark\n",
    "logging.getLogger(\"org.apache.spark\").setLevel(logging.ERROR)\n",
    "logging.getLogger(\"org.spark_project\").setLevel(logging.ERROR)\n",
    "\n",
    "# Configura√ß√£o do Spark para lidar melhor com c√≥digo complexo\n",
    "spark.conf.set(\"spark.sql.codegen.wholeStage\", \"false\")  # Desabilita codegen complexo\n",
    "spark.conf.set(\"spark.sql.codegen.maxFields\", \"500\")     # Aumenta limite de campos\n",
    "\n",
    "# ================================================================================\n",
    "# CLASSIFICA√á√ÉO DE CONTAS CONT√ÅBEIS - SCRIPT COMPLETO\n",
    "# C√âLULA 1: PAR√ÇMETROS E PREPARA√á√ÉO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üîß CONFIGURANDO PAR√ÇMETROS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ================================================================================\n",
    "# PAR√ÇMETROS CONFIGUR√ÅVEIS\n",
    "# ================================================================================\n",
    "\n",
    "# Ano de refer√™ncia\n",
    "ANO_REFERENCIA = 2024\n",
    "print(f\"üìÖ Ano de refer√™ncia: {ANO_REFERENCIA}\")\n",
    "\n",
    "# UF para filtro\n",
    "UF_FILTRO = 'SC'\n",
    "print(f\"üìç UF filtrada: {UF_FILTRO}\")\n",
    "\n",
    "# Toler√¢ncia para equa√ß√£o cont√°bil (%)\n",
    "TOLERANCIA_EQUACAO = 10.0  # 10%\n",
    "print(f\"‚öñÔ∏è  Toler√¢ncia equa√ß√£o cont√°bil: {TOLERANCIA_EQUACAO}%\")\n",
    "\n",
    "# Database de destino\n",
    "DATABASE_DESTINO = 'neac'\n",
    "print(f\"üíæ Database destino: {DATABASE_DESTINO}\")\n",
    "\n",
    "# Tabelas de refer√™ncia\n",
    "TABELA_REF_BP = f'{DATABASE_DESTINO}.ecd_pc_bp'\n",
    "TABELA_REF_DRE = f'{DATABASE_DESTINO}.ecd_pc_dre'\n",
    "print(f\"üìö Refer√™ncia BP: {TABELA_REF_BP}\")\n",
    "print(f\"üìö Refer√™ncia DRE: {TABELA_REF_DRE}\")\n",
    "\n",
    "# Tabela de sa√≠da\n",
    "TABELA_SAIDA = f'{DATABASE_DESTINO}.ecd_contas_classificadas'\n",
    "print(f\"üìä Tabela sa√≠da: {TABELA_SAIDA}\")\n",
    "\n",
    "print(\"\\n‚úÖ Par√¢metros configurados!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9229831-07b6-4238-926a-2c0abaf2b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 2: CRIAR TABELA UNIFICADA DE REFER√äNCIA (BP + DRE)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìö CRIANDO TABELA UNIFICADA DE REFER√äNCIA...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ================================================================================\n",
    "# DROPAR TABELA ANTIGA SE EXISTIR\n",
    "# ================================================================================\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.pc_referencia_completa PURGE\")\n",
    "    print(\"‚úÖ Tabela antiga removida\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Tabela n√£o existia (OK)\")\n",
    "\n",
    "# ================================================================================\n",
    "# CRIAR TABELA UNIFICADA: BP + DRE COM HIERARQUIA\n",
    "# ================================================================================\n",
    "\n",
    "sql_criar_referencia = f\"\"\"\n",
    "CREATE TABLE {DATABASE_DESTINO}.pc_referencia_completa AS\n",
    "WITH ref_bp AS (\n",
    "    SELECT \n",
    "        codigo AS cd_conta_ref,\n",
    "        descricao AS descr_conta_ref,\n",
    "        tipo AS tp_conta,  -- 'A' ou 'S'\n",
    "        conta_superior AS cd_conta_sup_ref,\n",
    "        nivel AS nivel_ref,\n",
    "        natureza AS cd_natureza_ref,  -- '1', '2', '3'\n",
    "        'BP' AS origem_ref,\n",
    "        \n",
    "        -- Classifica√ß√£o baseada no c√≥digo\n",
    "        CASE \n",
    "            WHEN codigo LIKE '1.01%' THEN 'ATIVO_CIRCULANTE'\n",
    "            WHEN codigo LIKE '1.02%' THEN 'ATIVO_NAO_CIRCULANTE'\n",
    "            WHEN codigo LIKE '1%' THEN 'ATIVO'\n",
    "            WHEN codigo LIKE '2.01%' THEN 'PASSIVO_CIRCULANTE'\n",
    "            WHEN codigo LIKE '2.02%' THEN 'PASSIVO_NAO_CIRCULANTE'\n",
    "            WHEN codigo LIKE '2.03%' THEN 'PATRIMONIO_LIQUIDO'\n",
    "            WHEN codigo LIKE '2%' THEN 'PASSIVO'\n",
    "            ELSE 'NAO_CLASSIFICADO'\n",
    "        END AS classificacao_nivel2,\n",
    "        \n",
    "        CASE \n",
    "            WHEN codigo LIKE '1.01.01%' THEN 'DISPONIBILIDADES'\n",
    "            WHEN codigo LIKE '1.01.02%' THEN 'CREDITOS'\n",
    "            WHEN codigo LIKE '1.01.03%' THEN 'ESTOQUES'\n",
    "            WHEN codigo LIKE '1.02.01%' THEN 'ATIVO_REALIZAVEL_LP'\n",
    "            WHEN codigo LIKE '1.02.02%' THEN 'INVESTIMENTOS'\n",
    "            WHEN codigo LIKE '1.02.03%' THEN 'IMOBILIZADO'\n",
    "            WHEN codigo LIKE '1.02.04%' THEN 'INTANGIVEL'\n",
    "            WHEN codigo LIKE '2.01.01%' THEN 'FORNECEDORES'\n",
    "            WHEN codigo LIKE '2.01.02%' THEN 'EMPRESTIMOS_CP'\n",
    "            WHEN codigo LIKE '2.01.03%' THEN 'TRIBUTOS_A_PAGAR'\n",
    "            WHEN codigo LIKE '2.02.01%' THEN 'EMPRESTIMOS_LP'\n",
    "            WHEN codigo LIKE '2.03.01%' THEN 'CAPITAL_SOCIAL'\n",
    "            WHEN codigo LIKE '2.03.02%' THEN 'RESERVAS'\n",
    "            WHEN codigo LIKE '2.03.03%' THEN 'LUCROS_PREJUIZOS_ACUMULADOS'\n",
    "            ELSE NULL\n",
    "        END AS classificacao_nivel3\n",
    "        \n",
    "    FROM {TABELA_REF_BP}\n",
    "    WHERE codigo IS NOT NULL\n",
    "),\n",
    "\n",
    "ref_dre AS (\n",
    "    SELECT \n",
    "        codigo AS cd_conta_ref,\n",
    "        descricao AS descr_conta_ref,\n",
    "        tipo AS tp_conta,\n",
    "        conta_superior AS cd_conta_sup_ref,\n",
    "        nivel AS nivel_ref,\n",
    "        natureza AS cd_natureza_ref,  -- '4'\n",
    "        'DRE' AS origem_ref,\n",
    "        \n",
    "        -- Classifica√ß√£o baseada no c√≥digo\n",
    "        CASE \n",
    "            WHEN codigo LIKE '3.01%' THEN 'RECEITA_BRUTA'\n",
    "            WHEN codigo LIKE '3.02%' THEN 'DEDUCOES_RECEITA'\n",
    "            WHEN codigo LIKE '3.03%' THEN 'CUSTOS'\n",
    "            WHEN codigo LIKE '3.04%' THEN 'DESPESAS_OPERACIONAIS'\n",
    "            WHEN codigo LIKE '3.05%' THEN 'OUTRAS_RECEITAS_DESPESAS'\n",
    "            WHEN codigo LIKE '3.06%' THEN 'RESULTADO_FINANCEIRO'\n",
    "            WHEN codigo LIKE '3.07%' THEN 'IMPOSTOS_RESULTADO'\n",
    "            WHEN codigo LIKE '3%' THEN 'RESULTADO'\n",
    "            ELSE 'NAO_CLASSIFICADO'\n",
    "        END AS classificacao_nivel2,\n",
    "        \n",
    "        CASE \n",
    "            WHEN codigo LIKE '3.01.01%' THEN 'RECEITA_VENDAS_PRODUTOS'\n",
    "            WHEN codigo LIKE '3.01.02%' THEN 'RECEITA_VENDAS_MERCADORIAS'\n",
    "            WHEN codigo LIKE '3.01.03%' THEN 'RECEITA_PRESTACAO_SERVICOS'\n",
    "            WHEN codigo LIKE '3.02.01%' THEN 'DEVOLUCOES_VENDAS'\n",
    "            WHEN codigo LIKE '3.02.02%' THEN 'IMPOSTOS_SOBRE_VENDAS'\n",
    "            WHEN codigo LIKE '3.03.01%' THEN 'CUSTO_PRODUTOS_VENDIDOS'\n",
    "            WHEN codigo LIKE '3.03.02%' THEN 'CUSTO_MERCADORIAS_VENDIDAS'\n",
    "            WHEN codigo LIKE '3.03.03%' THEN 'CUSTO_SERVICOS_PRESTADOS'\n",
    "            WHEN codigo LIKE '3.04.01%' THEN 'DESPESAS_VENDAS'\n",
    "            WHEN codigo LIKE '3.04.02%' THEN 'DESPESAS_ADMINISTRATIVAS'\n",
    "            WHEN codigo LIKE '3.06.01%' THEN 'RECEITAS_FINANCEIRAS'\n",
    "            WHEN codigo LIKE '3.06.02%' THEN 'DESPESAS_FINANCEIRAS'\n",
    "            ELSE NULL\n",
    "        END AS classificacao_nivel3\n",
    "        \n",
    "    FROM {TABELA_REF_DRE}\n",
    "    WHERE codigo IS NOT NULL\n",
    ")\n",
    "\n",
    "SELECT * FROM ref_bp\n",
    "UNION ALL\n",
    "SELECT * FROM ref_dre\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executando SQL...\")\n",
    "spark.sql(sql_criar_referencia)\n",
    "\n",
    "# Validar cria√ß√£o\n",
    "total_ref = spark.sql(f\"SELECT COUNT(*) as total FROM {DATABASE_DESTINO}.pc_referencia_completa\").collect()[0]['total']\n",
    "print(f\"‚úÖ Tabela criada com {total_ref:,} registros de refer√™ncia\")\n",
    "\n",
    "# Estat√≠sticas\n",
    "print(\"\\nüìä Estat√≠sticas da tabela de refer√™ncia:\")\n",
    "stats_ref = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        origem_ref,\n",
    "        COUNT(*) as qtd,\n",
    "        COUNT(DISTINCT cd_conta_ref) as contas_unicas\n",
    "    FROM {DATABASE_DESTINO}.pc_referencia_completa\n",
    "    GROUP BY origem_ref\n",
    "\"\"\")\n",
    "stats_ref.show()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ TABELA DE REFER√äNCIA CRIADA COM SUCESSO!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76cc9c50-b92e-4fd0-a088-618d4f1a0f13",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 3: EXTRAIR E CLASSIFICAR CONTAS - CAMADA 1 (VALIDA√á√ÉO CRUZADA)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîÑ EXTRAINDO E CLASSIFICANDO CONTAS - CAMADA 1...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ================================================================================\n",
    "# DROPAR TABELA ANTIGA\n",
    "# ================================================================================\n",
    "\n",
    "try:\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {TABELA_SAIDA} PURGE\")\n",
    "    print(\"‚úÖ Tabela antiga removida\")\n",
    "except:\n",
    "    print(\"‚ö†Ô∏è  Tabela n√£o existia (OK)\")\n",
    "\n",
    "# ================================================================================\n",
    "# SQL PRINCIPAL: UNI√ÉO DE BP + DRE COM VALIDA√á√ÉO CRUZADA\n",
    "# ================================================================================\n",
    "\n",
    "dt_ref_filter = int(f\"{ANO_REFERENCIA}01\")  # 202401\n",
    "\n",
    "sql_criar_classificacao = f\"\"\"\n",
    "CREATE TABLE {TABELA_SAIDA} AS\n",
    "\n",
    "WITH empresas_sc AS (\n",
    "    -- Filtrar apenas empresas de SC\n",
    "    SELECT DISTINCT \n",
    "        id_ecd,\n",
    "        nu_cnpj AS cnpj,\n",
    "        nm_empresarial,\n",
    "        cd_uf\n",
    "    FROM usr_sat_ecd.ecd_r0000_identificacao\n",
    "    WHERE dt_referencia = {dt_ref_filter}\n",
    "        AND cd_uf = '{UF_FILTRO}'\n",
    "),\n",
    "\n",
    "contas_bp AS (\n",
    "    -- BALAN√áO PATRIMONIAL\n",
    "    SELECT \n",
    "        bp.id_ecd,\n",
    "        {dt_ref_filter} AS dt_referencia,\n",
    "        emp.cnpj,\n",
    "        emp.nm_empresarial,\n",
    "        emp.cd_uf,\n",
    "        \n",
    "        -- Dados da conta\n",
    "        bp.cod_agl AS cd_conta,\n",
    "        bp.cod_agl_sup AS cd_conta_sup,\n",
    "        bp.descr_cod_agl AS descr_conta,\n",
    "        bp.nivel_agl AS nivel_conta,\n",
    "        bp.ind_cod_agl AS tp_conta_agl,  -- 'T' ou 'D'\n",
    "        'BP' AS origem_demonstrativo,\n",
    "        \n",
    "        -- Classifica√ß√£o BASE (campos originais)\n",
    "        bp.ind_grp_bal,  -- 'A' ou 'P'\n",
    "        NULL AS ind_grp_dre,\n",
    "        bp.ind_dc_cta_ini,\n",
    "        bp.ind_dc_cta_fin,\n",
    "        \n",
    "        -- Plano de contas (JOIN para pegar cd_natureza)\n",
    "        pc.cd_natureza,\n",
    "        pc.tp_conta AS tp_conta_pc,\n",
    "        \n",
    "        -- Valores\n",
    "        bp.vl_cta_ini,\n",
    "        bp.vl_cta_fin\n",
    "        \n",
    "    FROM usr_sat_ecd.ecd_rj100_balanco_patrimonial bp\n",
    "    INNER JOIN empresas_sc emp\n",
    "        ON bp.id_ecd = emp.id_ecd\n",
    "    LEFT JOIN usr_sat_ecd.ecd_ri050_plano_contas pc\n",
    "        ON bp.id_ecd = pc.id_ecd\n",
    "        AND bp.dt_referencia = pc.dt_referencia\n",
    "        AND bp.cod_agl = pc.cd_conta_anl\n",
    "    \n",
    "    WHERE bp.dt_referencia = {dt_ref_filter}\n",
    "        AND bp.cod_agl IS NOT NULL\n",
    "        AND (bp.vl_cta_ini != 0 OR bp.vl_cta_fin != 0)  -- Com movimenta√ß√£o\n",
    "),\n",
    "\n",
    "contas_dre AS (\n",
    "    -- DRE\n",
    "    SELECT \n",
    "        dre.id_ecd,\n",
    "        {dt_ref_filter} AS dt_referencia,\n",
    "        emp.cnpj,\n",
    "        emp.nm_empresarial,\n",
    "        emp.cd_uf,\n",
    "        \n",
    "        -- Dados da conta\n",
    "        dre.cod_agl AS cd_conta,\n",
    "        dre.cod_agl_sup AS cd_conta_sup,\n",
    "        dre.descr_cod_agl AS descr_conta,\n",
    "        dre.nivel_agl AS nivel_conta,\n",
    "        dre.ind_cod_agl AS tp_conta_agl,\n",
    "        'DRE' AS origem_demonstrativo,\n",
    "        \n",
    "        -- Classifica√ß√£o BASE\n",
    "        NULL AS ind_grp_bal,\n",
    "        dre.ind_grp_dre,  -- 'R' ou 'D'\n",
    "        dre.ind_dc_cta_ini,\n",
    "        dre.ind_dc_cta_fin,\n",
    "        \n",
    "        -- Plano de contas\n",
    "        pc.cd_natureza,\n",
    "        pc.tp_conta AS tp_conta_pc,\n",
    "        \n",
    "        -- Valores\n",
    "        dre.vl_cta_ini,\n",
    "        dre.vl_cta_fin\n",
    "        \n",
    "    FROM usr_sat_ecd.ecd_rj150_demonstracao_resultado_exercicio dre\n",
    "    INNER JOIN empresas_sc emp\n",
    "        ON dre.id_ecd = emp.id_ecd\n",
    "    LEFT JOIN usr_sat_ecd.ecd_ri050_plano_contas pc\n",
    "        ON dre.id_ecd = pc.id_ecd\n",
    "        AND dre.dt_referencia = pc.dt_referencia\n",
    "        AND dre.cod_agl = pc.cd_conta_anl\n",
    "    \n",
    "    WHERE dre.dt_referencia = {dt_ref_filter}\n",
    "        AND dre.cod_agl IS NOT NULL\n",
    "        AND (dre.vl_cta_ini != 0 OR dre.vl_cta_fin != 0)  -- Com movimenta√ß√£o\n",
    "),\n",
    "\n",
    "contas_unificadas AS (\n",
    "    SELECT * FROM contas_bp\n",
    "    UNION ALL\n",
    "    SELECT * FROM contas_dre\n",
    "),\n",
    "\n",
    "classificacao_camada1 AS (\n",
    "    SELECT \n",
    "        *,\n",
    "        \n",
    "        -- =========================================================\n",
    "        -- CAMADA 1: CLASSIFICA√á√ÉO POR VALIDA√á√ÉO CRUZADA\n",
    "        -- Prioridade: ind_grp + cd_natureza (99% consist√™ncia!)\n",
    "        -- =========================================================\n",
    "        \n",
    "        -- N√çVEL 1: ATIVO, PASSIVO, PL ou RESULTADO\n",
    "        CASE \n",
    "            -- BP: ATIVO\n",
    "            WHEN origem_demonstrativo = 'BP' \n",
    "                AND ind_grp_bal = 'A' \n",
    "                AND cd_natureza = '01' \n",
    "                THEN 'ATIVO'\n",
    "                \n",
    "            -- BP: PASSIVO\n",
    "            WHEN origem_demonstrativo = 'BP' \n",
    "                AND ind_grp_bal = 'P' \n",
    "                AND cd_natureza = '02' \n",
    "                THEN 'PASSIVO'\n",
    "                \n",
    "            -- BP: PATRIM√îNIO L√çQUIDO\n",
    "            WHEN origem_demonstrativo = 'BP' \n",
    "                AND ind_grp_bal = 'P' \n",
    "                AND cd_natureza = '03' \n",
    "                THEN 'PATRIMONIO_LIQUIDO'\n",
    "                \n",
    "            -- DRE: RESULTADO\n",
    "            WHEN origem_demonstrativo = 'DRE' \n",
    "                AND ind_grp_dre IN ('R', 'D') \n",
    "                AND cd_natureza = '04' \n",
    "                THEN 'RESULTADO'\n",
    "                \n",
    "            -- Fallback: s√≥ ind_grp (quando n√£o tem cd_natureza)\n",
    "            WHEN origem_demonstrativo = 'BP' AND ind_grp_bal = 'A' THEN 'ATIVO'\n",
    "            WHEN origem_demonstrativo = 'BP' AND ind_grp_bal = 'P' THEN 'PASSIVO'  -- pode ser PL tamb√©m\n",
    "            WHEN origem_demonstrativo = 'DRE' AND ind_grp_dre IN ('R', 'D') THEN 'RESULTADO'\n",
    "            \n",
    "            ELSE 'NAO_CLASSIFICADO'\n",
    "        END AS classificacao_nivel1,\n",
    "        \n",
    "        -- Confian√ßa da classifica√ß√£o N√≠vel 1\n",
    "        CASE \n",
    "            WHEN (origem_demonstrativo = 'BP' AND ind_grp_bal = 'A' AND cd_natureza = '01') \n",
    "                OR (origem_demonstrativo = 'BP' AND ind_grp_bal = 'P' AND cd_natureza IN ('02', '03'))\n",
    "                OR (origem_demonstrativo = 'DRE' AND ind_grp_dre IN ('R', 'D') AND cd_natureza = '04')\n",
    "                THEN 'MUITO_ALTA'  -- Valida√ß√£o cruzada perfeita\n",
    "                \n",
    "            WHEN cd_natureza IS NOT NULL \n",
    "                THEN 'ALTA'  -- Tem cd_natureza mas n√£o bate perfeitamente\n",
    "                \n",
    "            WHEN ind_grp_bal IS NOT NULL OR ind_grp_dre IS NOT NULL\n",
    "                THEN 'MEDIA'  -- S√≥ tem ind_grp\n",
    "                \n",
    "            ELSE 'BAIXA'\n",
    "        END AS confianca_nivel1,\n",
    "        \n",
    "        -- M√©todo usado\n",
    "        CASE \n",
    "            WHEN (origem_demonstrativo = 'BP' AND ind_grp_bal = 'A' AND cd_natureza = '01') \n",
    "                OR (origem_demonstrativo = 'BP' AND ind_grp_bal = 'P' AND cd_natureza IN ('02', '03'))\n",
    "                OR (origem_demonstrativo = 'DRE' AND ind_grp_dre IN ('R', 'D') AND cd_natureza = '04')\n",
    "                THEN 'VALIDACAO_CRUZADA'\n",
    "                \n",
    "            WHEN cd_natureza IS NOT NULL \n",
    "                THEN 'CD_NATUREZA'\n",
    "                \n",
    "            WHEN ind_grp_bal IS NOT NULL OR ind_grp_dre IS NOT NULL\n",
    "                THEN 'IND_GRP'\n",
    "                \n",
    "            ELSE 'NAO_CLASSIFICADO'\n",
    "        END AS metodo_nivel1\n",
    "        \n",
    "    FROM contas_unificadas\n",
    ")\n",
    "\n",
    "SELECT \n",
    "    id_ecd,\n",
    "    dt_referencia,\n",
    "    cnpj,\n",
    "    nm_empresarial,\n",
    "    cd_uf,\n",
    "    cd_conta,\n",
    "    cd_conta_sup,\n",
    "    descr_conta,\n",
    "    nivel_conta,\n",
    "    tp_conta_agl,\n",
    "    tp_conta_pc,\n",
    "    origem_demonstrativo,\n",
    "    ind_grp_bal,\n",
    "    ind_grp_dre,\n",
    "    cd_natureza,\n",
    "    ind_dc_cta_ini,\n",
    "    ind_dc_cta_fin,\n",
    "    classificacao_nivel1,\n",
    "    confianca_nivel1,\n",
    "    metodo_nivel1,\n",
    "    vl_cta_ini,\n",
    "    vl_cta_fin,\n",
    "    \n",
    "    -- Placeholders para pr√≥ximas camadas\n",
    "    CAST(NULL AS STRING) AS classificacao_nivel2,\n",
    "    CAST(NULL AS STRING) AS classificacao_nivel3,\n",
    "    CAST(NULL AS STRING) AS cd_conta_referencial_matched,\n",
    "    CAST(NULL AS DOUBLE) AS score_similaridade,\n",
    "    CAST(NULL AS STRING) AS metodo_final\n",
    "    \n",
    "FROM classificacao_camada1\n",
    "\"\"\"\n",
    "\n",
    "print(\"Executando SQL... (pode levar alguns minutos)\")\n",
    "spark.sql(sql_criar_classificacao)\n",
    "\n",
    "# Validar cria√ß√£o\n",
    "total = spark.sql(f\"SELECT COUNT(*) as total FROM {TABELA_SAIDA}\").collect()[0]['total']\n",
    "print(f\"\\n‚úÖ Tabela criada com {total:,} registros\")\n",
    "\n",
    "# Estat√≠sticas CAMADA 1\n",
    "print(\"\\nüìä ESTAT√çSTICAS - CAMADA 1 (Valida√ß√£o Cruzada):\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stats_camada1 = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        classificacao_nivel1,\n",
    "        confianca_nivel1,\n",
    "        metodo_nivel1,\n",
    "        COUNT(*) as qtd,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as perc\n",
    "    FROM {TABELA_SAIDA}\n",
    "    GROUP BY classificacao_nivel1, confianca_nivel1, metodo_nivel1\n",
    "    ORDER BY qtd DESC\n",
    "\"\"\")\n",
    "stats_camada1.show(30, truncate=False)\n",
    "\n",
    "print(\"\\nüìä RESUMO POR ORIGEM:\")\n",
    "resumo_origem = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        origem_demonstrativo,\n",
    "        COUNT(*) as total,\n",
    "        SUM(CASE WHEN classificacao_nivel1 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as classificados,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel1 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_classificado\n",
    "    FROM {TABELA_SAIDA}\n",
    "    GROUP BY origem_demonstrativo\n",
    "\"\"\")\n",
    "resumo_origem.show()\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ CAMADA 1 CONCLU√çDA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "674787a4-91b6-414d-bb8c-faecf9dc3ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 4: CLASSIFICA√á√ÉO CAMADA 2 - MATCH DE C√ìDIGO (Exato e Parcial)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîç APLICANDO CAMADA 2 - MATCH DE C√ìDIGO...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import col, when, length, substring, coalesce, lit\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR DADOS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"Carregando dados...\")\n",
    "df_contas = spark.table(f\"{TABELA_SAIDA}\")\n",
    "df_referencia = spark.table(f\"{DATABASE_DESTINO}.pc_referencia_completa\")\n",
    "\n",
    "print(f\"‚úÖ Contas carregadas: {df_contas.count():,}\")\n",
    "print(f\"‚úÖ Refer√™ncia carregada: {df_referencia.count():,}\")\n",
    "\n",
    "# ================================================================================\n",
    "# ESTRAT√âGIA DE MATCH\n",
    "# ================================================================================\n",
    "print(\"\\nüìã Estrat√©gia de Match:\")\n",
    "print(\"  1. Match EXATO: cod_agl = cd_conta_ref\")\n",
    "print(\"  2. Match PARCIAL (prefixo): primeiros 3-7 d√≠gitos\")\n",
    "print(\"  3. Heran√ßa via HIERARQUIA: usar cod_agl_sup\")\n",
    "\n",
    "# ================================================================================\n",
    "# MATCH EXATO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüéØ Aplicando MATCH EXATO...\")\n",
    "\n",
    "# JOIN para match exato\n",
    "df_match_exato = df_contas.alias(\"c\") \\\n",
    "    .join(\n",
    "        df_referencia.alias(\"r\"),\n",
    "        (col(\"c.cd_conta\") == col(\"r.cd_conta_ref\")) &\n",
    "        (col(\"c.origem_demonstrativo\") == col(\"r.origem_ref\")),\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"c.*\"),\n",
    "        \n",
    "        # Classifica√ß√µes da refer√™ncia (quando houver match)\n",
    "        when(col(\"r.cd_conta_ref\").isNotNull(), col(\"r.classificacao_nivel2\"))\n",
    "            .otherwise(col(\"c.classificacao_nivel2\"))\n",
    "            .alias(\"classificacao_nivel2_novo\"),\n",
    "            \n",
    "        when(col(\"r.cd_conta_ref\").isNotNull(), col(\"r.classificacao_nivel3\"))\n",
    "            .otherwise(col(\"c.classificacao_nivel3\"))\n",
    "            .alias(\"classificacao_nivel3_novo\"),\n",
    "            \n",
    "        when(col(\"r.cd_conta_ref\").isNotNull(), col(\"r.cd_conta_ref\"))\n",
    "            .otherwise(col(\"c.cd_conta_referencial_matched\"))\n",
    "            .alias(\"cd_conta_referencial_matched_novo\"),\n",
    "            \n",
    "        # Score de similaridade (1.0 = match perfeito)\n",
    "        when(col(\"r.cd_conta_ref\").isNotNull(), lit(1.0))\n",
    "            .otherwise(col(\"c.score_similaridade\"))\n",
    "            .alias(\"score_similaridade_novo\"),\n",
    "            \n",
    "        # M√©todo\n",
    "        when(col(\"r.cd_conta_ref\").isNotNull(), lit(\"MATCH_CODIGO_EXATO\"))\n",
    "            .otherwise(col(\"c.metodo_final\"))\n",
    "            .alias(\"metodo_final_novo\")\n",
    "    )\n",
    "\n",
    "# Estat√≠sticas do match exato\n",
    "match_exato_count = df_match_exato.filter(col(\"metodo_final_novo\") == \"MATCH_CODIGO_EXATO\").count()\n",
    "print(f\"‚úÖ Match exato: {match_exato_count:,} registros ({match_exato_count*100/df_contas.count():.2f}%)\")\n",
    "\n",
    "# ================================================================================\n",
    "# MATCH PARCIAL (PREFIXO)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîé Aplicando MATCH PARCIAL (prefixos)...\")\n",
    "\n",
    "# Para contas ainda n√£o classificadas em n√≠vel 2/3, tentar match parcial\n",
    "df_match_parcial = df_match_exato\n",
    "\n",
    "# Tentar prefixos de tamanho 7, 5, 3 (do mais espec√≠fico ao mais geral)\n",
    "for tam_prefixo in [7, 5, 3]:\n",
    "    print(f\"  Tentando prefixo de {tam_prefixo} caracteres...\")\n",
    "    \n",
    "    df_match_parcial = df_match_parcial.alias(\"c\") \\\n",
    "        .join(\n",
    "            df_referencia.alias(\"r\"),\n",
    "            (substring(col(\"c.cd_conta\"), 1, tam_prefixo) == substring(col(\"r.cd_conta_ref\"), 1, tam_prefixo)) &\n",
    "            (col(\"c.origem_demonstrativo\") == col(\"r.origem_ref\")) &\n",
    "            (col(\"c.classificacao_nivel2_novo\").isNull() | (col(\"c.classificacao_nivel2_novo\") == \"\")),\n",
    "            \"left\"\n",
    "        ) \\\n",
    "        .select(\n",
    "            col(\"c.id_ecd\"),\n",
    "            col(\"c.dt_referencia\"),\n",
    "            col(\"c.cnpj\"),\n",
    "            col(\"c.nm_empresarial\"),\n",
    "            col(\"c.cd_uf\"),\n",
    "            col(\"c.cd_conta\"),\n",
    "            col(\"c.cd_conta_sup\"),\n",
    "            col(\"c.descr_conta\"),\n",
    "            col(\"c.nivel_conta\"),\n",
    "            col(\"c.tp_conta_agl\"),\n",
    "            col(\"c.tp_conta_pc\"),\n",
    "            col(\"c.origem_demonstrativo\"),\n",
    "            col(\"c.ind_grp_bal\"),\n",
    "            col(\"c.ind_grp_dre\"),\n",
    "            col(\"c.cd_natureza\"),\n",
    "            col(\"c.ind_dc_cta_ini\"),\n",
    "            col(\"c.ind_dc_cta_fin\"),\n",
    "            col(\"c.classificacao_nivel1\"),\n",
    "            col(\"c.confianca_nivel1\"),\n",
    "            col(\"c.metodo_nivel1\"),\n",
    "            col(\"c.vl_cta_ini\"),\n",
    "            col(\"c.vl_cta_fin\"),\n",
    "            \n",
    "            # Atualizar classifica√ß√µes se encontrou match\n",
    "            when(\n",
    "                col(\"r.cd_conta_ref\").isNotNull() & \n",
    "                (col(\"c.classificacao_nivel2_novo\").isNull() | (col(\"c.classificacao_nivel2_novo\") == \"\")),\n",
    "                col(\"r.classificacao_nivel2\")\n",
    "            ).otherwise(col(\"c.classificacao_nivel2_novo\")).alias(\"classificacao_nivel2_novo\"),\n",
    "            \n",
    "            when(\n",
    "                col(\"r.cd_conta_ref\").isNotNull() & \n",
    "                (col(\"c.classificacao_nivel3_novo\").isNull() | (col(\"c.classificacao_nivel3_novo\") == \"\")),\n",
    "                col(\"r.classificacao_nivel3\")\n",
    "            ).otherwise(col(\"c.classificacao_nivel3_novo\")).alias(\"classificacao_nivel3_novo\"),\n",
    "            \n",
    "            when(\n",
    "                col(\"r.cd_conta_ref\").isNotNull() & \n",
    "                (col(\"c.cd_conta_referencial_matched_novo\").isNull() | (col(\"c.cd_conta_referencial_matched_novo\") == \"\")),\n",
    "                col(\"r.cd_conta_ref\")\n",
    "            ).otherwise(col(\"c.cd_conta_referencial_matched_novo\")).alias(\"cd_conta_referencial_matched_novo\"),\n",
    "            \n",
    "            when(\n",
    "                col(\"r.cd_conta_ref\").isNotNull() & \n",
    "                (col(\"c.score_similaridade_novo\").isNull()),\n",
    "                lit(tam_prefixo / 10.0)  # Score baseado no tamanho do prefixo\n",
    "            ).otherwise(col(\"c.score_similaridade_novo\")).alias(\"score_similaridade_novo\"),\n",
    "            \n",
    "            when(\n",
    "                col(\"r.cd_conta_ref\").isNotNull() & \n",
    "                (col(\"c.metodo_final_novo\").isNull() | (col(\"c.metodo_final_novo\") == \"\")),\n",
    "                lit(f\"MATCH_CODIGO_PARCIAL_{tam_prefixo}\")\n",
    "            ).otherwise(col(\"c.metodo_final_novo\")).alias(\"metodo_final_novo\")\n",
    "        )\n",
    "\n",
    "match_parcial_count = df_match_parcial.filter(\n",
    "    col(\"metodo_final_novo\").like(\"MATCH_CODIGO_PARCIAL%\")\n",
    ").count()\n",
    "print(f\"‚úÖ Match parcial: {match_parcial_count:,} registros adicionais\")\n",
    "\n",
    "# ================================================================================\n",
    "# HERAN√áA VIA HIERARQUIA (cod_agl_sup)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüå≥ Aplicando HERAN√áA VIA HIERARQUIA...\")\n",
    "\n",
    "# Para contas anal√≠ticas sem classifica√ß√£o, herdar da conta sint√©tica (pai)\n",
    "df_com_hierarquia = df_match_parcial.alias(\"c\") \\\n",
    "    .join(\n",
    "        df_match_parcial.alias(\"pai\").select(\n",
    "            col(\"id_ecd\").alias(\"pai_id_ecd\"),\n",
    "            col(\"cd_conta\").alias(\"pai_cd_conta\"),\n",
    "            col(\"classificacao_nivel2_novo\").alias(\"pai_nivel2\"),\n",
    "            col(\"classificacao_nivel3_novo\").alias(\"pai_nivel3\")\n",
    "        ),\n",
    "        (col(\"c.id_ecd\") == col(\"pai_id_ecd\")) &\n",
    "        (col(\"c.cd_conta_sup\") == col(\"pai_cd_conta\")) &\n",
    "        (col(\"c.classificacao_nivel2_novo\").isNull() | (col(\"c.classificacao_nivel2_novo\") == \"\")),\n",
    "        \"left\"\n",
    "    ) \\\n",
    "    .select(\n",
    "        col(\"c.id_ecd\"),\n",
    "        col(\"c.dt_referencia\"),\n",
    "        col(\"c.cnpj\"),\n",
    "        col(\"c.nm_empresarial\"),\n",
    "        col(\"c.cd_uf\"),\n",
    "        col(\"c.cd_conta\"),\n",
    "        col(\"c.cd_conta_sup\"),\n",
    "        col(\"c.descr_conta\"),\n",
    "        col(\"c.nivel_conta\"),\n",
    "        col(\"c.tp_conta_agl\"),\n",
    "        col(\"c.tp_conta_pc\"),\n",
    "        col(\"c.origem_demonstrativo\"),\n",
    "        col(\"c.ind_grp_bal\"),\n",
    "        col(\"c.ind_grp_dre\"),\n",
    "        col(\"c.cd_natureza\"),\n",
    "        col(\"c.ind_dc_cta_ini\"),\n",
    "        col(\"c.ind_dc_cta_fin\"),\n",
    "        col(\"c.classificacao_nivel1\"),\n",
    "        col(\"c.confianca_nivel1\"),\n",
    "        col(\"c.metodo_nivel1\"),\n",
    "        col(\"c.vl_cta_ini\"),\n",
    "        col(\"c.vl_cta_fin\"),\n",
    "        \n",
    "        # Herdar classifica√ß√£o do pai\n",
    "        coalesce(col(\"c.classificacao_nivel2_novo\"), col(\"pai_nivel2\")).alias(\"classificacao_nivel2\"),\n",
    "        coalesce(col(\"c.classificacao_nivel3_novo\"), col(\"pai_nivel3\")).alias(\"classificacao_nivel3\"),\n",
    "        col(\"c.cd_conta_referencial_matched_novo\").alias(\"cd_conta_referencial_matched\"),\n",
    "        col(\"c.score_similaridade_novo\").alias(\"score_similaridade\"),\n",
    "        \n",
    "        when(\n",
    "            col(\"c.metodo_final_novo\").isNull() & col(\"pai_nivel2\").isNotNull(),\n",
    "            lit(\"HERANCA_HIERARQUIA\")\n",
    "        ).otherwise(col(\"c.metodo_final_novo\")).alias(\"metodo_final\")\n",
    "    )\n",
    "\n",
    "heranca_count = df_com_hierarquia.filter(col(\"metodo_final\") == \"HERANCA_HIERARQUIA\").count()\n",
    "print(f\"‚úÖ Heran√ßa via hierarquia: {heranca_count:,} registros adicionais\")\n",
    "\n",
    "# ================================================================================\n",
    "# SALVAR RESULTADO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ Salvando resultado...\")\n",
    "\n",
    "df_com_hierarquia.createOrReplaceTempView(\"temp_camada2\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {TABELA_SAIDA}\n",
    "    SELECT * FROM temp_camada2\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Tabela atualizada!\")\n",
    "\n",
    "# ================================================================================\n",
    "# ESTAT√çSTICAS FINAIS CAMADA 2\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä ESTAT√çSTICAS - AP√ìS CAMADA 2:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stats_camada2 = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        metodo_final,\n",
    "        COUNT(*) as qtd,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as perc\n",
    "    FROM {TABELA_SAIDA}\n",
    "    GROUP BY metodo_final\n",
    "    ORDER BY qtd DESC\n",
    "\"\"\")\n",
    "stats_camada2.show(20, truncate=False)\n",
    "\n",
    "print(\"\\nüìä COBERTURA DE CLASSIFICA√á√ÉO:\")\n",
    "cobertura = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total,\n",
    "        SUM(CASE WHEN classificacao_nivel2 IS NOT NULL THEN 1 ELSE 0 END) as com_nivel2,\n",
    "        SUM(CASE WHEN classificacao_nivel3 IS NOT NULL THEN 1 ELSE 0 END) as com_nivel3,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel2 IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_nivel2,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel3 IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_nivel3\n",
    "    FROM {TABELA_SAIDA}\n",
    "\"\"\")\n",
    "cobertura.show(truncate=False)\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"‚úÖ CAMADA 2 CONCLU√çDA!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c9e9d7-be09-4015-9e0e-407d59f14fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 5 OTIMIZADA: MATCH POR DESCRI√á√ÉO (SEM LEVENSHTEIN - R√ÅPIDO!)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìù APLICANDO CAMADA 3 - MATCH POR DESCRI√á√ÉO (VERS√ÉO OTIMIZADA)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lower, trim, regexp_replace, length, \n",
    "    coalesce, lit, split, array_contains, substring\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# PREPARAR DADOS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Preparando dados para text mining...\")\n",
    "\n",
    "# Carregar apenas contas ainda SEM classifica√ß√£o n√≠vel 2\n",
    "df_sem_classificacao = spark.sql(f\"\"\"\n",
    "    SELECT *\n",
    "    FROM {TABELA_SAIDA}\n",
    "    WHERE classificacao_nivel2 IS NULL \n",
    "        OR classificacao_nivel2 = ''\n",
    "        OR classificacao_nivel2 = 'NAO_CLASSIFICADO'\n",
    "\"\"\")\n",
    "\n",
    "total_sem_class = df_sem_classificacao.count()\n",
    "print(f\"üìä Contas sem classifica√ß√£o n√≠vel 2: {total_sem_class:,}\")\n",
    "\n",
    "if total_sem_class == 0:\n",
    "    print(\"‚úÖ Todas as contas j√° foram classificadas! Pulando CAMADA 3.\")\n",
    "else:\n",
    "    # ================================================================================\n",
    "    # NORMALIZAR DESCRI√á√ïES\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüîß Normalizando descri√ß√µes...\")\n",
    "    \n",
    "    df_normalizado = df_sem_classificacao.withColumn(\n",
    "        \"descr_normalizada\",\n",
    "        lower(trim(regexp_replace(col(\"descr_conta\"), \"[^a-zA-Z0-9\\\\s]\", \" \")))\n",
    "    ).withColumn(\n",
    "        \"primeira_palavra\",\n",
    "        split(col(\"descr_normalizada\"), \" \").getItem(0)\n",
    "    ).withColumn(\n",
    "        \"primeiras_3_palavras\",\n",
    "        substring(col(\"descr_normalizada\"), 1, 50)  # Primeiros 50 caracteres\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Descri√ß√µes normalizadas!\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # ESTRAT√âGIA 1: PALAVRAS-CHAVE (Keywords)\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüîë ESTRAT√âGIA 1: Aplicando busca por palavras-chave...\")\n",
    "    \n",
    "    # Mapa de palavras-chave EXPANDIDO (mais abrangente)\n",
    "    keywords_map = {\n",
    "        # ATIVO\n",
    "        \"DISPONIBILIDADES\": [\"caixa\", \"banco\", \"deposito\", \"disponivel\", \"numerario\", \"dinheiro\"],\n",
    "        \"CREDITOS\": [\"cliente\", \"duplicata\", \"receber\", \"credito\", \"venda prazo\", \"contas receber\"],\n",
    "        \"ESTOQUES\": [\"estoque\", \"mercadoria\", \"produto acabado\", \"materia prima\", \"almoxarifado\", \"insumo\"],\n",
    "        \"INVESTIMENTOS\": [\"investimento\", \"participacao societaria\", \"acao\", \"quota\", \"aplicacao financeira\"],\n",
    "        \"IMOBILIZADO\": [\"imovel\", \"veiculo\", \"maquina\", \"equipamento\", \"movel\", \"instalacao\", \"imobilizado\", \"edificacao\"],\n",
    "        \"INTANGIVEL\": [\"software\", \"marca\", \"patente\", \"intangivel\", \"direito uso\", \"fundo comercio\"],\n",
    "        \n",
    "        # PASSIVO\n",
    "        \"FORNECEDORES\": [\"fornecedor\", \"fornecimento\", \"compra prazo\", \"contas pagar\"],\n",
    "        \"EMPRESTIMOS_CP\": [\"emprestimo\", \"financiamento curto\", \"banco emprestimo\"],\n",
    "        \"TRIBUTOS_A_PAGAR\": [\"tributo\", \"imposto pagar\", \"icms pagar\", \"pis pagar\", \"cofins pagar\", \"irpj pagar\", \"csll pagar\"],\n",
    "        \"SALARIOS_A_PAGAR\": [\"salario\", \"folha pagamento\", \"provisa ferias\", \"13 salario\", \"ordenado\", \"remuneracao\"],\n",
    "        \"EMPRESTIMOS_LP\": [\"emprestimo longo prazo\", \"financiamento longo\"],\n",
    "        \n",
    "        # PATRIM√îNIO L√çQUIDO\n",
    "        \"CAPITAL_SOCIAL\": [\"capital social\", \"capital subscrito\", \"capital integralizado\"],\n",
    "        \"RESERVAS\": [\"reserva\", \"reserva legal\", \"reserva lucro\"],\n",
    "        \"LUCROS_PREJUIZOS_ACUMULADOS\": [\"lucro acumulado\", \"prejuizo acumulado\", \"resultado exercicio\"],\n",
    "        \n",
    "        # RESULTADO - RECEITAS\n",
    "        \"RECEITA_BRUTA\": [\"receita\", \"venda\", \"faturamento\", \"receita bruta\", \"comercializacao\"],\n",
    "        \"DEDUCOES_RECEITA\": [\"devolucao\", \"desconto\", \"abatimento\", \"icms venda\", \"deducao\"],\n",
    "        \n",
    "        # RESULTADO - CUSTOS/DESPESAS\n",
    "        \"CUSTOS\": [\"custo\", \"cmv\", \"cpv\", \"custo mercadoria\", \"custo produto\"],\n",
    "        \"DESPESAS_OPERACIONAIS\": [\"despesa\", \"gasto\", \"despesa administrativa\", \"despesa comercial\", \"despesa venda\"],\n",
    "        \"RECEITAS_FINANCEIRAS\": [\"juros recebido\", \"receita financeira\", \"rendimento\", \"aplicacao\"],\n",
    "        \"DESPESAS_FINANCEIRAS\": [\"juros pago\", \"despesa financeira\", \"encargo\", \"iof\"],\n",
    "        \n",
    "        # Adicionais\n",
    "        \"OUTRAS_RECEITAS_DESPESAS\": [\"outra receita\", \"outra despesa\", \"diversa\"],\n",
    "        \"IMPOSTOS_RESULTADO\": [\"irpj\", \"csll\", \"imposto renda\", \"contribuicao social\"]\n",
    "    }\n",
    "    \n",
    "    # Criar condi√ß√µes WHEN para cada classifica√ß√£o\n",
    "    classificacao_por_keyword = lit(None)\n",
    "    \n",
    "    for classificacao, keywords in keywords_map.items():\n",
    "        condicao = lit(False)\n",
    "        for keyword in keywords:\n",
    "            condicao = condicao | col(\"descr_normalizada\").contains(keyword)\n",
    "        \n",
    "        classificacao_por_keyword = when(condicao, lit(classificacao)).otherwise(classificacao_por_keyword)\n",
    "    \n",
    "    df_com_keywords = df_normalizado.withColumn(\n",
    "        \"classificacao_keyword\",\n",
    "        classificacao_por_keyword\n",
    "    )\n",
    "    \n",
    "    keywords_match_count = df_com_keywords.filter(\n",
    "        col(\"classificacao_keyword\").isNotNull()\n",
    "    ).count()\n",
    "    \n",
    "    print(f\"‚úÖ Match por palavras-chave: {keywords_match_count:,} registros ({keywords_match_count*100/total_sem_class:.2f}%)\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # ESTRAT√âGIA 2: REGRAS HEUR√çSTICAS (cd_natureza + primeira_palavra)\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüéØ ESTRAT√âGIA 2: Aplicando regras heur√≠sticas...\")\n",
    "    \n",
    "    # Para contas ainda n√£o classificadas por keyword\n",
    "    df_com_heuristicas = df_com_keywords.withColumn(\n",
    "        \"classificacao_heuristica\",\n",
    "        when(\n",
    "            # Se j√° tem keyword, manter\n",
    "            col(\"classificacao_keyword\").isNotNull(),\n",
    "            col(\"classificacao_keyword\")\n",
    "        ).otherwise(\n",
    "            # Regras baseadas em cd_natureza + primeira palavra\n",
    "            when(\n",
    "                # ATIVO (natureza 01)\n",
    "                (col(\"cd_natureza\") == \"01\") & col(\"primeira_palavra\").isin(\n",
    "                    \"banco\", \"caixa\", \"bcr\", \"bb\", \"santander\", \"itau\", \"bradesco\", \"sicoob\"\n",
    "                ), lit(\"DISPONIBILIDADES\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"01\") & col(\"primeira_palavra\").isin(\n",
    "                    \"estoque\", \"mercadoria\", \"produto\", \"materia\", \"insumo\"\n",
    "                ), lit(\"ESTOQUES\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"01\") & col(\"primeira_palavra\").isin(\n",
    "                    \"cliente\", \"duplicata\", \"conta\", \"titulo\"\n",
    "                ), lit(\"CREDITOS\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"01\") & (col(\"nivel_conta\") >= 3),\n",
    "                # Ativo n√≠vel 3+ sem match espec√≠fico = provavelmente circulante\n",
    "                lit(\"ATIVO_CIRCULANTE\")\n",
    "            ).when(\n",
    "                # PASSIVO (natureza 02)\n",
    "                (col(\"cd_natureza\") == \"02\") & col(\"primeira_palavra\").isin(\n",
    "                    \"fornecedor\", \"fornecimento\", \"duplicata\", \"compra\"\n",
    "                ), lit(\"FORNECEDORES\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"02\") & col(\"primeira_palavra\").isin(\n",
    "                    \"salario\", \"ordenado\", \"folha\", \"ferias\", \"13\"\n",
    "                ), lit(\"SALARIOS_A_PAGAR\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"02\") & col(\"primeira_palavra\").isin(\n",
    "                    \"tributo\", \"imposto\", \"icms\", \"pis\", \"cofins\", \"iss\"\n",
    "                ), lit(\"TRIBUTOS_A_PAGAR\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"02\") & col(\"primeira_palavra\").isin(\n",
    "                    \"emprestimo\", \"financiamento\", \"banco\"\n",
    "                ), lit(\"EMPRESTIMOS_CP\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"02\") & (col(\"nivel_conta\") >= 3),\n",
    "                lit(\"PASSIVO_CIRCULANTE\")\n",
    "            ).when(\n",
    "                # PATRIM√îNIO L√çQUIDO (natureza 03)\n",
    "                (col(\"cd_natureza\") == \"03\") & col(\"primeira_palavra\").isin(\n",
    "                    \"capital\", \"subscrito\", \"integralizado\"\n",
    "                ), lit(\"CAPITAL_SOCIAL\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"03\") & col(\"primeira_palavra\").isin(\n",
    "                    \"reserva\"\n",
    "                ), lit(\"RESERVAS\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"03\") & col(\"primeira_palavra\").isin(\n",
    "                    \"lucro\", \"prejuizo\", \"resultado\"\n",
    "                ), lit(\"LUCROS_PREJUIZOS_ACUMULADOS\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"03\"),\n",
    "                lit(\"PATRIMONIO_LIQUIDO\")  # Fallback gen√©rico\n",
    "            ).when(\n",
    "                # RESULTADO (natureza 04)\n",
    "                (col(\"cd_natureza\") == \"04\") & (col(\"ind_grp_dre\") == \"R\") & col(\"primeira_palavra\").isin(\n",
    "                    \"receita\", \"venda\", \"faturamento\", \"comercializacao\"\n",
    "                ), lit(\"RECEITA_BRUTA\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"04\") & (col(\"ind_grp_dre\") == \"D\") & col(\"primeira_palavra\").isin(\n",
    "                    \"custo\", \"cmv\", \"cpv\"\n",
    "                ), lit(\"CUSTOS\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"04\") & (col(\"ind_grp_dre\") == \"D\") & col(\"primeira_palavra\").isin(\n",
    "                    \"despesa\", \"gasto\"\n",
    "                ), lit(\"DESPESAS_OPERACIONAIS\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"04\") & (col(\"ind_grp_dre\") == \"D\") & col(\"primeira_palavra\").isin(\n",
    "                    \"devolucao\", \"desconto\", \"abatimento\"\n",
    "                ), lit(\"DEDUCOES_RECEITA\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"04\") & col(\"primeira_palavra\").isin(\n",
    "                    \"juros\", \"juro\"\n",
    "                ),\n",
    "                when(col(\"ind_grp_dre\") == \"R\", lit(\"RECEITAS_FINANCEIRAS\"))\n",
    "                .otherwise(lit(\"DESPESAS_FINANCEIRAS\"))\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"04\") & (col(\"ind_grp_dre\") == \"R\"),\n",
    "                lit(\"RECEITA_BRUTA\")  # Fallback: Resultado + Receita\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"04\") & (col(\"ind_grp_dre\") == \"D\"),\n",
    "                lit(\"DESPESAS_OPERACIONAIS\")  # Fallback: Resultado + Despesa\n",
    "            ).otherwise(lit(None))\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    heuristica_match_count = df_com_heuristicas.filter(\n",
    "        col(\"classificacao_heuristica\").isNotNull()\n",
    "    ).count()\n",
    "    \n",
    "    adicional_heuristica = heuristica_match_count - keywords_match_count\n",
    "    print(f\"‚úÖ Match adicional por heur√≠sticas: {adicional_heuristica:,} registros\")\n",
    "    print(f\"üìä Total classificados na CAMADA 3: {heuristica_match_count:,} ({heuristica_match_count*100/total_sem_class:.2f}%)\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # ESTRAT√âGIA 3: FALLBACK POR cd_natureza + origem\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüîÑ ESTRAT√âGIA 3: Aplicando fallback gen√©rico...\")\n",
    "    \n",
    "    df_com_fallback = df_com_heuristicas.withColumn(\n",
    "        \"classificacao_final_camada3\",\n",
    "        when(\n",
    "            col(\"classificacao_heuristica\").isNotNull(),\n",
    "            col(\"classificacao_heuristica\")\n",
    "        ).otherwise(\n",
    "            # Fallback: classifica√ß√£o gen√©rica baseada em cd_natureza\n",
    "            when(\n",
    "                (col(\"cd_natureza\") == \"01\") & (col(\"origem_demonstrativo\") == \"BP\"),\n",
    "                when(col(\"nivel_conta\") <= 2, lit(\"ATIVO\")).otherwise(lit(\"ATIVO_CIRCULANTE\"))\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"02\") & (col(\"origem_demonstrativo\") == \"BP\"),\n",
    "                when(col(\"nivel_conta\") <= 2, lit(\"PASSIVO\")).otherwise(lit(\"PASSIVO_CIRCULANTE\"))\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"03\") & (col(\"origem_demonstrativo\") == \"BP\"),\n",
    "                lit(\"PATRIMONIO_LIQUIDO\")\n",
    "            ).when(\n",
    "                (col(\"cd_natureza\") == \"04\") & (col(\"origem_demonstrativo\") == \"DRE\"),\n",
    "                when(col(\"ind_grp_dre\") == \"R\", lit(\"RECEITA_BRUTA\")).otherwise(lit(\"DESPESAS_OPERACIONAIS\"))\n",
    "            ).otherwise(lit(None))\n",
    "        )\n",
    "    ).withColumn(\n",
    "        \"metodo_camada3\",\n",
    "        when(\n",
    "            col(\"classificacao_keyword\").isNotNull(),\n",
    "            lit(\"MATCH_KEYWORDS\")\n",
    "        ).when(\n",
    "            col(\"classificacao_heuristica\").isNotNull(),\n",
    "            lit(\"MATCH_HEURISTICAS\")\n",
    "        ).when(\n",
    "            col(\"classificacao_final_camada3\").isNotNull(),\n",
    "            lit(\"FALLBACK_NATUREZA\")\n",
    "        ).otherwise(lit(None))\n",
    "    )\n",
    "    \n",
    "    fallback_match_count = df_com_fallback.filter(\n",
    "        col(\"classificacao_final_camada3\").isNotNull()\n",
    "    ).count()\n",
    "    \n",
    "    adicional_fallback = fallback_match_count - heuristica_match_count\n",
    "    print(f\"‚úÖ Match adicional por fallback: {adicional_fallback:,} registros\")\n",
    "    print(f\"üìä TOTAL FINAL classificados: {fallback_match_count:,} ({fallback_match_count*100/total_sem_class:.2f}%)\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # ATUALIZAR TABELA PRINCIPAL\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüíæ Atualizando tabela principal...\")\n",
    "    \n",
    "    df_com_fallback.createOrReplaceTempView(\"temp_camada3\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW temp_merge AS\n",
    "        SELECT \n",
    "            principal.id_ecd,\n",
    "            principal.dt_referencia,\n",
    "            principal.cnpj,\n",
    "            principal.nm_empresarial,\n",
    "            principal.cd_uf,\n",
    "            principal.cd_conta,\n",
    "            principal.cd_conta_sup,\n",
    "            principal.descr_conta,\n",
    "            principal.nivel_conta,\n",
    "            principal.tp_conta_agl,\n",
    "            principal.tp_conta_pc,\n",
    "            principal.origem_demonstrativo,\n",
    "            principal.ind_grp_bal,\n",
    "            principal.ind_grp_dre,\n",
    "            principal.cd_natureza,\n",
    "            principal.ind_dc_cta_ini,\n",
    "            principal.ind_dc_cta_fin,\n",
    "            principal.classificacao_nivel1,\n",
    "            principal.confianca_nivel1,\n",
    "            principal.metodo_nivel1,\n",
    "            principal.vl_cta_ini,\n",
    "            principal.vl_cta_fin,\n",
    "            \n",
    "            -- Atualizar classificacao_nivel2 e nivel3\n",
    "            COALESCE(cam3.classificacao_final_camada3, principal.classificacao_nivel2) AS classificacao_nivel2,\n",
    "            principal.classificacao_nivel3,\n",
    "            principal.cd_conta_referencial_matched,\n",
    "            \n",
    "            -- Score (0.8 para keywords/heur√≠sticas, 0.6 para fallback)\n",
    "            COALESCE(\n",
    "                CASE \n",
    "                    WHEN cam3.metodo_camada3 = 'MATCH_KEYWORDS' THEN 0.8\n",
    "                    WHEN cam3.metodo_camada3 = 'MATCH_HEURISTICAS' THEN 0.75\n",
    "                    WHEN cam3.metodo_camada3 = 'FALLBACK_NATUREZA' THEN 0.6\n",
    "                    ELSE NULL\n",
    "                END,\n",
    "                principal.score_similaridade\n",
    "            ) AS score_similaridade,\n",
    "            \n",
    "            -- Atualizar m√©todo\n",
    "            COALESCE(cam3.metodo_camada3, principal.metodo_final) AS metodo_final\n",
    "            \n",
    "        FROM {TABELA_SAIDA} principal\n",
    "        LEFT JOIN temp_camada3 cam3\n",
    "            ON principal.id_ecd = cam3.id_ecd\n",
    "            AND principal.cd_conta = cam3.cd_conta\n",
    "    \"\"\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT OVERWRITE TABLE {TABELA_SAIDA}\n",
    "        SELECT * FROM temp_merge\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"‚úÖ Tabela atualizada!\")\n",
    "\n",
    "# ================================================================================\n",
    "# ESTAT√çSTICAS FINAIS CAMADA 3\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä ESTAT√çSTICAS - AP√ìS CAMADA 3:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "stats_metodos = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COALESCE(metodo_final, 'NAO_CLASSIFICADO') as metodo,\n",
    "        COUNT(*) as qtd,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as perc\n",
    "    FROM {TABELA_SAIDA}\n",
    "    GROUP BY metodo_final\n",
    "    ORDER BY qtd DESC\n",
    "\"\"\")\n",
    "stats_metodos.show(20, truncate=False)\n",
    "\n",
    "print(\"\\nüìä COBERTURA FINAL DE CLASSIFICA√á√ÉO:\")\n",
    "cobertura_final = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total,\n",
    "        SUM(CASE WHEN classificacao_nivel1 IS NOT NULL AND classificacao_nivel1 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as com_nivel1,\n",
    "        SUM(CASE WHEN classificacao_nivel2 IS NOT NULL AND classificacao_nivel2 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as com_nivel2,\n",
    "        SUM(CASE WHEN classificacao_nivel3 IS NOT NULL AND classificacao_nivel3 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as com_nivel3,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel1 IS NOT NULL AND classificacao_nivel1 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_nivel1,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel2 IS NOT NULL AND classificacao_nivel2 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_nivel2,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel3 IS NOT NULL AND classificacao_nivel3 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_nivel3\n",
    "    FROM {TABELA_SAIDA}\n",
    "\"\"\")\n",
    "cobertura_final.show(truncate=False)\n",
    "\n",
    "print(\"\\nüìä TOP 20 CLASSIFICA√á√ïES N√çVEL 2:\")\n",
    "top20 = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        classificacao_nivel2,\n",
    "        COUNT(*) as qtd,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as perc\n",
    "    FROM {TABELA_SAIDA}\n",
    "    WHERE classificacao_nivel2 IS NOT NULL \n",
    "        AND classificacao_nivel2 != 'NAO_CLASSIFICADO'\n",
    "    GROUP BY classificacao_nivel2\n",
    "    ORDER BY qtd DESC\n",
    "    LIMIT 20\n",
    "\"\"\")\n",
    "top20.show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ CAMADA 3 OTIMIZADA CONCLU√çDA!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\n‚ö° Tempo economizado: HORAS ‚Üí MINUTOS!\")\n",
    "print(\"üí™ Performance: Sem cross join custoso!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96640970-00d9-4f91-bdae-e756b6138b57",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 6: VALIDA√á√ÉO CONT√ÅBIL E PREPARA√á√ÉO PARA ML\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  VALIDA√á√ÉO CONT√ÅBIL E PREPARA√á√ÉO PARA ML...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import col, sum as spark_sum, abs as spark_abs, when, lit\n",
    "\n",
    "# ================================================================================\n",
    "# CALCULAR SALDOS CONT√ÅBEIS POR EMPRESA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Calculando saldos cont√°beis por empresa...\")\n",
    "\n",
    "# Calcular saldo cont√°bil (considerando natureza D/C)\n",
    "df_com_saldos = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        -- Saldo final cont√°bil (D negativo, C positivo)\n",
    "        CASE \n",
    "            WHEN ind_dc_cta_fin = 'D' THEN -vl_cta_fin\n",
    "            WHEN ind_dc_cta_fin = 'C' THEN vl_cta_fin\n",
    "            ELSE 0\n",
    "        END AS saldo_final_contabil,\n",
    "        \n",
    "        -- Saldo inicial cont√°bil\n",
    "        CASE \n",
    "            WHEN ind_dc_cta_ini = 'D' THEN -vl_cta_ini\n",
    "            WHEN ind_dc_cta_ini = 'C' THEN vl_cta_ini\n",
    "            ELSE 0\n",
    "        END AS saldo_inicial_contabil\n",
    "        \n",
    "    FROM {TABELA_SAIDA}\n",
    "    WHERE origem_demonstrativo = 'BP'  -- S√≥ BP para equa√ß√£o cont√°bil\n",
    "\"\"\")\n",
    "\n",
    "df_com_saldos.createOrReplaceTempView(\"temp_saldos\")\n",
    "\n",
    "# ================================================================================\n",
    "# EQUA√á√ÉO CONT√ÅBIL POR EMPRESA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüßÆ Calculando equa√ß√£o cont√°bil: ATIVO = PASSIVO + PL...\")\n",
    "\n",
    "equacao_por_empresa = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        id_ecd,\n",
    "        cnpj,\n",
    "        nm_empresarial,\n",
    "        \n",
    "        -- ATIVO (d√©bito = negativo, ent√£o soma com sinal negativo = valor positivo)\n",
    "        SUM(CASE WHEN classificacao_nivel1 = 'ATIVO' THEN -saldo_final_contabil ELSE 0 END) AS total_ativo,\n",
    "        \n",
    "        -- PASSIVO (cr√©dito = positivo)\n",
    "        SUM(CASE WHEN classificacao_nivel1 = 'PASSIVO' THEN saldo_final_contabil ELSE 0 END) AS total_passivo,\n",
    "        \n",
    "        -- PATRIM√îNIO L√çQUIDO (cr√©dito = positivo)\n",
    "        SUM(CASE WHEN classificacao_nivel1 = 'PATRIMONIO_LIQUIDO' THEN saldo_final_contabil ELSE 0 END) AS total_pl,\n",
    "        \n",
    "        -- Equa√ß√£o: Ativo - (Passivo + PL) = deveria ser 0\n",
    "        SUM(CASE WHEN classificacao_nivel1 = 'ATIVO' THEN -saldo_final_contabil ELSE 0 END) -\n",
    "        (\n",
    "            SUM(CASE WHEN classificacao_nivel1 = 'PASSIVO' THEN saldo_final_contabil ELSE 0 END) +\n",
    "            SUM(CASE WHEN classificacao_nivel1 = 'PATRIMONIO_LIQUIDO' THEN saldo_final_contabil ELSE 0 END)\n",
    "        ) AS diferenca_equacao,\n",
    "        \n",
    "        -- Diferen√ßa percentual\n",
    "        CASE \n",
    "            WHEN SUM(CASE WHEN classificacao_nivel1 = 'ATIVO' THEN -saldo_final_contabil ELSE 0 END) != 0 THEN\n",
    "                ABS(\n",
    "                    (SUM(CASE WHEN classificacao_nivel1 = 'ATIVO' THEN -saldo_final_contabil ELSE 0 END) -\n",
    "                    (SUM(CASE WHEN classificacao_nivel1 = 'PASSIVO' THEN saldo_final_contabil ELSE 0 END) +\n",
    "                     SUM(CASE WHEN classificacao_nivel1 = 'PATRIMONIO_LIQUIDO' THEN saldo_final_contabil ELSE 0 END))) * 100.0 /\n",
    "                    SUM(CASE WHEN classificacao_nivel1 = 'ATIVO' THEN -saldo_final_contabil ELSE 0 END)\n",
    "                )\n",
    "            ELSE 999.99\n",
    "        END AS diferenca_percentual\n",
    "        \n",
    "    FROM temp_saldos\n",
    "    GROUP BY id_ecd, cnpj, nm_empresarial\n",
    "    HAVING SUM(CASE WHEN classificacao_nivel1 = 'ATIVO' THEN -saldo_final_contabil ELSE 0 END) != 0\n",
    "\"\"\")\n",
    "\n",
    "equacao_por_empresa.createOrReplaceTempView(\"temp_equacao\")\n",
    "\n",
    "print(\"‚úÖ Equa√ß√£o cont√°bil calculada!\")\n",
    "\n",
    "# ================================================================================\n",
    "# VALIDAR EMPRESAS (Toler√¢ncia configur√°vel)\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\n‚úÖ Validando empresas (toler√¢ncia: {TOLERANCIA_EQUACAO}%)...\")\n",
    "\n",
    "empresas_validas = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        *,\n",
    "        CASE \n",
    "            WHEN ABS(diferenca_percentual) <= {TOLERANCIA_EQUACAO} THEN TRUE\n",
    "            ELSE FALSE\n",
    "        END AS equacao_valida\n",
    "    FROM temp_equacao\n",
    "\"\"\")\n",
    "\n",
    "empresas_validas.createOrReplaceTempView(\"temp_empresas_validas\")\n",
    "\n",
    "# Estat√≠sticas\n",
    "total_empresas = empresas_validas.count()\n",
    "empresas_ok = empresas_validas.filter(col(\"equacao_valida\") == True).count()\n",
    "empresas_nok = total_empresas - empresas_ok\n",
    "\n",
    "print(f\"\\nüìä RESULTADOS DA VALIDA√á√ÉO:\")\n",
    "print(f\"  Total de empresas: {total_empresas:,}\")\n",
    "print(f\"  ‚úÖ Equa√ß√£o OK (‚â§{TOLERANCIA_EQUACAO}%): {empresas_ok:,} ({empresas_ok*100/total_empresas:.2f}%)\")\n",
    "print(f\"  ‚ùå Equa√ß√£o NOK (>{TOLERANCIA_EQUACAO}%): {empresas_nok:,} ({empresas_nok*100/total_empresas:.2f}%)\")\n",
    "\n",
    "# Amostra de empresas com equa√ß√£o OK\n",
    "print(\"\\nüìã AMOSTRA - Empresas com equa√ß√£o OK:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cnpj, nm_empresarial,\n",
    "        ROUND(total_ativo, 2) as ativo,\n",
    "        ROUND(total_passivo, 2) as passivo,\n",
    "        ROUND(total_pl, 2) as pl,\n",
    "        ROUND(diferenca_equacao, 2) as diferenca,\n",
    "        ROUND(diferenca_percentual, 2) as diff_perc\n",
    "    FROM temp_empresas_validas\n",
    "    WHERE equacao_valida = TRUE\n",
    "    ORDER BY ABS(diferenca_percentual)\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# Amostra de empresas com problemas\n",
    "print(\"\\n‚ö†Ô∏è  AMOSTRA - Empresas com equa√ß√£o com problemas:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        cnpj, nm_empresarial,\n",
    "        ROUND(total_ativo, 2) as ativo,\n",
    "        ROUND(total_passivo, 2) as passivo,\n",
    "        ROUND(total_pl, 2) as pl,\n",
    "        ROUND(diferenca_equacao, 2) as diferenca,\n",
    "        ROUND(diferenca_percentual, 2) as diff_perc\n",
    "    FROM temp_empresas_validas\n",
    "    WHERE equacao_valida = FALSE\n",
    "    ORDER BY ABS(diferenca_percentual)\n",
    "    LIMIT 10\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# ================================================================================\n",
    "# MARCAR CONTAS PARA TREINO DE ML\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nü§ñ Marcando contas para treino de Machine Learning...\")\n",
    "\n",
    "# Criar flag indicando se a conta pode ser usada para treino\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE OR REPLACE TEMP VIEW temp_final AS\n",
    "    SELECT \n",
    "        c.*,\n",
    "        CASE \n",
    "            WHEN e.equacao_valida = TRUE \n",
    "                AND c.confianca_nivel1 IN ('MUITO_ALTA', 'ALTA')\n",
    "                AND c.classificacao_nivel2 IS NOT NULL\n",
    "                THEN TRUE\n",
    "            ELSE FALSE\n",
    "        END AS usar_para_treino_ml,\n",
    "        \n",
    "        e.equacao_valida AS empresa_equacao_ok,\n",
    "        e.diferenca_percentual AS empresa_diff_percentual\n",
    "        \n",
    "    FROM {TABELA_SAIDA} c\n",
    "    LEFT JOIN temp_empresas_validas e\n",
    "        ON c.id_ecd = e.id_ecd\n",
    "\"\"\")\n",
    "\n",
    "# Estat√≠sticas de contas para ML\n",
    "contas_para_ml = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_contas,\n",
    "        SUM(CASE WHEN usar_para_treino_ml = TRUE THEN 1 ELSE 0 END) as contas_treino,\n",
    "        ROUND(SUM(CASE WHEN usar_para_treino_ml = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_treino\n",
    "    FROM temp_final\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nüìä CONTAS DISPON√çVEIS PARA TREINO DE ML:\")\n",
    "contas_para_ml.show(truncate=False)\n",
    "\n",
    "# Distribui√ß√£o por classifica√ß√£o\n",
    "print(\"\\nüìä DISTRIBUI√á√ÉO DE CONTAS PARA TREINO (por classifica√ß√£o):\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        classificacao_nivel2,\n",
    "        COUNT(*) as qtd_treino\n",
    "    FROM temp_final\n",
    "    WHERE usar_para_treino_ml = TRUE\n",
    "    GROUP BY classificacao_nivel2\n",
    "    ORDER BY qtd_treino DESC\n",
    "\"\"\").show(20, truncate=False)\n",
    "\n",
    "# ================================================================================\n",
    "# SALVAR TABELA FINAL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ Salvando tabela final...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE_DESTINO}.ecd_contas_classificadas_final (\n",
    "        id_ecd BIGINT,\n",
    "        dt_referencia INT,\n",
    "        cnpj STRING,\n",
    "        nm_empresarial STRING,\n",
    "        cd_uf STRING,\n",
    "        cd_conta STRING,\n",
    "        cd_conta_sup STRING,\n",
    "        descr_conta STRING,\n",
    "        nivel_conta INT,\n",
    "        tp_conta_agl STRING,\n",
    "        tp_conta_pc STRING,\n",
    "        origem_demonstrativo STRING,\n",
    "        ind_grp_bal STRING,\n",
    "        ind_grp_dre STRING,\n",
    "        cd_natureza STRING,\n",
    "        ind_dc_cta_ini STRING,\n",
    "        ind_dc_cta_fin STRING,\n",
    "        classificacao_nivel1 STRING,\n",
    "        confianca_nivel1 STRING,\n",
    "        metodo_nivel1 STRING,\n",
    "        vl_cta_ini DOUBLE,\n",
    "        vl_cta_fin DOUBLE,\n",
    "        classificacao_nivel2 STRING,\n",
    "        classificacao_nivel3 STRING,\n",
    "        cd_conta_referencial_matched STRING,\n",
    "        score_similaridade DOUBLE,\n",
    "        metodo_final STRING,\n",
    "        usar_para_treino_ml BOOLEAN,\n",
    "        empresa_equacao_ok BOOLEAN,\n",
    "        empresa_diff_percentual DOUBLE\n",
    "    )\n",
    "    PARTITIONED BY (ano_referencia INT)\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    PARTITION (ano_referencia = {ANO_REFERENCIA})\n",
    "    SELECT \n",
    "        id_ecd, dt_referencia, cnpj, nm_empresarial, cd_uf,\n",
    "        cd_conta, cd_conta_sup, descr_conta, nivel_conta, tp_conta_agl, tp_conta_pc,\n",
    "        origem_demonstrativo, ind_grp_bal, ind_grp_dre, cd_natureza,\n",
    "        ind_dc_cta_ini, ind_dc_cta_fin,\n",
    "        classificacao_nivel1, confianca_nivel1, metodo_nivel1,\n",
    "        vl_cta_ini, vl_cta_fin,\n",
    "        classificacao_nivel2, classificacao_nivel3,\n",
    "        cd_conta_referencial_matched, score_similaridade, metodo_final,\n",
    "        usar_para_treino_ml, empresa_equacao_ok, empresa_diff_percentual\n",
    "    FROM temp_final\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Tabela final salva: {DATABASE_DESTINO}.ecd_contas_classificadas_final\")\n",
    "\n",
    "# ================================================================================\n",
    "# RELAT√ìRIO FINAL CONSOLIDADO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä RELAT√ìRIO FINAL - PIPELINE COMPLETO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£  COBERTURA DE CLASSIFICA√á√ÉO:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_contas,\n",
    "        SUM(CASE WHEN classificacao_nivel1 IS NOT NULL THEN 1 ELSE 0 END) as nivel1,\n",
    "        SUM(CASE WHEN classificacao_nivel2 IS NOT NULL THEN 1 ELSE 0 END) as nivel2,\n",
    "        SUM(CASE WHEN classificacao_nivel3 IS NOT NULL THEN 1 ELSE 0 END) as nivel3,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel1 IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_n1,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel2 IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_n2,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel3 IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_n3\n",
    "    FROM temp_final\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£  M√âTODOS DE CLASSIFICA√á√ÉO:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        metodo_final,\n",
    "        COUNT(*) as qtd,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as perc\n",
    "    FROM temp_final\n",
    "    WHERE metodo_final IS NOT NULL AND metodo_final != ''\n",
    "    GROUP BY metodo_final\n",
    "    ORDER BY qtd DESC\n",
    "\"\"\").show(20, truncate=False)\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£  CONFIAN√áA DA CLASSIFICA√á√ÉO:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        confianca_nivel1,\n",
    "        COUNT(*) as qtd,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as perc\n",
    "    FROM temp_final\n",
    "    GROUP BY confianca_nivel1\n",
    "    ORDER BY \n",
    "        CASE confianca_nivel1\n",
    "            WHEN 'MUITO_ALTA' THEN 1\n",
    "            WHEN 'ALTA' THEN 2\n",
    "            WHEN 'MEDIA' THEN 3\n",
    "            WHEN 'BAIXA' THEN 4\n",
    "        END\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£  VALIDA√á√ÉO CONT√ÅBIL:\")\n",
    "print(f\"  ‚úÖ Empresas com equa√ß√£o OK: {empresas_ok:,} / {total_empresas:,} ({empresas_ok*100/total_empresas:.2f}%)\")\n",
    "print(f\"  ü§ñ Contas prontas para ML: {contas_para_ml.collect()[0]['contas_treino']:,}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PIPELINE COMPLETO EXECUTADO COM SUCESSO!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìÇ Tabela final: {DATABASE_DESTINO}.ecd_contas_classificadas_final\")\n",
    "print(f\"üìÖ Ano processado: {ANO_REFERENCIA}\")\n",
    "print(f\"üìç UF: {UF_FILTRO}\")\n",
    "print(\"\\nüéØ Pr√≥ximos passos:\")\n",
    "print(\"  1. Treinar modelos de ML com as contas marcadas (usar_para_treino_ml = TRUE)\")\n",
    "print(\"  2. Aplicar ML nas contas n√£o classificadas\")\n",
    "print(\"  3. Iterar para melhorar cobertura\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a1a0ea-8af2-4449-b794-0f40f18a98ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 6.5: DEDUPLICA√á√ÉO DA TABELA FINAL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüßπ DEDUPLICANDO TABELA FINAL...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ================================================================================\n",
    "# DIAGN√ìSTICO INICIAL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä DIAGN√ìSTICO - Antes da deduplica√ß√£o:\")\n",
    "\n",
    "# Total de registros\n",
    "total_antes = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "\"\"\").collect()[0]['total']\n",
    "\n",
    "print(f\"  Total de registros: {total_antes:,}\")\n",
    "\n",
    "# Registros √∫nicos (por id_ecd + cd_conta)\n",
    "unicos_antes = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT id_ecd, cd_conta) as unicos\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "\"\"\").collect()[0]['unicos']\n",
    "\n",
    "print(f\"  Registros √∫nicos (id_ecd + cd_conta): {unicos_antes:,}\")\n",
    "\n",
    "duplicatas = total_antes - unicos_antes\n",
    "perc_duplicatas = (duplicatas * 100.0 / total_antes) if total_antes > 0 else 0\n",
    "\n",
    "print(f\"  Duplicatas: {duplicatas:,} ({perc_duplicatas:.2f}%)\")\n",
    "\n",
    "if duplicatas == 0:\n",
    "    print(\"\\n‚úÖ N√£o h√° duplicatas! Tabela j√° est√° limpa.\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    # ================================================================================\n",
    "    # ESTRAT√âGIA DE DEDUPLICA√á√ÉO\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüîß Estrat√©gia de deduplica√ß√£o:\")\n",
    "    print(\"  1. Manter apenas 1 registro por (id_ecd, cd_conta)\")\n",
    "    print(\"  2. Prioridade: melhor m√©todo de classifica√ß√£o + maior confian√ßa\")\n",
    "    print(\"  3. Usar ROW_NUMBER() com ORDER BY apropriado\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # CRIAR TABELA DEDUPLICADA\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüíæ Criando tabela deduplicada...\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE OR REPLACE TEMP VIEW temp_deduplicada AS\n",
    "        WITH ranked AS (\n",
    "            SELECT \n",
    "                *,\n",
    "                ROW_NUMBER() OVER (\n",
    "                    PARTITION BY id_ecd, cd_conta\n",
    "                    ORDER BY \n",
    "                        -- Prioridade 1: Melhor m√©todo\n",
    "                        CASE metodo_final\n",
    "                            WHEN 'MATCH_CODIGO_EXATO' THEN 1\n",
    "                            WHEN 'MATCH_CODIGO_PARCIAL_7' THEN 2\n",
    "                            WHEN 'MATCH_CODIGO_PARCIAL_5' THEN 3\n",
    "                            WHEN 'MATCH_CODIGO_PARCIAL_3' THEN 4\n",
    "                            WHEN 'HERANCA_HIERARQUIA' THEN 5\n",
    "                            WHEN 'MATCH_KEYWORDS' THEN 6\n",
    "                            WHEN 'MATCH_HEURISTICAS' THEN 7\n",
    "                            WHEN 'FALLBACK_NATUREZA' THEN 8\n",
    "                            ELSE 99\n",
    "                        END,\n",
    "                        -- Prioridade 2: Melhor confian√ßa\n",
    "                        CASE confianca_nivel1\n",
    "                            WHEN 'MUITO_ALTA' THEN 1\n",
    "                            WHEN 'ALTA' THEN 2\n",
    "                            WHEN 'MEDIA' THEN 3\n",
    "                            WHEN 'BAIXA' THEN 4\n",
    "                            ELSE 99\n",
    "                        END,\n",
    "                        -- Prioridade 3: Maior score de similaridade\n",
    "                        score_similaridade DESC NULLS LAST,\n",
    "                        -- Prioridade 4: Mais recente (desempate)\n",
    "                        dt_referencia DESC\n",
    "                ) as rn\n",
    "            FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "            WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "        )\n",
    "        SELECT \n",
    "            id_ecd, dt_referencia, cnpj, nm_empresarial, cd_uf,\n",
    "            cd_conta, cd_conta_sup, descr_conta, nivel_conta, \n",
    "            tp_conta_agl, tp_conta_pc, origem_demonstrativo,\n",
    "            ind_grp_bal, ind_grp_dre, cd_natureza,\n",
    "            ind_dc_cta_ini, ind_dc_cta_fin,\n",
    "            classificacao_nivel1, confianca_nivel1, metodo_nivel1,\n",
    "            vl_cta_ini, vl_cta_fin,\n",
    "            classificacao_nivel2, classificacao_nivel3,\n",
    "            cd_conta_referencial_matched, score_similaridade, metodo_final,\n",
    "            usar_para_treino_ml, empresa_equacao_ok, empresa_diff_percentual\n",
    "        FROM ranked\n",
    "        WHERE rn = 1\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"‚úÖ View tempor√°ria criada!\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # VALIDAR DEDUPLICA√á√ÉO\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüîç Validando deduplica√ß√£o...\")\n",
    "    \n",
    "    total_depois = spark.sql(\"SELECT COUNT(*) as total FROM temp_deduplicada\").collect()[0]['total']\n",
    "    \n",
    "    print(f\"  Registros ap√≥s deduplica√ß√£o: {total_depois:,}\")\n",
    "    print(f\"  Registros removidos: {total_antes - total_depois:,}\")\n",
    "    print(f\"  Redu√ß√£o: {((total_antes - total_depois) * 100.0 / total_antes):.2f}%\")\n",
    "    \n",
    "    # Verificar se n√£o perdemos contas √∫nicas\n",
    "    if total_depois != unicos_antes:\n",
    "        print(f\"\\n‚ö†Ô∏è  ATEN√á√ÉO: Esperava {unicos_antes:,} registros √∫nicos, mas obteve {total_depois:,}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ Valida√ß√£o OK: n√∫mero de registros √∫nicos est√° correto!\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # ESTAT√çSTICAS DA DEDUPLICA√á√ÉO\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüìä Estat√≠sticas ap√≥s deduplica√ß√£o:\")\n",
    "    \n",
    "    # Cobertura\n",
    "    cobertura = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total,\n",
    "            SUM(CASE WHEN classificacao_nivel1 IS NOT NULL THEN 1 ELSE 0 END) as nivel1,\n",
    "            SUM(CASE WHEN classificacao_nivel2 IS NOT NULL THEN 1 ELSE 0 END) as nivel2,\n",
    "            SUM(CASE WHEN classificacao_nivel3 IS NOT NULL THEN 1 ELSE 0 END) as nivel3,\n",
    "            ROUND(SUM(CASE WHEN classificacao_nivel1 IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_n1,\n",
    "            ROUND(SUM(CASE WHEN classificacao_nivel2 IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_n2,\n",
    "            ROUND(SUM(CASE WHEN classificacao_nivel3 IS NOT NULL THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_n3\n",
    "        FROM temp_deduplicada\n",
    "    \"\"\")\n",
    "    cobertura.show(truncate=False)\n",
    "    \n",
    "    # M√©todos\n",
    "    print(\"\\nüìä Distribui√ß√£o por m√©todo (ap√≥s deduplica√ß√£o):\")\n",
    "    metodos = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COALESCE(metodo_final, 'NAO_CLASSIFICADO') as metodo,\n",
    "            COUNT(*) as qtd,\n",
    "            ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as perc\n",
    "        FROM temp_deduplicada\n",
    "        GROUP BY metodo_final\n",
    "        ORDER BY qtd DESC\n",
    "        LIMIT 10\n",
    "    \"\"\")\n",
    "    metodos.show(truncate=False)\n",
    "    \n",
    "    # Contas para ML\n",
    "    print(\"\\nüìä Contas para ML (ap√≥s deduplica√ß√£o):\")\n",
    "    ml_stats = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            COUNT(*) as total,\n",
    "            SUM(CASE WHEN usar_para_treino_ml = TRUE THEN 1 ELSE 0 END) as para_ml,\n",
    "            ROUND(SUM(CASE WHEN usar_para_treino_ml = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_ml\n",
    "        FROM temp_deduplicada\n",
    "    \"\"\")\n",
    "    ml_stats.show(truncate=False)\n",
    "    \n",
    "    # ================================================================================\n",
    "    # SALVAR TABELA DEDUPLICADA\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüíæ Salvando tabela deduplicada...\")\n",
    "    print(\"‚è≥ Isso pode levar alguns minutos...\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        INSERT OVERWRITE TABLE {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "        PARTITION (ano_referencia = {ANO_REFERENCIA})\n",
    "        SELECT * FROM temp_deduplicada\n",
    "    \"\"\")\n",
    "    \n",
    "    print(\"‚úÖ Tabela deduplicada salva!\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # VALIDA√á√ÉO FINAL\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüîç Valida√ß√£o final...\")\n",
    "    \n",
    "    final_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as total\n",
    "        FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "        WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "    \"\"\").collect()[0]['total']\n",
    "    \n",
    "    print(f\"  Registros na tabela final: {final_count:,}\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # RESUMO\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä RESUMO DA DEDUPLICA√á√ÉO\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"  Registros ANTES: {total_antes:,}\")\n",
    "    print(f\"  Registros DEPOIS: {final_count:,}\")\n",
    "    print(f\"  Duplicatas removidas: {total_antes - final_count:,}\")\n",
    "    print(f\"  Redu√ß√£o: {((total_antes - final_count) * 100.0 / total_antes):.2f}%\")\n",
    "    print(f\"  Espa√ßo economizado: ~{((total_antes - final_count) / 1000000):.1f} milh√µes de registros\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"‚úÖ DEDUPLICA√á√ÉO CONCLU√çDA!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe8dd07-b10b-44cf-886d-85ee78e70406",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 7: VISUALIZA√á√ïES E AN√ÅLISES DETALHADAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä VISUALIZA√á√ïES E AN√ÅLISES DETALHADAS DOS RESULTADOS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import pandas as pd\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR DADOS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Carregando dados para an√°lise...\")\n",
    "\n",
    "df_final = spark.table(f\"{DATABASE_DESTINO}.ecd_contas_classificadas_final\").filter(\n",
    "    col(\"ano_referencia\") == ANO_REFERENCIA\n",
    ")\n",
    "\n",
    "total_registros = df_final.count()\n",
    "print(f\"‚úÖ Total de registros: {total_registros:,}\")\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE 1: COBERTURA DE CLASSIFICA√á√ÉO POR N√çVEL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISE 1: Cobertura de Classifica√ß√£o\")\n",
    "\n",
    "cobertura_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        'N√≠vel 1' as nivel,\n",
    "        COUNT(*) as total,\n",
    "        SUM(CASE WHEN classificacao_nivel1 IS NOT NULL AND classificacao_nivel1 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as classificados,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel1 IS NOT NULL AND classificacao_nivel1 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as percentual\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'N√≠vel 2' as nivel,\n",
    "        COUNT(*) as total,\n",
    "        SUM(CASE WHEN classificacao_nivel2 IS NOT NULL AND classificacao_nivel2 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as classificados,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel2 IS NOT NULL AND classificacao_nivel2 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as percentual\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'N√≠vel 3' as nivel,\n",
    "        COUNT(*) as total,\n",
    "        SUM(CASE WHEN classificacao_nivel3 IS NOT NULL AND classificacao_nivel3 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as classificados,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel3 IS NOT NULL AND classificacao_nivel3 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as percentual\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(cobertura_df)\n",
    "\n",
    "# Gr√°fico de cobertura\n",
    "fig_cobertura = go.Figure()\n",
    "fig_cobertura.add_trace(go.Bar(\n",
    "    x=cobertura_df['nivel'],\n",
    "    y=cobertura_df['percentual'],\n",
    "    text=cobertura_df['percentual'].apply(lambda x: f\"{x:.1f}%\"),\n",
    "    textposition='auto',\n",
    "    marker_color=['#2ecc71', '#3498db', '#e74c3c']\n",
    "))\n",
    "fig_cobertura.update_layout(\n",
    "    title=f\"Cobertura de Classifica√ß√£o por N√≠vel - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "    xaxis_title=\"N√≠vel de Classifica√ß√£o\",\n",
    "    yaxis_title=\"Percentual Classificado (%)\",\n",
    "    yaxis_range=[0, 105],\n",
    "    height=400\n",
    ")\n",
    "fig_cobertura.show()\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE 2: DISTRIBUI√á√ÉO POR M√âTODO DE CLASSIFICA√á√ÉO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISE 2: Distribui√ß√£o por M√©todo de Classifica√ß√£o\")\n",
    "\n",
    "metodos_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COALESCE(metodo_final, 'NAO_CLASSIFICADO') as metodo,\n",
    "        COUNT(*) as qtd,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentual\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "    GROUP BY metodo_final\n",
    "    ORDER BY qtd DESC\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(metodos_df)\n",
    "\n",
    "# Gr√°fico de pizza - M√©todos\n",
    "fig_metodos = go.Figure(data=[go.Pie(\n",
    "    labels=metodos_df['metodo'],\n",
    "    values=metodos_df['qtd'],\n",
    "    hole=.3,\n",
    "    textinfo='label+percent',\n",
    "    textposition='auto'\n",
    ")])\n",
    "fig_metodos.update_layout(\n",
    "    title=f\"Distribui√ß√£o por M√©todo de Classifica√ß√£o - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "    height=500\n",
    ")\n",
    "fig_metodos.show()\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE 3: CONFIAN√áA DA CLASSIFICA√á√ÉO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISE 3: N√≠veis de Confian√ßa\")\n",
    "\n",
    "confianca_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COALESCE(confianca_nivel1, 'NAO_DEFINIDA') as confianca,\n",
    "        COUNT(*) as qtd,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentual\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "    GROUP BY confianca_nivel1\n",
    "    ORDER BY \n",
    "        CASE confianca_nivel1\n",
    "            WHEN 'MUITO_ALTA' THEN 1\n",
    "            WHEN 'ALTA' THEN 2\n",
    "            WHEN 'MEDIA' THEN 3\n",
    "            WHEN 'BAIXA' THEN 4\n",
    "            ELSE 5\n",
    "        END\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(confianca_df)\n",
    "\n",
    "# Gr√°fico de barras - Confian√ßa\n",
    "fig_confianca = go.Figure()\n",
    "colors_conf = {'MUITO_ALTA': '#27ae60', 'ALTA': '#2ecc71', 'MEDIA': '#f39c12', 'BAIXA': '#e74c3c', 'NAO_DEFINIDA': '#95a5a6'}\n",
    "fig_confianca.add_trace(go.Bar(\n",
    "    x=confianca_df['confianca'],\n",
    "    y=confianca_df['qtd'],\n",
    "    text=confianca_df['percentual'].apply(lambda x: f\"{x:.1f}%\"),\n",
    "    textposition='auto',\n",
    "    marker_color=[colors_conf.get(x, '#95a5a6') for x in confianca_df['confianca']]\n",
    "))\n",
    "fig_confianca.update_layout(\n",
    "    title=f\"Distribui√ß√£o por N√≠vel de Confian√ßa - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "    xaxis_title=\"N√≠vel de Confian√ßa\",\n",
    "    yaxis_title=\"Quantidade de Contas\",\n",
    "    height=400\n",
    ")\n",
    "fig_confianca.show()\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE 4: TOP 20 CLASSIFICA√á√ïES N√çVEL 2\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISE 4: Top 20 Classifica√ß√µes (N√≠vel 2)\")\n",
    "\n",
    "top_class_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COALESCE(classificacao_nivel2, 'NAO_CLASSIFICADO') as classificacao,\n",
    "        origem_demonstrativo,\n",
    "        COUNT(*) as qtd,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentual\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "    GROUP BY classificacao_nivel2, origem_demonstrativo\n",
    "    ORDER BY qtd DESC\n",
    "    LIMIT 20\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(top_class_df)\n",
    "\n",
    "# Gr√°fico de barras horizontais - Top classifica√ß√µes\n",
    "fig_top = go.Figure()\n",
    "for origem in top_class_df['origem_demonstrativo'].unique():\n",
    "    df_origem = top_class_df[top_class_df['origem_demonstrativo'] == origem]\n",
    "    fig_top.add_trace(go.Bar(\n",
    "        name=origem,\n",
    "        y=df_origem['classificacao'],\n",
    "        x=df_origem['qtd'],\n",
    "        orientation='h',\n",
    "        text=df_origem['percentual'].apply(lambda x: f\"{x:.1f}%\"),\n",
    "        textposition='auto'\n",
    "    ))\n",
    "\n",
    "fig_top.update_layout(\n",
    "    title=f\"Top 20 Classifica√ß√µes (N√≠vel 2) - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "    xaxis_title=\"Quantidade de Contas\",\n",
    "    yaxis_title=\"Classifica√ß√£o\",\n",
    "    barmode='stack',\n",
    "    height=600\n",
    ")\n",
    "fig_top.show()\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE 5: VALIDA√á√ÉO CONT√ÅBIL - EMPRESAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISE 5: Valida√ß√£o da Equa√ß√£o Cont√°bil\")\n",
    "\n",
    "validacao_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        CASE WHEN empresa_equacao_ok = TRUE THEN 'Equa√ß√£o OK' ELSE 'Equa√ß√£o com Erro' END as status,\n",
    "        COUNT(DISTINCT id_ecd) as qtd_empresas,\n",
    "        ROUND(COUNT(DISTINCT id_ecd) * 100.0 / SUM(COUNT(DISTINCT id_ecd)) OVER (), 2) as percentual\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "        AND empresa_equacao_ok IS NOT NULL\n",
    "    GROUP BY empresa_equacao_ok\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(validacao_df)\n",
    "\n",
    "# Gr√°fico de pizza - Valida√ß√£o\n",
    "fig_validacao = go.Figure(data=[go.Pie(\n",
    "    labels=validacao_df['status'],\n",
    "    values=validacao_df['qtd_empresas'],\n",
    "    marker_colors=['#27ae60', '#e74c3c'],\n",
    "    hole=.4,\n",
    "    textinfo='label+percent+value',\n",
    "    textposition='auto'\n",
    ")])\n",
    "fig_validacao.update_layout(\n",
    "    title=f\"Valida√ß√£o da Equa√ß√£o Cont√°bil (Toler√¢ncia {TOLERANCIA_EQUACAO}%) - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "    height=400\n",
    ")\n",
    "fig_validacao.show()\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE 6: DISTRIBUI√á√ÉO DE ERRO NA EQUA√á√ÉO CONT√ÅBIL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISE 6: Distribui√ß√£o do Erro na Equa√ß√£o Cont√°bil\")\n",
    "\n",
    "erro_dist_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        empresa_diff_percentual\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "        AND empresa_diff_percentual IS NOT NULL\n",
    "        AND empresa_diff_percentual < 100  -- Filtrar outliers extremos\n",
    "    GROUP BY id_ecd, empresa_diff_percentual\n",
    "\"\"\").toPandas()\n",
    "\n",
    "if len(erro_dist_df) > 0:\n",
    "    fig_erro_dist = px.histogram(\n",
    "        erro_dist_df, \n",
    "        x='empresa_diff_percentual',\n",
    "        nbins=50,\n",
    "        title=f\"Distribui√ß√£o do Erro Percentual na Equa√ß√£o Cont√°bil - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "        labels={'empresa_diff_percentual': 'Erro Percentual (%)', 'count': 'N√∫mero de Empresas'}\n",
    "    )\n",
    "    fig_erro_dist.add_vline(x=TOLERANCIA_EQUACAO, line_dash=\"dash\", line_color=\"red\", \n",
    "                            annotation_text=f\"Toler√¢ncia {TOLERANCIA_EQUACAO}%\")\n",
    "    fig_erro_dist.update_layout(height=400)\n",
    "    fig_erro_dist.show()\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE 7: CONTAS DISPON√çVEIS PARA ML\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISE 7: Contas Dispon√≠veis para Machine Learning\")\n",
    "\n",
    "ml_stats_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        usar_para_treino_ml,\n",
    "        COUNT(*) as qtd_contas,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentual\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "    GROUP BY usar_para_treino_ml\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(ml_stats_df)\n",
    "\n",
    "# Distribui√ß√£o de contas para ML por classifica√ß√£o\n",
    "ml_por_class_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COALESCE(classificacao_nivel2, 'NAO_CLASSIFICADO') as classificacao,\n",
    "        COUNT(*) as qtd_treino\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "        AND usar_para_treino_ml = TRUE\n",
    "    GROUP BY classificacao_nivel2\n",
    "    ORDER BY qtd_treino DESC\n",
    "    LIMIT 15\n",
    "\"\"\").toPandas()\n",
    "\n",
    "print(\"\\nTop 15 Classifica√ß√µes para Treino ML:\")\n",
    "print(ml_por_class_df)\n",
    "\n",
    "# Gr√°fico - Contas para ML\n",
    "fig_ml = go.Figure()\n",
    "fig_ml.add_trace(go.Bar(\n",
    "    x=ml_por_class_df['classificacao'],\n",
    "    y=ml_por_class_df['qtd_treino'],\n",
    "    marker_color='#3498db',\n",
    "    text=ml_por_class_df['qtd_treino'],\n",
    "    textposition='auto'\n",
    "))\n",
    "fig_ml.update_layout(\n",
    "    title=f\"Top 15 Classifica√ß√µes com Mais Contas para Treino ML - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "    xaxis_title=\"Classifica√ß√£o (N√≠vel 2)\",\n",
    "    yaxis_title=\"Quantidade de Contas\",\n",
    "    height=500,\n",
    "    xaxis_tickangle=-45\n",
    ")\n",
    "fig_ml.show()\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE 8: QUALIDADE DA CLASSIFICA√á√ÉO - SCORE DE SIMILARIDADE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISE 8: Distribui√ß√£o do Score de Similaridade\")\n",
    "\n",
    "score_dist_df = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        score_similaridade,\n",
    "        metodo_final\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "        AND score_similaridade IS NOT NULL\n",
    "        AND metodo_final IN ('MATCH_CODIGO_EXATO', 'MATCH_CODIGO_PARCIAL_7', \n",
    "                             'MATCH_CODIGO_PARCIAL_5', 'MATCH_CODIGO_PARCIAL_3',\n",
    "                             'MATCH_KEYWORDS', 'MATCH_SIMILARIDADE')\n",
    "\"\"\").toPandas()\n",
    "\n",
    "if len(score_dist_df) > 0:\n",
    "    fig_score = px.box(\n",
    "        score_dist_df,\n",
    "        x='metodo_final',\n",
    "        y='score_similaridade',\n",
    "        title=f\"Distribui√ß√£o do Score de Similaridade por M√©todo - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "        labels={'metodo_final': 'M√©todo de Classifica√ß√£o', 'score_similaridade': 'Score de Similaridade'}\n",
    "    )\n",
    "    fig_score.update_layout(height=500, xaxis_tickangle=-45)\n",
    "    fig_score.show()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ AN√ÅLISES E VISUALIZA√á√ïES CONCLU√çDAS!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48020a66-cc75-48a8-ace5-a32776050093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 8: CRIAR TABELAS ANAL√çTICAS NO HIVE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ CRIANDO TABELAS ANAL√çTICAS NO HIVE...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# ================================================================================\n",
    "# CONFIGURA√á√ÉO\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"üìä Database: {DATABASE_DESTINO}\")\n",
    "print(f\"üìÖ Ano: {ANO_REFERENCIA}\")\n",
    "print(f\"üìç UF: {UF_FILTRO}\")\n",
    "\n",
    "# ================================================================================\n",
    "# TABELA 1: RESUMO EXECUTIVO POR ANO/UF\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Criando Tabela 1: neac.ecd_resumo_executivo...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE_DESTINO}.ecd_resumo_executivo (\n",
    "        total_contas BIGINT,\n",
    "        total_empresas BIGINT,\n",
    "        total_cnpjs BIGINT,\n",
    "        contas_nivel1 BIGINT,\n",
    "        contas_nivel2 BIGINT,\n",
    "        contas_nivel3 BIGINT,\n",
    "        perc_nivel1 DOUBLE,\n",
    "        perc_nivel2 DOUBLE,\n",
    "        perc_nivel3 DOUBLE,\n",
    "        contas_confianca_muito_alta BIGINT,\n",
    "        contas_confianca_alta BIGINT,\n",
    "        contas_confianca_media BIGINT,\n",
    "        contas_confianca_baixa BIGINT,\n",
    "        empresas_equacao_ok BIGINT,\n",
    "        empresas_equacao_nok BIGINT,\n",
    "        perc_empresas_ok DOUBLE,\n",
    "        contas_para_ml BIGINT,\n",
    "        perc_contas_ml DOUBLE\n",
    "    )\n",
    "    PARTITIONED BY (ano_referencia INT, uf STRING)\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {DATABASE_DESTINO}.ecd_resumo_executivo\n",
    "    PARTITION (ano_referencia = {ANO_REFERENCIA}, uf = '{UF_FILTRO}')\n",
    "    SELECT \n",
    "        COUNT(*) as total_contas,\n",
    "        COUNT(DISTINCT id_ecd) as total_empresas,\n",
    "        COUNT(DISTINCT cnpj) as total_cnpjs,\n",
    "        \n",
    "        SUM(CASE WHEN classificacao_nivel1 IS NOT NULL AND classificacao_nivel1 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as contas_nivel1,\n",
    "        SUM(CASE WHEN classificacao_nivel2 IS NOT NULL AND classificacao_nivel2 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as contas_nivel2,\n",
    "        SUM(CASE WHEN classificacao_nivel3 IS NOT NULL AND classificacao_nivel3 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as contas_nivel3,\n",
    "        \n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel1 IS NOT NULL AND classificacao_nivel1 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_nivel1,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel2 IS NOT NULL AND classificacao_nivel2 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_nivel2,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel3 IS NOT NULL AND classificacao_nivel3 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_nivel3,\n",
    "        \n",
    "        SUM(CASE WHEN confianca_nivel1 = 'MUITO_ALTA' THEN 1 ELSE 0 END) as contas_confianca_muito_alta,\n",
    "        SUM(CASE WHEN confianca_nivel1 = 'ALTA' THEN 1 ELSE 0 END) as contas_confianca_alta,\n",
    "        SUM(CASE WHEN confianca_nivel1 = 'MEDIA' THEN 1 ELSE 0 END) as contas_confianca_media,\n",
    "        SUM(CASE WHEN confianca_nivel1 = 'BAIXA' THEN 1 ELSE 0 END) as contas_confianca_baixa,\n",
    "        \n",
    "        COUNT(DISTINCT CASE WHEN empresa_equacao_ok = TRUE THEN id_ecd END) as empresas_equacao_ok,\n",
    "        COUNT(DISTINCT CASE WHEN empresa_equacao_ok = FALSE THEN id_ecd END) as empresas_equacao_nok,\n",
    "        ROUND(COUNT(DISTINCT CASE WHEN empresa_equacao_ok = TRUE THEN id_ecd END) * 100.0 / COUNT(DISTINCT id_ecd), 2) as perc_empresas_ok,\n",
    "        \n",
    "        SUM(CASE WHEN usar_para_treino_ml = TRUE THEN 1 ELSE 0 END) as contas_para_ml,\n",
    "        ROUND(SUM(CASE WHEN usar_para_treino_ml = TRUE THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_contas_ml\n",
    "        \n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Tabela criada: neac.ecd_resumo_executivo\")\n",
    "\n",
    "# Mostrar resultado\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {DATABASE_DESTINO}.ecd_resumo_executivo\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA} AND uf = '{UF_FILTRO}'\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# ================================================================================\n",
    "# TABELA 2: DETALHAMENTO POR M√âTODO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Criando Tabela 2: neac.ecd_detalhamento_metodo...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE_DESTINO}.ecd_detalhamento_metodo (\n",
    "        metodo STRING,\n",
    "        origem_demonstrativo STRING,\n",
    "        qtd_contas BIGINT,\n",
    "        qtd_empresas BIGINT,\n",
    "        percentual_total DOUBLE,\n",
    "        avg_score DOUBLE\n",
    "    )\n",
    "    PARTITIONED BY (ano_referencia INT, uf STRING)\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {DATABASE_DESTINO}.ecd_detalhamento_metodo\n",
    "    PARTITION (ano_referencia = {ANO_REFERENCIA}, uf = '{UF_FILTRO}')\n",
    "    SELECT \n",
    "        COALESCE(metodo_final, 'NAO_CLASSIFICADO') as metodo,\n",
    "        origem_demonstrativo,\n",
    "        COUNT(*) as qtd_contas,\n",
    "        COUNT(DISTINCT id_ecd) as qtd_empresas,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentual_total,\n",
    "        ROUND(AVG(CASE WHEN score_similaridade IS NOT NULL THEN score_similaridade END), 3) as avg_score\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "    GROUP BY metodo_final, origem_demonstrativo\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Tabela criada: neac.ecd_detalhamento_metodo\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {DATABASE_DESTINO}.ecd_detalhamento_metodo\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA} AND uf = '{UF_FILTRO}'\n",
    "    ORDER BY qtd_contas DESC\n",
    "    LIMIT 20\n",
    "\"\"\").show(truncate=False)\n",
    "\n",
    "# ================================================================================\n",
    "# TABELA 3: TOP CLASSIFICA√á√ïES\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Criando Tabela 3: neac.ecd_top_classificacoes...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE_DESTINO}.ecd_top_classificacoes (\n",
    "        classificacao_nivel1 STRING,\n",
    "        classificacao_nivel2 STRING,\n",
    "        classificacao_nivel3 STRING,\n",
    "        origem_demonstrativo STRING,\n",
    "        qtd_contas BIGINT,\n",
    "        qtd_empresas BIGINT,\n",
    "        percentual DOUBLE,\n",
    "        media_saldo_final DOUBLE\n",
    "    )\n",
    "    PARTITIONED BY (ano_referencia INT, uf STRING)\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {DATABASE_DESTINO}.ecd_top_classificacoes\n",
    "    PARTITION (ano_referencia = {ANO_REFERENCIA}, uf = '{UF_FILTRO}')\n",
    "    SELECT \n",
    "        classificacao_nivel1,\n",
    "        classificacao_nivel2,\n",
    "        classificacao_nivel3,\n",
    "        origem_demonstrativo,\n",
    "        COUNT(*) as qtd_contas,\n",
    "        COUNT(DISTINCT id_ecd) as qtd_empresas,\n",
    "        ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentual,\n",
    "        ROUND(AVG(vl_cta_fin), 2) as media_saldo_final\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "        AND classificacao_nivel2 IS NOT NULL\n",
    "        AND classificacao_nivel2 != 'NAO_CLASSIFICADO'\n",
    "    GROUP BY classificacao_nivel1, classificacao_nivel2, classificacao_nivel3, origem_demonstrativo\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Tabela criada: neac.ecd_top_classificacoes\")\n",
    "\n",
    "# ================================================================================\n",
    "# TABELA 4: EMPRESAS COM EQUA√á√ÉO OK\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Criando Tabela 4: neac.ecd_empresas_equacao_ok...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE_DESTINO}.ecd_empresas_equacao_ok (\n",
    "        id_ecd BIGINT,\n",
    "        cnpj STRING,\n",
    "        nm_empresarial STRING,\n",
    "        erro_percentual DOUBLE,\n",
    "        total_contas BIGINT,\n",
    "        contas_para_ml BIGINT\n",
    "    )\n",
    "    PARTITIONED BY (ano_referencia INT, uf STRING)\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {DATABASE_DESTINO}.ecd_empresas_equacao_ok\n",
    "    PARTITION (ano_referencia = {ANO_REFERENCIA}, uf = '{UF_FILTRO}')\n",
    "    SELECT DISTINCT\n",
    "        id_ecd,\n",
    "        cnpj,\n",
    "        nm_empresarial,\n",
    "        empresa_diff_percentual as erro_percentual,\n",
    "        COUNT(*) OVER (PARTITION BY id_ecd) as total_contas,\n",
    "        SUM(CASE WHEN usar_para_treino_ml = TRUE THEN 1 ELSE 0 END) OVER (PARTITION BY id_ecd) as contas_para_ml\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "        AND empresa_equacao_ok = TRUE\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Tabela criada: neac.ecd_empresas_equacao_ok\")\n",
    "\n",
    "# ================================================================================\n",
    "# TABELA 5: CONTAS PARA ML (Amostra Balanceada)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Criando Tabela 5: neac.ecd_amostra_ml...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE_DESTINO}.ecd_amostra_ml (\n",
    "        id_ecd BIGINT,\n",
    "        cnpj STRING,\n",
    "        cd_conta STRING,\n",
    "        descr_conta STRING,\n",
    "        nivel_conta INT,\n",
    "        origem_demonstrativo STRING,\n",
    "        classificacao_nivel1 STRING,\n",
    "        classificacao_nivel2 STRING,\n",
    "        classificacao_nivel3 STRING,\n",
    "        confianca_nivel1 STRING,\n",
    "        metodo_final STRING,\n",
    "        score_similaridade DOUBLE,\n",
    "        vl_cta_ini DOUBLE,\n",
    "        vl_cta_fin DOUBLE\n",
    "    )\n",
    "    PARTITIONED BY (ano_referencia INT, uf STRING)\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {DATABASE_DESTINO}.ecd_amostra_ml\n",
    "    PARTITION (ano_referencia = {ANO_REFERENCIA}, uf = '{UF_FILTRO}')\n",
    "    WITH ranked AS (\n",
    "        SELECT \n",
    "            *,\n",
    "            ROW_NUMBER() OVER (\n",
    "                PARTITION BY classificacao_nivel2 \n",
    "                ORDER BY RAND()\n",
    "            ) as rn\n",
    "        FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "        WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "            AND usar_para_treino_ml = TRUE\n",
    "            AND classificacao_nivel2 IS NOT NULL\n",
    "    )\n",
    "    SELECT \n",
    "        id_ecd, cnpj, cd_conta, descr_conta, nivel_conta,\n",
    "        origem_demonstrativo, classificacao_nivel1, classificacao_nivel2, classificacao_nivel3,\n",
    "        confianca_nivel1, metodo_final, score_similaridade,\n",
    "        vl_cta_ini, vl_cta_fin\n",
    "    FROM ranked\n",
    "    WHERE rn <= 100\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Tabela criada: neac.ecd_amostra_ml\")\n",
    "\n",
    "# ================================================================================\n",
    "# TABELA 6: CONTAS N√ÉO CLASSIFICADAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Criando Tabela 6: neac.ecd_contas_nao_classificadas...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE_DESTINO}.ecd_contas_nao_classificadas (\n",
    "        id_ecd BIGINT,\n",
    "        cnpj STRING,\n",
    "        nm_empresarial STRING,\n",
    "        cd_conta STRING,\n",
    "        descr_conta STRING,\n",
    "        nivel_conta INT,\n",
    "        origem_demonstrativo STRING,\n",
    "        ind_grp_bal STRING,\n",
    "        ind_grp_dre STRING,\n",
    "        cd_natureza STRING,\n",
    "        vl_cta_fin DOUBLE,\n",
    "        metodo_nivel1 STRING,\n",
    "        cd_conta_sup STRING\n",
    "    )\n",
    "    PARTITIONED BY (ano_referencia INT, uf STRING)\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {DATABASE_DESTINO}.ecd_contas_nao_classificadas\n",
    "    PARTITION (ano_referencia = {ANO_REFERENCIA}, uf = '{UF_FILTRO}')\n",
    "    SELECT \n",
    "        id_ecd, cnpj, nm_empresarial, cd_conta, descr_conta, nivel_conta,\n",
    "        origem_demonstrativo, ind_grp_bal, ind_grp_dre, cd_natureza,\n",
    "        vl_cta_fin, metodo_nivel1, cd_conta_sup\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "        AND (classificacao_nivel2 IS NULL OR classificacao_nivel2 = 'NAO_CLASSIFICADO')\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Tabela criada: neac.ecd_contas_nao_classificadas\")\n",
    "\n",
    "# ================================================================================\n",
    "# TABELA 7: ESTAT√çSTICAS POR EMPRESA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Criando Tabela 7: neac.ecd_stats_por_empresa...\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE_DESTINO}.ecd_stats_por_empresa (\n",
    "        id_ecd BIGINT,\n",
    "        cnpj STRING,\n",
    "        nm_empresarial STRING,\n",
    "        total_contas BIGINT,\n",
    "        contas_bp BIGINT,\n",
    "        contas_dre BIGINT,\n",
    "        contas_classificadas BIGINT,\n",
    "        perc_classificadas DOUBLE,\n",
    "        contas_alta_confianca BIGINT,\n",
    "        contas_treino_ml BIGINT,\n",
    "        equacao_ok BOOLEAN,\n",
    "        erro_percentual DOUBLE\n",
    "    )\n",
    "    PARTITIONED BY (ano_referencia INT, uf STRING)\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {DATABASE_DESTINO}.ecd_stats_por_empresa\n",
    "    PARTITION (ano_referencia = {ANO_REFERENCIA}, uf = '{UF_FILTRO}')\n",
    "    SELECT \n",
    "        id_ecd,\n",
    "        cnpj,\n",
    "        nm_empresarial,\n",
    "        COUNT(*) as total_contas,\n",
    "        SUM(CASE WHEN origem_demonstrativo = 'BP' THEN 1 ELSE 0 END) as contas_bp,\n",
    "        SUM(CASE WHEN origem_demonstrativo = 'DRE' THEN 1 ELSE 0 END) as contas_dre,\n",
    "        SUM(CASE WHEN classificacao_nivel2 IS NOT NULL AND classificacao_nivel2 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) as contas_classificadas,\n",
    "        ROUND(SUM(CASE WHEN classificacao_nivel2 IS NOT NULL AND classificacao_nivel2 != 'NAO_CLASSIFICADO' THEN 1 ELSE 0 END) * 100.0 / COUNT(*), 2) as perc_classificadas,\n",
    "        SUM(CASE WHEN confianca_nivel1 IN ('MUITO_ALTA', 'ALTA') THEN 1 ELSE 0 END) as contas_alta_confianca,\n",
    "        SUM(CASE WHEN usar_para_treino_ml = TRUE THEN 1 ELSE 0 END) as contas_treino_ml,\n",
    "        MAX(empresa_equacao_ok) as equacao_ok,\n",
    "        MAX(empresa_diff_percentual) as erro_percentual\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "    GROUP BY id_ecd, cnpj, nm_empresarial\n",
    "\"\"\")\n",
    "\n",
    "print(\"‚úÖ Tabela criada: neac.ecd_stats_por_empresa\")\n",
    "\n",
    "# ================================================================================\n",
    "# RESUMO DAS TABELAS CRIADAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìÅ TABELAS ANAL√çTICAS CRIADAS NO HIVE:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "tabelas_criadas = [\n",
    "    (\"neac.ecd_resumo_executivo\", \"Resumo executivo geral\"),\n",
    "    (\"neac.ecd_detalhamento_metodo\", \"Detalhamento por m√©todo de classifica√ß√£o\"),\n",
    "    (\"neac.ecd_top_classificacoes\", \"Top classifica√ß√µes mais frequentes\"),\n",
    "    (\"neac.ecd_empresas_equacao_ok\", \"Empresas com equa√ß√£o cont√°bil OK\"),\n",
    "    (\"neac.ecd_amostra_ml\", \"Amostra balanceada para ML\"),\n",
    "    (\"neac.ecd_contas_nao_classificadas\", \"Contas n√£o classificadas\"),\n",
    "    (\"neac.ecd_stats_por_empresa\", \"Estat√≠sticas detalhadas por empresa\")\n",
    "]\n",
    "\n",
    "for i, (tabela, descricao) in enumerate(tabelas_criadas, 1):\n",
    "    print(f\"{i}. {tabela}\")\n",
    "    print(f\"   ‚îî‚îÄ‚îÄ {descricao}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TODAS AS TABELAS ANAL√çTICAS FORAM CRIADAS COM SUCESSO!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nüìä Particionadas por: ano_referencia = {ANO_REFERENCIA}, uf = '{UF_FILTRO}'\")\n",
    "print(f\"üíæ Database: {DATABASE_DESTINO}\")\n",
    "print(\"\\nüéØ Use estas tabelas para:\")\n",
    "print(\"  - An√°lises r√°pidas e dashboards\")\n",
    "print(\"  - Monitoramento da qualidade da classifica√ß√£o\")\n",
    "print(\"  - Identifica√ß√£o de empresas priorit√°rias\")\n",
    "print(\"  - Prepara√ß√£o de datasets para ML\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309203d-9131-4458-a92d-a6e57fdbd5d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 9: PREPARA√á√ÉO DE FEATURES PARA MACHINE LEARNING\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nü§ñ PREPARA√á√ÉO DE FEATURES PARA MACHINE LEARNING...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col, length, regexp_extract, when, lit, split, abs as spark_abs\n",
    "import pandas as pd\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR DADOS DE TREINO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Carregando dados para treino...\")\n",
    "\n",
    "df_treino = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        id_ecd, cnpj, cd_conta, descr_conta, nivel_conta, cd_conta_sup,\n",
    "        origem_demonstrativo, ind_grp_bal, ind_grp_dre, cd_natureza,\n",
    "        tp_conta_agl, tp_conta_pc,\n",
    "        classificacao_nivel1, classificacao_nivel2, classificacao_nivel3,\n",
    "        vl_cta_ini, vl_cta_fin\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "        AND usar_para_treino_ml = TRUE\n",
    "        AND classificacao_nivel2 IS NOT NULL\n",
    "        AND classificacao_nivel2 != 'NAO_CLASSIFICADO'\n",
    "\"\"\")\n",
    "\n",
    "total_treino = df_treino.count()\n",
    "print(f\"‚úÖ Registros para treino: {total_treino:,}\")\n",
    "\n",
    "# Distribui√ß√£o de classes\n",
    "print(\"\\nüìä Distribui√ß√£o de classes (top 20):\")\n",
    "df_treino.groupBy(\"classificacao_nivel2\").count().orderBy(col(\"count\").desc()).show(20)\n",
    "\n",
    "# ================================================================================\n",
    "# FEATURE ENGINEERING\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîß Criando features...\")\n",
    "\n",
    "df_features = df_treino \\\n",
    "    .withColumn(\"tamanho_descricao\", length(col(\"descr_conta\"))) \\\n",
    "    .withColumn(\"tamanho_codigo\", length(col(\"cd_conta\"))) \\\n",
    "    .withColumn(\"primeiro_digito_codigo\", regexp_extract(col(\"cd_conta\"), r\"^(\\d)\", 1)) \\\n",
    "    .withColumn(\"tem_ponto_codigo\", when(col(\"cd_conta\").contains(\".\"), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"tem_hifen_codigo\", when(col(\"cd_conta\").contains(\"-\"), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"tem_underscore_codigo\", when(col(\"cd_conta\").contains(\"_\"), lit(1)).otherwise(lit(0))) \\\n",
    "    .withColumn(\"valor_absoluto_final\", when(col(\"vl_cta_fin\").isNull(), lit(0.0)).otherwise(abs(col(\"vl_cta_fin\")))) \\\n",
    "    .withColumn(\"valor_absoluto_inicial\", when(col(\"vl_cta_ini\").isNull(), lit(0.0)).otherwise(abs(col(\"vl_cta_ini\")))) \\\n",
    "    .withColumn(\"variacao_valor\", col(\"valor_absoluto_final\") - col(\"valor_absoluto_inicial\")) \\\n",
    "    .withColumn(\"tem_conta_superior\", when(col(\"cd_conta_sup\").isNotNull(), lit(1)).otherwise(lit(0)))\n",
    "\n",
    "# Tratar nulos em campos categ√≥ricos\n",
    "df_features = df_features \\\n",
    "    .fillna({\n",
    "        'ind_grp_bal': 'DESCONHECIDO',\n",
    "        'ind_grp_dre': 'DESCONHECIDO',\n",
    "        'cd_natureza': '00',\n",
    "        'tp_conta_agl': 'DESCONHECIDO',\n",
    "        'tp_conta_pc': 'DESCONHECIDO',\n",
    "        'primeiro_digito_codigo': '0'\n",
    "    })\n",
    "\n",
    "# IMPORTANTE: Substituir strings vazias por 'DESCONHECIDO'\n",
    "for col_name in ['ind_grp_bal', 'ind_grp_dre', 'tp_conta_agl', 'tp_conta_pc', \n",
    "                 'cd_natureza', 'primeiro_digito_codigo', 'classificacao_nivel1']:\n",
    "    df_features = df_features.withColumn(\n",
    "        col_name,\n",
    "        when((col(col_name).isNull()) | (col(col_name) == ''), lit('DESCONHECIDO'))\n",
    "        .otherwise(col(col_name))\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Features criadas!\")\n",
    "\n",
    "# ================================================================================\n",
    "# CRIAR FEATURES TEXTUAIS (Palavras-chave)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìù Criando features textuais (presen√ßa de palavras-chave)...\")\n",
    "\n",
    "# Palavras-chave importantes\n",
    "keywords_features = {\n",
    "    'tem_palavra_caixa': ['caixa', 'cash'],\n",
    "    'tem_palavra_banco': ['banco', 'bank'],\n",
    "    'tem_palavra_estoque': ['estoque', 'mercadoria', 'produto'],\n",
    "    'tem_palavra_cliente': ['cliente', 'duplicata', 'receber'],\n",
    "    'tem_palavra_fornecedor': ['fornecedor', 'fornecimento'],\n",
    "    'tem_palavra_salario': ['salario', 'folha', 'ferias'],\n",
    "    'tem_palavra_tributo': ['tributo', 'imposto', 'icms', 'pis', 'cofins'],\n",
    "    'tem_palavra_receita': ['receita', 'venda', 'faturamento'],\n",
    "    'tem_palavra_despesa': ['despesa', 'custo', 'gasto'],\n",
    "    'tem_palavra_financeiro': ['juros', 'financeiro', 'emprestimo'],\n",
    "    'tem_palavra_imobilizado': ['imovel', 'veiculo', 'maquina', 'equipamento'],\n",
    "    'tem_palavra_capital': ['capital', 'social'],\n",
    "    'tem_palavra_lucro': ['lucro', 'prejuizo', 'resultado']\n",
    "}\n",
    "\n",
    "for feature_name, keywords in keywords_features.items():\n",
    "    condicao = lit(False)\n",
    "    for keyword in keywords:\n",
    "        condicao = condicao | col(\"descr_conta\").contains(keyword)\n",
    "    df_features = df_features.withColumn(feature_name, when(condicao, lit(1)).otherwise(lit(0)))\n",
    "\n",
    "print(\"‚úÖ Features textuais criadas!\")\n",
    "\n",
    "# ================================================================================\n",
    "# ENCODING DE VARI√ÅVEIS CATEG√ìRICAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüî¢ Aplicando encoding de vari√°veis categ√≥ricas...\")\n",
    "\n",
    "# Campos categ√≥ricos para encoding\n",
    "categorical_cols = [\n",
    "    'origem_demonstrativo',\n",
    "    'ind_grp_bal',\n",
    "    'ind_grp_dre',\n",
    "    'cd_natureza',\n",
    "    'tp_conta_agl',\n",
    "    'tp_conta_pc',\n",
    "    'primeiro_digito_codigo',\n",
    "    'classificacao_nivel1'\n",
    "]\n",
    "\n",
    "# Criar indexers\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\", handleInvalid=\"keep\")\n",
    "    for col_name in categorical_cols\n",
    "]\n",
    "\n",
    "# Criar encoders (opcional - para algoritmos que se beneficiam disso)\n",
    "encoders = [\n",
    "    OneHotEncoder(inputCol=f\"{col_name}_index\", outputCol=f\"{col_name}_encoded\")\n",
    "    for col_name in categorical_cols\n",
    "]\n",
    "\n",
    "# Pipeline de transforma√ß√£o\n",
    "pipeline_encoding = Pipeline(stages=indexers + encoders)\n",
    "\n",
    "print(\"Aplicando pipeline de encoding...\")\n",
    "model_encoding = pipeline_encoding.fit(df_features)\n",
    "df_encoded = model_encoding.transform(df_features)\n",
    "\n",
    "print(\"‚úÖ Encoding aplicado!\")\n",
    "\n",
    "# ================================================================================\n",
    "# SELECIONAR FEATURES FINAIS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìã Selecionando features finais...\")\n",
    "\n",
    "# Features num√©ricas\n",
    "numeric_features = [\n",
    "    'nivel_conta',\n",
    "    'tamanho_descricao',\n",
    "    'tamanho_codigo',\n",
    "    'tem_ponto_codigo',\n",
    "    'tem_hifen_codigo',\n",
    "    'tem_underscore_codigo',\n",
    "    'valor_absoluto_final',\n",
    "    'valor_absoluto_inicial',\n",
    "    'variacao_valor',\n",
    "    'tem_conta_superior'\n",
    "] + list(keywords_features.keys())\n",
    "\n",
    "# Features categ√≥ricas (encoded)\n",
    "categorical_encoded_features = [f\"{col_name}_encoded\" for col_name in categorical_cols]\n",
    "\n",
    "# Todas as features\n",
    "all_features = numeric_features + categorical_encoded_features\n",
    "\n",
    "print(f\"Total de features: {len(all_features)}\")\n",
    "print(f\"  - Num√©ricas: {len(numeric_features)}\")\n",
    "print(f\"  - Categ√≥ricas (encoded): {len(categorical_encoded_features)}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CRIAR VETOR DE FEATURES\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîó Criando vetor de features...\")\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=all_features,\n",
    "    outputCol=\"features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df_encoded)\n",
    "\n",
    "print(\"‚úÖ Vetor de features criado!\")\n",
    "\n",
    "# ================================================================================\n",
    "# CRIAR LABEL (TARGET)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüéØ Criando label (target)...\")\n",
    "\n",
    "# Indexar a classifica√ß√£o_nivel2 como label\n",
    "label_indexer = StringIndexer(\n",
    "    inputCol=\"classificacao_nivel2\",\n",
    "    outputCol=\"label\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "df_final_ml = label_indexer.fit(df_assembled).transform(df_assembled)\n",
    "\n",
    "print(\"‚úÖ Label criada!\")\n",
    "\n",
    "# Verificar distribui√ß√£o de labels\n",
    "print(\"\\nüìä Distribui√ß√£o de labels (top 20):\")\n",
    "df_final_ml.groupBy(\"label\", \"classificacao_nivel2\").count().orderBy(\"label\").show(20)\n",
    "\n",
    "# ================================================================================\n",
    "# SALVAR DATASET PREPARADO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ Salvando dataset preparado para ML...\")\n",
    "\n",
    "# Selecionar apenas colunas necess√°rias\n",
    "df_ml_final = df_final_ml.select(\n",
    "    \"id_ecd\", \"cnpj\", \"cd_conta\", \"descr_conta\",\n",
    "    \"classificacao_nivel2\", \"label\", \"features\",\n",
    "    *numeric_features,\n",
    "    *[f\"{col_name}_index\" for col_name in categorical_cols]\n",
    ")\n",
    "\n",
    "# Salvar como tabela Hive\n",
    "print(f\"üíæ Salvando como tabela: {DATABASE_DESTINO}.ecd_ml_dataset...\")\n",
    "\n",
    "# Dropar tabela se existir (para evitar conflito de schema)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_dataset\")\n",
    "\n",
    "# Criar tabela diretamente com CTAS (Create Table As Select)\n",
    "df_ml_final.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_DESTINO}.ecd_ml_dataset\")\n",
    "\n",
    "print(f\"‚úÖ Dataset salvo: {DATABASE_DESTINO}.ecd_ml_dataset\")\n",
    "\n",
    "# ================================================================================\n",
    "# ESTAT√çSTICAS FINAIS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä ESTAT√çSTICAS DO DATASET PARA ML:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Criar view tempor√°ria ANTES de usar no SQL\n",
    "df_ml_final.createOrReplaceTempView(\"temp_stats_ml\")\n",
    "\n",
    "# Agora executar a query\n",
    "stats = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total_registros,\n",
    "        COUNT(DISTINCT classificacao_nivel2) as total_classes,\n",
    "        ROUND(AVG(nivel_conta), 2) as avg_nivel_conta,\n",
    "        ROUND(AVG(tamanho_descricao), 2) as avg_tamanho_descricao,\n",
    "        ROUND(AVG(valor_absoluto_final), 2) as avg_valor_final\n",
    "    FROM temp_stats_ml\n",
    "\"\"\")\n",
    "\n",
    "stats.show(truncate=False)\n",
    "\n",
    "# Mostrar mais estat√≠sticas\n",
    "print(\"\\nüìä Total e classes:\")\n",
    "spark.sql(\"SELECT COUNT(*) as total, COUNT(DISTINCT label) as total_classes FROM temp_stats_ml\").show()\n",
    "\n",
    "print(\"\\nüìã Amostra do dataset preparado:\")\n",
    "df_ml_final.select(\"cd_conta\", \"descr_conta\", \"classificacao_nivel2\", \"label\").show(10, truncate=50)\n",
    "\n",
    "# ================================================================================\n",
    "# SPLIT TREINO/TESTE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n‚úÇÔ∏è  Criando split treino/valida√ß√£o/teste...\")\n",
    "\n",
    "# Split: 70% treino, 15% valida√ß√£o, 15% teste\n",
    "train_data, temp_data = df_ml_final.randomSplit([0.7, 0.3], seed=42)\n",
    "val_data, test_data = temp_data.randomSplit([0.5, 0.5], seed=42)\n",
    "\n",
    "print(f\"üìä Treino: {train_data.count():,} registros\")\n",
    "print(f\"üìä Valida√ß√£o: {val_data.count():,} registros\")\n",
    "print(f\"üìä Teste: {test_data.count():,} registros\")\n",
    "\n",
    "# Salvar splits como tabelas Hive\n",
    "print(\"\\nüíæ Salvando splits em tabelas Hive...\")\n",
    "\n",
    "# Dropar tabelas se existirem\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_train\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_val\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_test\")\n",
    "\n",
    "# Salvar diretamente\n",
    "train_data.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_DESTINO}.ecd_ml_train\")\n",
    "val_data.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_DESTINO}.ecd_ml_val\")\n",
    "test_data.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_DESTINO}.ecd_ml_test\")\n",
    "\n",
    "print(f\"‚úÖ Train salvo: {DATABASE_DESTINO}.ecd_ml_train\")\n",
    "print(f\"‚úÖ Val salvo: {DATABASE_DESTINO}.ecd_ml_val\")\n",
    "print(f\"‚úÖ Test salvo: {DATABASE_DESTINO}.ecd_ml_test\")\n",
    "\n",
    "# ================================================================================\n",
    "# EXPORTAR MAPEAMENTO DE LABELS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìÑ Salvando mapeamento de labels...\")\n",
    "\n",
    "# Criar mapeamento label -> classificacao_nivel2\n",
    "label_mapping = df_final_ml.select(\"label\", \"classificacao_nivel2\") \\\n",
    "    .distinct() \\\n",
    "    .orderBy(\"label\") \\\n",
    "    .toPandas()\n",
    "\n",
    "print(\"\\nüìã Mapeamento de labels:\")\n",
    "print(label_mapping)\n",
    "\n",
    "# Salvar como tabela Hive (mais confi√°vel que arquivo local)\n",
    "df_label_mapping = spark.createDataFrame(label_mapping)\n",
    "df_label_mapping.createOrReplaceTempView(\"temp_label_mapping\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS {DATABASE_DESTINO}.ml_label_mapping (\n",
    "        label DOUBLE,\n",
    "        classificacao_nivel2 STRING\n",
    "    )\n",
    "    STORED AS PARQUET\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    INSERT OVERWRITE TABLE {DATABASE_DESTINO}.ml_label_mapping\n",
    "    SELECT * FROM temp_label_mapping\n",
    "\"\"\")\n",
    "\n",
    "print(f\"‚úÖ Mapeamento salvo na tabela: {DATABASE_DESTINO}.ml_label_mapping\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PREPARA√á√ÉO DE FEATURES PARA ML CONCLU√çDA!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüéØ Pr√≥ximos passos:\")\n",
    "print(\"  1. Treinar modelos (Random Forest, XGBoost, etc.)\")\n",
    "print(\"  2. Avaliar performance nos dados de valida√ß√£o\")\n",
    "print(\"  3. Fazer predi√ß√µes nas contas n√£o classificadas\")\n",
    "print(\"  4. Refinar features e retreinar se necess√°rio\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9ed7878-cf06-4e4c-80b6-2e257213337e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 10: TREINAR MODELOS DE MACHINE LEARNING\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nü§ñ TREINAMENTO DE MODELOS DE MACHINE LEARNING...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier, GBTClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.functions import col\n",
    "import time\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR DADOS DE TREINO E VALIDA√á√ÉO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Carregando datasets...\")\n",
    "\n",
    "try:\n",
    "    df_train = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_train\")\n",
    "    df_val = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_val\")\n",
    "    df_test = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_test\")\n",
    "    \n",
    "    print(f\"‚úÖ Treino: {df_train.count():,} registros\")\n",
    "    print(f\"‚úÖ Valida√ß√£o: {df_val.count():,} registros\")\n",
    "    print(f\"‚úÖ Teste: {df_test.count():,} registros\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO ao carregar datasets: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Execute a C√âLULA 9 primeiro para preparar os dados!\")\n",
    "    raise\n",
    "\n",
    "# Verificar distribui√ß√£o de classes\n",
    "print(\"\\nüìä Distribui√ß√£o de classes no dataset de treino (Top 20):\")\n",
    "df_train.groupBy(\"label\", \"classificacao_nivel2\").count().orderBy(\"count\", ascending=False).show(20)\n",
    "\n",
    "# ================================================================================\n",
    "# MODELO 1: RANDOM FOREST\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüå≤ MODELO 1: RANDOM FOREST CLASSIFIER\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "print(\"Configurando Random Forest...\")\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"rf_prediction\",\n",
    "    probabilityCol=\"rf_probability\",\n",
    "    rawPredictionCol=\"rf_raw_prediction\",\n",
    "    # Hiperpar√¢metros otimizados\n",
    "    numTrees=100,              # N√∫mero de √°rvores\n",
    "    maxDepth=10,               # Profundidade m√°xima\n",
    "    maxBins=32,                # Bins para features categ√≥ricas\n",
    "    minInstancesPerNode=10,    # M√≠nimo de inst√¢ncias por n√≥\n",
    "    subsamplingRate=0.8,       # Taxa de subamostragem\n",
    "    featureSubsetStrategy=\"sqrt\",  # Estrat√©gia de sele√ß√£o de features\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "print(\"Treinando Random Forest...\")\n",
    "print(\"‚è≥ Isso pode levar 5-15 minutos dependendo do volume de dados...\")\n",
    "inicio_rf = time.time()\n",
    "\n",
    "rf_model = rf.fit(df_train)\n",
    "\n",
    "tempo_rf = time.time() - inicio_rf\n",
    "print(f\"‚úÖ Random Forest treinado em {tempo_rf:.2f} segundos ({tempo_rf/60:.2f} minutos)\")\n",
    "\n",
    "# Feature Importance\n",
    "print(\"\\nüìä Feature Importance (Random Forest) - Top 20:\")\n",
    "feature_importance_rf = list(zip(\n",
    "    range(len(rf_model.featureImportances)),\n",
    "    rf_model.featureImportances.toArray()\n",
    "))\n",
    "feature_importance_rf.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "for idx, importance in feature_importance_rf[:20]:\n",
    "    print(f\"  Feature {idx}: {importance:.6f}\")\n",
    "\n",
    "# Fazer predi√ß√µes no conjunto de valida√ß√£o\n",
    "print(\"\\nüîÆ Fazendo predi√ß√µes no conjunto de valida√ß√£o...\")\n",
    "df_val_rf = rf_model.transform(df_val)\n",
    "\n",
    "# ================================================================================\n",
    "# AVALIA√á√ÉO R√ÅPIDA - RANDOM FOREST\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AVALIA√á√ÉO R√ÅPIDA - RANDOM FOREST:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"rf_prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"rf_prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"rf_prediction\",\n",
    "    metricName=\"weightedPrecision\"\n",
    ")\n",
    "\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"rf_prediction\",\n",
    "    metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "accuracy_rf = evaluator_accuracy.evaluate(df_val_rf)\n",
    "f1_rf = evaluator_f1.evaluate(df_val_rf)\n",
    "precision_rf = evaluator_precision.evaluate(df_val_rf)\n",
    "recall_rf = evaluator_recall.evaluate(df_val_rf)\n",
    "\n",
    "print(f\"  Accuracy:  {accuracy_rf:.4f} ({accuracy_rf*100:.2f}%)\")\n",
    "print(f\"  F1-Score:  {f1_rf:.4f}\")\n",
    "print(f\"  Precision: {precision_rf:.4f}\")\n",
    "print(f\"  Recall:    {recall_rf:.4f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# MODELO 2: ALTERNATIVA - LOGISTIC REGRESSION (MULTICLASSE)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä MODELO 2: LOGISTIC REGRESSION (One-vs-Rest)\")\n",
    "print(\"-\" * 80)\n",
    "print(\"‚ÑπÔ∏è  GBT nativo do Spark ML s√≥ suporta classifica√ß√£o bin√°ria.\")\n",
    "print(\"‚ÑπÔ∏è  Usando Logistic Regression com estrat√©gia One-vs-Rest para multiclasse.\")\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression, OneVsRest\n",
    "\n",
    "print(\"\\nConfigurando Logistic Regression...\")\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    maxIter=10,\n",
    "    regParam=0.01,\n",
    "    elasticNetParam=0.0,\n",
    "    family=\"multinomial\"  # Suporta multiclasse diretamente\n",
    ")\n",
    "\n",
    "print(\"Treinando Logistic Regression...\")\n",
    "print(\"‚è≥ Isso pode levar 5-10 minutos...\")\n",
    "inicio_lr = time.time()\n",
    "\n",
    "lr_model = lr.fit(df_train)\n",
    "\n",
    "tempo_lr = time.time() - inicio_lr\n",
    "print(f\"‚úÖ Logistic Regression treinado em {tempo_lr:.2f} segundos ({tempo_lr/60:.2f} minutos)\")\n",
    "\n",
    "# Fazer predi√ß√µes no conjunto de valida√ß√£o\n",
    "print(\"\\nüîÆ Fazendo predi√ß√µes no conjunto de valida√ß√£o...\")\n",
    "df_val_lr = lr_model.transform(df_val)\n",
    "\n",
    "# ================================================================================\n",
    "# AVALIA√á√ÉO R√ÅPIDA - LOGISTIC REGRESSION\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AVALIA√á√ÉO R√ÅPIDA - LOGISTIC REGRESSION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "evaluator_accuracy_lr = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "\n",
    "evaluator_f1_lr = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    metricName=\"f1\"\n",
    ")\n",
    "\n",
    "accuracy_lr = evaluator_accuracy_lr.evaluate(df_val_lr)\n",
    "f1_lr = evaluator_f1_lr.evaluate(df_val_lr)\n",
    "\n",
    "print(f\"  Accuracy:  {accuracy_lr:.4f} ({accuracy_lr*100:.2f}%)\")\n",
    "print(f\"  F1-Score:  {f1_lr:.4f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# SALVAR MODELOS EM HDFS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ Salvando modelos treinados...\")\n",
    "\n",
    "# Criar diret√≥rio de modelos se n√£o existir\n",
    "try:\n",
    "    # Salvar Random Forest\n",
    "    rf_path = f\"/user/{spark.sparkContext.sparkUser()}/models/rf_{UF_FILTRO}_{ANO_REFERENCIA}\"\n",
    "    rf_model.write().overwrite().save(rf_path)\n",
    "    print(f\"‚úÖ Random Forest salvo: {rf_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Aviso ao salvar Random Forest: {e}\")\n",
    "    print(\"   Modelo permanece em mem√≥ria para uso nesta sess√£o\")\n",
    "\n",
    "try:\n",
    "    # Salvar Logistic Regression\n",
    "    lr_path = f\"/user/{spark.sparkContext.sparkUser()}/models/lr_{UF_FILTRO}_{ANO_REFERENCIA}\"\n",
    "    lr_model.write().overwrite().save(lr_path)\n",
    "    print(f\"‚úÖ Logistic Regression salvo: {lr_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Aviso ao salvar Logistic Regression: {e}\")\n",
    "    print(\"   Modelo permanece em mem√≥ria para uso nesta sess√£o\")\n",
    "\n",
    "# ================================================================================\n",
    "# SALVAR PREDI√á√ïES EM TABELAS HIVE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ Salvando predi√ß√µes do conjunto de valida√ß√£o em tabelas Hive...\")\n",
    "\n",
    "# Preparar predi√ß√µes RF\n",
    "df_val_rf_final = df_val_rf.select(\n",
    "    \"id_ecd\", \"cnpj\", \"cd_conta\", \"descr_conta\", \n",
    "    \"classificacao_nivel2\", \"label\", \"rf_prediction\"\n",
    ")\n",
    "\n",
    "# Salvar RF\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_predictions_rf_val\")\n",
    "df_val_rf_final.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_DESTINO}.ecd_ml_predictions_rf_val\")\n",
    "print(f\"‚úÖ Predi√ß√µes RF salvas: {DATABASE_DESTINO}.ecd_ml_predictions_rf_val\")\n",
    "\n",
    "# Preparar predi√ß√µes LR\n",
    "df_val_lr_final = df_val_lr.select(\n",
    "    \"id_ecd\", \"cnpj\", \"cd_conta\", \"descr_conta\", \n",
    "    \"classificacao_nivel2\", \"label\", \n",
    "    col(\"prediction\").alias(\"lr_prediction\")\n",
    ")\n",
    "\n",
    "# Salvar LR\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_predictions_lr_val\")\n",
    "df_val_lr_final.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_DESTINO}.ecd_ml_predictions_lr_val\")\n",
    "print(f\"‚úÖ Predi√ß√µes LR salvas: {DATABASE_DESTINO}.ecd_ml_predictions_lr_val\")\n",
    "\n",
    "# ================================================================================\n",
    "# SALVAR M√âTRICAS EM TABELA HIVE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ Salvando m√©tricas dos modelos...\")\n",
    "\n",
    "# Criar DataFrame com m√©tricas\n",
    "from pyspark.sql import Row\n",
    "\n",
    "metricas_data = [\n",
    "    Row(\n",
    "        modelo=\"RandomForest\",\n",
    "        accuracy=float(accuracy_rf),\n",
    "        f1_score=float(f1_rf),\n",
    "        precision=float(precision_rf),\n",
    "        recall=float(recall_rf),\n",
    "        tempo_treino_segundos=float(tempo_rf),\n",
    "        num_trees=rf.getNumTrees(),\n",
    "        max_depth=rf.getMaxDepth(),\n",
    "        ano_referencia=ANO_REFERENCIA,\n",
    "        uf=UF_FILTRO\n",
    "    ),\n",
    "    Row(\n",
    "        modelo=\"LogisticRegression\",\n",
    "        accuracy=float(accuracy_lr),\n",
    "        f1_score=float(f1_lr),\n",
    "        precision=None,\n",
    "        recall=None,\n",
    "        tempo_treino_segundos=float(tempo_lr),\n",
    "        num_trees=None,\n",
    "        max_depth=None,\n",
    "        ano_referencia=ANO_REFERENCIA,\n",
    "        uf=UF_FILTRO\n",
    "    )\n",
    "]\n",
    "\n",
    "df_metricas = spark.createDataFrame(metricas_data)\n",
    "\n",
    "# Salvar m√©tricas\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_metricas\")\n",
    "df_metricas.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_DESTINO}.ecd_ml_metricas\")\n",
    "print(f\"‚úÖ M√©tricas salvas: {DATABASE_DESTINO}.ecd_ml_metricas\")\n",
    "\n",
    "# ================================================================================\n",
    "# ESTAT√çSTICAS FINAIS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä RESUMO DO TREINAMENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüå≤ Random Forest:\")\n",
    "print(f\"  - √Årvores: {rf.getNumTrees()}\")\n",
    "print(f\"  - Profundidade m√°xima: {rf.getMaxDepth()}\")\n",
    "print(f\"  - Tempo de treino: {tempo_rf/60:.2f} minutos\")\n",
    "print(f\"  - Accuracy: {accuracy_rf*100:.2f}%\")\n",
    "print(f\"  - F1-Score: {f1_rf:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä Logistic Regression:\")\n",
    "print(f\"  - Max Iterations: {lr.getMaxIter()}\")\n",
    "print(f\"  - Regularization: {lr.getRegParam()}\")\n",
    "print(f\"  - Tempo de treino: {tempo_lr/60:.2f} minutos\")\n",
    "print(f\"  - Accuracy: {accuracy_lr*100:.2f}%\")\n",
    "print(f\"  - F1-Score: {f1_lr:.4f}\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Tempo total: {(tempo_rf + tempo_lr)/60:.2f} minutos\")\n",
    "\n",
    "# Melhor modelo\n",
    "melhor_modelo = \"Random Forest\" if accuracy_rf > accuracy_lr else \"Logistic Regression\"\n",
    "melhor_accuracy = accuracy_rf if accuracy_rf > accuracy_lr else accuracy_lr\n",
    "print(f\"\\nüèÜ Melhor modelo: {melhor_modelo} (Accuracy: {melhor_accuracy*100:.2f}%)\")\n",
    "\n",
    "print(\"\\nüìä Tabelas criadas:\")\n",
    "print(f\"  - {DATABASE_DESTINO}.ecd_ml_predictions_rf_val\")\n",
    "print(f\"  - {DATABASE_DESTINO}.ecd_ml_predictions_lr_val\")\n",
    "print(f\"  - {DATABASE_DESTINO}.ecd_ml_metricas\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TREINAMENTO CONCLU√çDO!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüéØ Pr√≥ximo passo: Execute C√âLULA 11 para an√°lise detalhada\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c1ed60-4ef4-4d53-9cdd-ef655f765acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 11: AVALIAR PERFORMANCE DOS MODELOS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìà AVALIA√á√ÉO DETALHADA DE PERFORMANCE DOS MODELOS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when, count, sum as spark_sum\n",
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR PREDI√á√ïES DAS TABELAS HIVE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Carregando predi√ß√µes das tabelas Hive...\")\n",
    "\n",
    "try:\n",
    "    # Carregar predi√ß√µes salvas\n",
    "    df_val_rf = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_predictions_rf_val\")\n",
    "    \n",
    "    # Carregar m√©tricas b√°sicas\n",
    "    df_metricas = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_metricas\")\n",
    "    \n",
    "    print(\"‚úÖ Predi√ß√µes e m√©tricas carregadas!\")\n",
    "    \n",
    "    # Mostrar m√©tricas b√°sicas\n",
    "    print(\"\\nüìä M√©tricas B√°sicas (j√° calculadas):\")\n",
    "    df_metricas.show(truncate=False)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO ao carregar dados: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Execute a C√âLULA 10 primeiro para treinar os modelos!\")\n",
    "    raise\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR MAPEAMENTO DE LABELS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Carregando mapeamento de labels...\")\n",
    "\n",
    "label_mapping = spark.table(f\"{DATABASE_DESTINO}.ml_label_mapping\").toPandas()\n",
    "print(f\"‚úÖ {len(label_mapping)} labels carregados\")\n",
    "\n",
    "# Criar dicion√°rio de mapeamento\n",
    "label_to_class = dict(zip(label_mapping['label'], label_mapping['classificacao_nivel2']))\n",
    "\n",
    "print(\"\\nüìã Amostra do mapeamento (primeiras 10 classes):\")\n",
    "print(label_mapping.head(10))\n",
    "\n",
    "# ================================================================================\n",
    "# M√âTRICAS DETALHADAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä EXTRAINDO M√âTRICAS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Converter m√©tricas para Pandas\n",
    "metricas_pd = df_metricas.toPandas()\n",
    "\n",
    "# Extrair Random Forest\n",
    "rf_row = metricas_pd[metricas_pd['modelo'] == 'RandomForest'].iloc[0]\n",
    "rf_accuracy = rf_row['accuracy']\n",
    "rf_f1 = rf_row['f1_score']\n",
    "rf_precision = rf_row['precision'] if 'precision' in rf_row and rf_row['precision'] is not None else None\n",
    "rf_recall = rf_row['recall'] if 'recall' in rf_row and rf_row['recall'] is not None else None\n",
    "\n",
    "print(\"\\nüå≤ RANDOM FOREST:\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"  Accuracy:  {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
    "print(f\"  F1-Score:  {rf_f1:.4f}\")\n",
    "if rf_precision:\n",
    "    print(f\"  Precision: {rf_precision:.4f}\")\n",
    "if rf_recall:\n",
    "    print(f\"  Recall:    {rf_recall:.4f}\")\n",
    "\n",
    "# Extrair Logistic Regression se existir\n",
    "lr_existe = False\n",
    "try:\n",
    "    lr_row = metricas_pd[metricas_pd['modelo'] == 'LogisticRegression'].iloc[0]\n",
    "    lr_accuracy = lr_row['accuracy']\n",
    "    lr_f1 = lr_row['f1_score']\n",
    "    lr_existe = True\n",
    "    \n",
    "    print(\"\\nüìä LOGISTIC REGRESSION:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"  Accuracy:  {lr_accuracy:.4f} ({lr_accuracy*100:.2f}%)\")\n",
    "    print(f\"  F1-Score:  {lr_f1:.4f}\")\n",
    "except:\n",
    "    print(\"\\n‚ÑπÔ∏è  Logistic Regression n√£o encontrado (apenas RF dispon√≠vel)\")\n",
    "\n",
    "# ================================================================================\n",
    "# GR√ÅFICO DE COMPARA√á√ÉO DE MODELOS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Gerando gr√°fico de compara√ß√£o...\")\n",
    "\n",
    "# Criar DataFrame de compara√ß√£o\n",
    "if lr_existe:\n",
    "    metricas_comparacao = pd.DataFrame({\n",
    "        'Modelo': ['Random Forest', 'Logistic Regression'],\n",
    "        'Accuracy': [rf_accuracy, lr_accuracy],\n",
    "        'F1-Score': [rf_f1, lr_f1]\n",
    "    })\n",
    "    cores = ['#2ecc71', '#3498db']\n",
    "else:\n",
    "    metricas_comparacao = pd.DataFrame({\n",
    "        'Modelo': ['Random Forest'],\n",
    "        'Accuracy': [rf_accuracy],\n",
    "        'F1-Score': [rf_f1]\n",
    "    })\n",
    "    cores = ['#2ecc71']\n",
    "\n",
    "print(metricas_comparacao)\n",
    "\n",
    "# Gr√°fico de barras\n",
    "fig_comparacao = go.Figure()\n",
    "\n",
    "for idx, modelo in enumerate(metricas_comparacao['Modelo']):\n",
    "    row = metricas_comparacao[metricas_comparacao['Modelo'] == modelo].iloc[0]\n",
    "    valores = [row['Accuracy'], row['F1-Score']]\n",
    "    \n",
    "    fig_comparacao.add_trace(go.Bar(\n",
    "        name=modelo,\n",
    "        x=['Accuracy', 'F1-Score'],\n",
    "        y=valores,\n",
    "        text=[f\"{v:.2%}\" for v in valores],\n",
    "        textposition='auto',\n",
    "        marker_color=cores[idx]\n",
    "    ))\n",
    "\n",
    "fig_comparacao.update_layout(\n",
    "    title=f\"Performance dos Modelos - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "    yaxis_title=\"Score\",\n",
    "    yaxis_range=[0, 1.05],\n",
    "    barmode='group',\n",
    "    height=500\n",
    ")\n",
    "fig_comparacao.show()\n",
    "\n",
    "# ================================================================================\n",
    "# MATRIZ DE CONFUS√ÉO (RANDOM FOREST)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä CALCULANDO MATRIZ DE CONFUS√ÉO (Random Forest)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calcular matriz de confus√£o\n",
    "confusion_matrix_data = df_val_rf.groupBy(\"label\", \"rf_prediction\").count().toPandas()\n",
    "\n",
    "# Pegar top 15 classes mais frequentes\n",
    "top_classes = df_val_rf.groupBy(\"label\").count().orderBy(col(\"count\").desc()).limit(15).toPandas()\n",
    "top_labels = top_classes['label'].tolist()\n",
    "\n",
    "print(f\"Analisando top {len(top_labels)} classes...\")\n",
    "\n",
    "# Filtrar matriz\n",
    "confusion_filtered = confusion_matrix_data[\n",
    "    confusion_matrix_data['label'].isin(top_labels) & \n",
    "    confusion_matrix_data['rf_prediction'].isin(top_labels)\n",
    "]\n",
    "\n",
    "# Criar pivot\n",
    "confusion_pivot = confusion_filtered.pivot_table(\n",
    "    index='label', \n",
    "    columns='rf_prediction', \n",
    "    values='count', \n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Normalizar\n",
    "confusion_normalized = confusion_pivot.div(confusion_pivot.sum(axis=1), axis=0)\n",
    "\n",
    "# Mapear labels\n",
    "index_labels = [label_to_class.get(l, f\"Label_{l}\") for l in confusion_normalized.index]\n",
    "column_labels = [label_to_class.get(l, f\"Label_{l}\") for l in confusion_normalized.columns]\n",
    "\n",
    "# Heatmap\n",
    "fig_confusion = px.imshow(\n",
    "    confusion_normalized.values,\n",
    "    x=column_labels,\n",
    "    y=index_labels,\n",
    "    labels=dict(x=\"Predi√ß√£o\", y=\"Real\", color=\"% Predito\"),\n",
    "    title=f\"Matriz de Confus√£o (Top {len(top_labels)} Classes) - Random Forest\",\n",
    "    color_continuous_scale=\"Blues\",\n",
    "    height=800,\n",
    "    width=900,\n",
    "    text_auto='.2%'\n",
    ")\n",
    "\n",
    "fig_confusion.update_layout(xaxis_tickangle=-45, font=dict(size=10))\n",
    "fig_confusion.show()\n",
    "\n",
    "# ================================================================================\n",
    "# PERFORMANCE POR CLASSE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä CALCULANDO PERFORMANCE POR CLASSE...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "performance_por_classe_spark = df_val_rf.groupBy(\"label\", \"classificacao_nivel2\").agg(\n",
    "    count(\"*\").alias(\"total\"),\n",
    "    spark_sum(when(col(\"label\") == col(\"rf_prediction\"), 1).otherwise(0)).alias(\"corretos\")\n",
    ").withColumn(\n",
    "    \"accuracy_classe\",\n",
    "    col(\"corretos\") / col(\"total\")\n",
    ").orderBy(col(\"total\").desc())\n",
    "\n",
    "performance_por_classe = performance_por_classe_spark.limit(30).toPandas()\n",
    "\n",
    "print(\"\\nüìã Top 30 Classes por Volume:\")\n",
    "print(performance_por_classe)\n",
    "\n",
    "# Salvar\n",
    "print(\"\\nüíæ Salvando performance por classe...\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_performance_por_classe\")\n",
    "performance_por_classe_spark.write.mode(\"overwrite\").saveAsTable(\n",
    "    f\"{DATABASE_DESTINO}.ecd_ml_performance_por_classe\"\n",
    ")\n",
    "print(f\"‚úÖ Salvo: {DATABASE_DESTINO}.ecd_ml_performance_por_classe\")\n",
    "\n",
    "# Gr√°fico\n",
    "performance_plot = performance_por_classe.head(25)\n",
    "\n",
    "fig_classe = go.Figure()\n",
    "fig_classe.add_trace(go.Bar(\n",
    "    x=performance_plot['classificacao_nivel2'],\n",
    "    y=performance_plot['accuracy_classe'],\n",
    "    text=performance_plot['accuracy_classe'].apply(lambda x: f\"{x:.1%}\"),\n",
    "    textposition='auto',\n",
    "    marker_color=performance_plot['accuracy_classe'].apply(\n",
    "        lambda x: '#27ae60' if x >= 0.9 else '#f39c12' if x >= 0.7 else '#e74c3c'\n",
    "    ),\n",
    "    hovertemplate='<b>%{x}</b><br>Accuracy: %{y:.2%}<br>Total: %{customdata}<extra></extra>',\n",
    "    customdata=performance_plot['total']\n",
    "))\n",
    "\n",
    "fig_classe.update_layout(\n",
    "    title=f\"Accuracy por Classe (Top 25) - Random Forest\",\n",
    "    xaxis_title=\"Classifica√ß√£o\",\n",
    "    yaxis_title=\"Accuracy\",\n",
    "    yaxis_range=[0, 1.05],\n",
    "    height=600,\n",
    "    xaxis_tickangle=-45,\n",
    "    showlegend=False\n",
    ")\n",
    "fig_classe.show()\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE DE ERROS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n‚ùå AN√ÅLISE DE ERROS...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "erros = df_val_rf.filter(col(\"label\") != col(\"rf_prediction\"))\n",
    "total_erros = erros.count()\n",
    "total_val = df_val_rf.count()\n",
    "\n",
    "print(f\"Total de erros: {total_erros:,} / {total_val:,} ({total_erros*100/total_val:.2f}%)\")\n",
    "print(f\"Taxa de acerto: {(1 - total_erros/total_val)*100:.2f}%\")\n",
    "\n",
    "# Top erros\n",
    "print(\"\\nüîù Top 10 Pares de Erros Mais Comuns:\")\n",
    "erros_comuns = erros.groupBy(\"label\", \"rf_prediction\", \"classificacao_nivel2\").count() \\\n",
    "    .orderBy(col(\"count\").desc()).limit(10)\n",
    "erros_comuns_df = erros_comuns.toPandas()\n",
    "print(erros_comuns_df)\n",
    "\n",
    "# Salvar erros\n",
    "print(\"\\nüíæ Salvando erros...\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_erros_rf\")\n",
    "erros.select(\n",
    "    \"id_ecd\", \"cnpj\", \"cd_conta\", \"descr_conta\",\n",
    "    \"label\", \"classificacao_nivel2\", \"rf_prediction\"\n",
    ").write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_DESTINO}.ecd_ml_erros_rf\")\n",
    "print(f\"‚úÖ Salvo: {DATABASE_DESTINO}.ecd_ml_erros_rf ({total_erros:,} registros)\")\n",
    "\n",
    "# Amostra\n",
    "print(\"\\nüìã Amostra de Erros:\")\n",
    "erros.select(\"cd_conta\", \"descr_conta\", \"classificacao_nivel2\", \"rf_prediction\").show(20, truncate=50)\n",
    "\n",
    "# ================================================================================\n",
    "# RESUMO FINAL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä RESUMO DA AVALIA√á√ÉO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüèÜ MODELO PRINCIPAL: Random Forest\")\n",
    "print(f\"   Accuracy: {rf_accuracy:.4f} ({rf_accuracy*100:.2f}%)\")\n",
    "print(f\"   F1-Score: {rf_f1:.4f}\")\n",
    "\n",
    "print(f\"\\nüìä ESTAT√çSTICAS:\")\n",
    "print(f\"   Total valida√ß√£o: {total_val:,} contas\")\n",
    "print(f\"   Acertos: {total_val - total_erros:,} ({(1-total_erros/total_val)*100:.2f}%)\")\n",
    "print(f\"   Erros: {total_erros:,} ({total_erros*100/total_val:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüìä CLASSES:\")\n",
    "print(f\"   Total de classes: {len(label_mapping)}\")\n",
    "\n",
    "# Top/Bottom performers\n",
    "top_performers = performance_por_classe.nlargest(5, 'accuracy_classe')\n",
    "print(f\"\\nüåü TOP 5 CLASSES (Melhor Accuracy):\")\n",
    "for idx, row in top_performers.iterrows():\n",
    "    print(f\"   {row['classificacao_nivel2']}: {row['accuracy_classe']:.2%} ({row['total']:,} contas)\")\n",
    "\n",
    "bottom_performers = performance_por_classe.nsmallest(5, 'accuracy_classe')\n",
    "print(f\"\\n‚ö†Ô∏è  TOP 5 CLASSES (Pior Accuracy):\")\n",
    "for idx, row in bottom_performers.iterrows():\n",
    "    print(f\"   {row['classificacao_nivel2']}: {row['accuracy_classe']:.2%} ({row['total']:,} contas)\")\n",
    "\n",
    "print(f\"\\nüìÇ TABELAS CRIADAS:\")\n",
    "print(f\"   - {DATABASE_DESTINO}.ecd_ml_performance_por_classe\")\n",
    "print(f\"   - {DATABASE_DESTINO}.ecd_ml_erros_rf\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ AVALIA√á√ÉO CONCLU√çDA!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüéØ Modelo pronto para classificar contas n√£o classificadas!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "506a8421-146d-4463-82cd-7f929936d9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 12: APLICAR MODELO NAS CONTAS N√ÉO CLASSIFICADAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîÆ APLICANDO MODELO NAS CONTAS N√ÉO CLASSIFICADAS...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import col, length, regexp_extract, when, lit, abs as spark_abs, split, udf, coalesce\n",
    "from pyspark.sql.types import StringType, DoubleType\n",
    "import pandas as pd\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR MODELO RANDOM FOREST\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Carregando modelo Random Forest...\")\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "try:\n",
    "    # Tentar carregar do HDFS\n",
    "    rf_path = f\"/user/{spark.sparkContext.sparkUser()}/models/rf_{UF_FILTRO}_{ANO_REFERENCIA}\"\n",
    "    rf_model = RandomForestClassificationModel.load(rf_path)\n",
    "    print(f\"‚úÖ Modelo carregado de: {rf_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Erro ao carregar modelo: {e}\")\n",
    "    print(\"‚ùå Execute a C√âLULA 10 primeiro para treinar o modelo!\")\n",
    "    raise\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR CONTAS N√ÉO CLASSIFICADAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Carregando contas n√£o classificadas...\")\n",
    "\n",
    "df_nao_classificadas = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        id_ecd, cnpj, cd_conta, descr_conta, nivel_conta, cd_conta_sup,\n",
    "        origem_demonstrativo, ind_grp_bal, ind_grp_dre, cd_natureza,\n",
    "        tp_conta_agl, tp_conta_pc,\n",
    "        classificacao_nivel1,\n",
    "        vl_cta_ini, vl_cta_fin\n",
    "    FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "    WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "        AND (classificacao_nivel2 IS NULL \n",
    "             OR classificacao_nivel2 = '' \n",
    "             OR classificacao_nivel2 = 'NAO_CLASSIFICADO')\n",
    "\"\"\")\n",
    "\n",
    "total_nao_classificadas = df_nao_classificadas.count()\n",
    "print(f\"‚úÖ Contas n√£o classificadas: {total_nao_classificadas:,}\")\n",
    "\n",
    "if total_nao_classificadas == 0:\n",
    "    print(\"\\nüéâ Todas as contas j√° est√£o classificadas! Nada a fazer.\")\n",
    "    print(\"=\" * 80)\n",
    "else:\n",
    "    # ================================================================================\n",
    "    # APLICAR FEATURE ENGINEERING (MESMA L√ìGICA DA C√âLULA 9)\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüîß Aplicando feature engineering...\")\n",
    "    print(\"‚è≥ Processando features para {:,} contas...\".format(total_nao_classificadas))\n",
    "    \n",
    "    df_features = df_nao_classificadas \\\n",
    "        .withColumn(\"tamanho_descricao\", length(col(\"descr_conta\"))) \\\n",
    "        .withColumn(\"tamanho_codigo\", length(col(\"cd_conta\"))) \\\n",
    "        .withColumn(\"primeiro_digito_codigo\", regexp_extract(col(\"cd_conta\"), r\"^(\\d)\", 1)) \\\n",
    "        .withColumn(\"tem_ponto_codigo\", when(col(\"cd_conta\").contains(\".\"), lit(1)).otherwise(lit(0))) \\\n",
    "        .withColumn(\"tem_hifen_codigo\", when(col(\"cd_conta\").contains(\"-\"), lit(1)).otherwise(lit(0))) \\\n",
    "        .withColumn(\"tem_underscore_codigo\", when(col(\"cd_conta\").contains(\"_\"), lit(1)).otherwise(lit(0))) \\\n",
    "        .withColumn(\"valor_absoluto_final\", when(col(\"vl_cta_fin\").isNull(), lit(0.0)).otherwise(spark_abs(col(\"vl_cta_fin\")))) \\\n",
    "        .withColumn(\"valor_absoluto_inicial\", when(col(\"vl_cta_ini\").isNull(), lit(0.0)).otherwise(spark_abs(col(\"vl_cta_ini\")))) \\\n",
    "        .withColumn(\"variacao_valor\", col(\"valor_absoluto_final\") - col(\"valor_absoluto_inicial\")) \\\n",
    "        .withColumn(\"tem_conta_superior\", when(col(\"cd_conta_sup\").isNotNull(), lit(1)).otherwise(lit(0)))\n",
    "    \n",
    "    # Tratar nulos e strings vazias\n",
    "    for col_name in ['ind_grp_bal', 'ind_grp_dre', 'tp_conta_agl', 'tp_conta_pc', \n",
    "                     'cd_natureza', 'primeiro_digito_codigo', 'classificacao_nivel1']:\n",
    "        df_features = df_features.withColumn(\n",
    "            col_name,\n",
    "            when((col(col_name).isNull()) | (col(col_name) == ''), lit('DESCONHECIDO'))\n",
    "            .otherwise(col(col_name))\n",
    "        )\n",
    "    \n",
    "    # Features textuais (keywords)\n",
    "    keywords_features = {\n",
    "        'tem_palavra_caixa': ['caixa', 'cash'],\n",
    "        'tem_palavra_banco': ['banco', 'bank'],\n",
    "        'tem_palavra_estoque': ['estoque', 'mercadoria', 'produto'],\n",
    "        'tem_palavra_cliente': ['cliente', 'duplicata', 'receber'],\n",
    "        'tem_palavra_fornecedor': ['fornecedor', 'fornecimento'],\n",
    "        'tem_palavra_salario': ['salario', 'folha', 'ferias'],\n",
    "        'tem_palavra_tributo': ['tributo', 'imposto', 'icms', 'pis', 'cofins'],\n",
    "        'tem_palavra_receita': ['receita', 'venda', 'faturamento'],\n",
    "        'tem_palavra_despesa': ['despesa', 'custo', 'gasto'],\n",
    "        'tem_palavra_financeiro': ['juros', 'financeiro', 'emprestimo'],\n",
    "        'tem_palavra_imobilizado': ['imovel', 'veiculo', 'maquina', 'equipamento'],\n",
    "        'tem_palavra_capital': ['capital', 'social'],\n",
    "        'tem_palavra_lucro': ['lucro', 'prejuizo', 'resultado']\n",
    "    }\n",
    "    \n",
    "    for feature_name, keywords in keywords_features.items():\n",
    "        condicao = lit(False)\n",
    "        for keyword in keywords:\n",
    "            condicao = condicao | col(\"descr_conta\").contains(keyword)\n",
    "        df_features = df_features.withColumn(feature_name, when(condicao, lit(1)).otherwise(lit(0)))\n",
    "    \n",
    "    print(\"‚úÖ Features criadas!\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # ENCODING (MESMO PIPELINE DA C√âLULA 9)\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüî¢ Aplicando encoding de vari√°veis categ√≥ricas...\")\n",
    "    \n",
    "    from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "    from pyspark.ml import Pipeline\n",
    "    \n",
    "    categorical_cols = [\n",
    "        'origem_demonstrativo', 'ind_grp_bal', 'ind_grp_dre', 'cd_natureza',\n",
    "        'tp_conta_agl', 'tp_conta_pc', 'primeiro_digito_codigo', 'classificacao_nivel1'\n",
    "    ]\n",
    "    \n",
    "    indexers = [\n",
    "        StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_index\", handleInvalid=\"keep\")\n",
    "        for col_name in categorical_cols\n",
    "    ]\n",
    "    \n",
    "    encoders = [\n",
    "        OneHotEncoder(inputCol=f\"{col_name}_index\", outputCol=f\"{col_name}_encoded\")\n",
    "        for col_name in categorical_cols\n",
    "    ]\n",
    "    \n",
    "    pipeline_encoding = Pipeline(stages=indexers + encoders)\n",
    "    model_encoding = pipeline_encoding.fit(df_features)\n",
    "    df_encoded = model_encoding.transform(df_features)\n",
    "    \n",
    "    print(\"‚úÖ Encoding aplicado!\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # ASSEMBLER DE FEATURES\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüîó Criando vetor de features...\")\n",
    "    \n",
    "    numeric_features = [\n",
    "        'nivel_conta', 'tamanho_descricao', 'tamanho_codigo',\n",
    "        'tem_ponto_codigo', 'tem_hifen_codigo', 'tem_underscore_codigo',\n",
    "        'valor_absoluto_final', 'valor_absoluto_inicial', 'variacao_valor', 'tem_conta_superior'\n",
    "    ] + list(keywords_features.keys())\n",
    "    \n",
    "    categorical_encoded_features = [f\"{col_name}_encoded\" for col_name in categorical_cols]\n",
    "    all_features = numeric_features + categorical_encoded_features\n",
    "    \n",
    "    assembler = VectorAssembler(\n",
    "        inputCols=all_features,\n",
    "        outputCol=\"features\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    \n",
    "    df_assembled = assembler.transform(df_encoded)\n",
    "    \n",
    "    print(\"‚úÖ Vetor de features criado!\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # FAZER PREDI√á√ïES\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüîÆ Fazendo predi√ß√µes com Random Forest...\")\n",
    "    print(\"‚è≥ Isso pode levar alguns minutos...\")\n",
    "    \n",
    "    df_predictions = rf_model.transform(df_assembled)\n",
    "    \n",
    "    print(\"‚úÖ Predi√ß√µes conclu√≠das!\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # CONVERTER LABEL NUM√âRICA PARA CLASSIFICA√á√ÉO\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüîÑ Convertendo labels num√©ricas para classifica√ß√µes...\")\n",
    "    \n",
    "    # Carregar mapeamento de tabela Hive\n",
    "    label_mapping_df = spark.table(f\"{DATABASE_DESTINO}.ml_label_mapping\")\n",
    "    label_mapping = label_mapping_df.toPandas()\n",
    "    label_map_dict = dict(zip(label_mapping['label'], label_mapping['classificacao_nivel2']))\n",
    "    \n",
    "    print(f\"‚úÖ {len(label_map_dict)} mapeamentos carregados\")\n",
    "    \n",
    "    # Broadcast do dicion√°rio para melhor performance\n",
    "    label_map_broadcast = spark.sparkContext.broadcast(label_map_dict)\n",
    "    \n",
    "    # Criar UDF para converter\n",
    "    @udf(returnType=StringType())\n",
    "    def label_to_class(label):\n",
    "        if label is None:\n",
    "            return 'DESCONHECIDO'\n",
    "        try:\n",
    "            return label_map_broadcast.value.get(float(label), 'DESCONHECIDO')\n",
    "        except:\n",
    "            return 'DESCONHECIDO'\n",
    "    \n",
    "    df_predictions = df_predictions.withColumn(\n",
    "        \"classificacao_predita\",\n",
    "        label_to_class(col(\"rf_prediction\"))\n",
    "    )\n",
    "    \n",
    "    # Extrair confian√ßa (probabilidade m√°xima)\n",
    "    @udf(returnType=DoubleType())\n",
    "    def get_max_probability(probability_vector):\n",
    "        if probability_vector is None:\n",
    "            return 0.0\n",
    "        try:\n",
    "            probs = probability_vector.toArray()\n",
    "            return float(max(probs)) if len(probs) > 0 else 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    df_predictions = df_predictions.withColumn(\n",
    "        \"confianca_predicao\",\n",
    "        get_max_probability(col(\"rf_probability\"))\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ Labels convertidas!\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # SALVAR PREDI√á√ïES EM TABELA HIVE\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüíæ Salvando predi√ß√µes em tabela Hive...\")\n",
    "    \n",
    "    # Selecionar colunas relevantes\n",
    "    df_final_pred = df_predictions.select(\n",
    "        \"id_ecd\", \"cnpj\", \"cd_conta\", \"descr_conta\", \"nivel_conta\",\n",
    "        \"classificacao_nivel1\", \"classificacao_predita\", \"confianca_predicao\"\n",
    "    ).withColumn(\"modelo_usado\", lit(\"RandomForest\"))\n",
    "    \n",
    "    # Salvar\n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_predicoes\")\n",
    "    df_final_pred.write.mode(\"overwrite\").saveAsTable(\n",
    "        f\"{DATABASE_DESTINO}.ecd_ml_predicoes\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Predi√ß√µes salvas: {DATABASE_DESTINO}.ecd_ml_predicoes\")\n",
    "    \n",
    "    # ================================================================================\n",
    "    # ESTAT√çSTICAS DAS PREDI√á√ïES\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\nüìä ESTAT√çSTICAS DAS PREDI√á√ïES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(\"\\nüìã Top 20 Classifica√ß√µes Preditas:\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            classificacao_predita,\n",
    "            COUNT(*) as qtd,\n",
    "            ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentual\n",
    "        FROM {DATABASE_DESTINO}.ecd_ml_predicoes\n",
    "        GROUP BY classificacao_predita\n",
    "        ORDER BY qtd DESC\n",
    "        LIMIT 20\n",
    "    \"\"\").show(truncate=False)\n",
    "    \n",
    "    print(\"\\nüìä Distribui√ß√£o de Confian√ßa:\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            ROUND(MIN(confianca_predicao), 4) as min,\n",
    "            ROUND(PERCENTILE(confianca_predicao, 0.25), 4) as q1,\n",
    "            ROUND(PERCENTILE(confianca_predicao, 0.50), 4) as mediana,\n",
    "            ROUND(PERCENTILE(confianca_predicao, 0.75), 4) as q3,\n",
    "            ROUND(MAX(confianca_predicao), 4) as max,\n",
    "            ROUND(AVG(confianca_predicao), 4) as media\n",
    "        FROM {DATABASE_DESTINO}.ecd_ml_predicoes\n",
    "    \"\"\").show(truncate=False)\n",
    "    \n",
    "    # Distribui√ß√£o por faixa de confian√ßa\n",
    "    print(\"\\nüìä Predi√ß√µes por Faixa de Confian√ßa:\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            CASE \n",
    "                WHEN confianca_predicao >= 0.9 THEN 'Muito Alta (>=90%)'\n",
    "                WHEN confianca_predicao >= 0.7 THEN 'Alta (70-90%)'\n",
    "                WHEN confianca_predicao >= 0.5 THEN 'M√©dia (50-70%)'\n",
    "                ELSE 'Baixa (<50%)'\n",
    "            END as faixa_confianca,\n",
    "            COUNT(*) as qtd,\n",
    "            ROUND(COUNT(*) * 100.0 / SUM(COUNT(*)) OVER (), 2) as percentual\n",
    "        FROM {DATABASE_DESTINO}.ecd_ml_predicoes\n",
    "        GROUP BY \n",
    "            CASE \n",
    "                WHEN confianca_predicao >= 0.9 THEN 'Muito Alta (>=90%)'\n",
    "                WHEN confianca_predicao >= 0.7 THEN 'Alta (70-90%)'\n",
    "                WHEN confianca_predicao >= 0.5 THEN 'M√©dia (50-70%)'\n",
    "                ELSE 'Baixa (<50%)'\n",
    "            END\n",
    "        ORDER BY \n",
    "            CASE \n",
    "                WHEN faixa_confianca = 'Muito Alta (>=90%)' THEN 1\n",
    "                WHEN faixa_confianca = 'Alta (70-90%)' THEN 2\n",
    "                WHEN faixa_confianca = 'M√©dia (50-70%)' THEN 3\n",
    "                ELSE 4\n",
    "            END\n",
    "    \"\"\").show(truncate=False)\n",
    "    \n",
    "    # Amostra de predi√ß√µes com alta confian√ßa\n",
    "    print(\"\\nüìã Amostra de Predi√ß√µes (confian√ßa >= 80%):\")\n",
    "    spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            cd_conta, descr_conta, classificacao_nivel1, \n",
    "            classificacao_predita, \n",
    "            ROUND(confianca_predicao, 4) as confianca\n",
    "        FROM {DATABASE_DESTINO}.ecd_ml_predicoes\n",
    "        WHERE confianca_predicao >= 0.8\n",
    "        ORDER BY confianca_predicao DESC\n",
    "        LIMIT 20\n",
    "    \"\"\").show(truncate=50)\n",
    "    \n",
    "    # Amostra de predi√ß√µes com baixa confian√ßa (para revis√£o)\n",
    "    print(\"\\n‚ö†Ô∏è  Amostra de Predi√ß√µes com Baixa Confian√ßa (<50%):\")\n",
    "    baixa_confianca_count = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as cnt\n",
    "        FROM {DATABASE_DESTINO}.ecd_ml_predicoes\n",
    "        WHERE confianca_predicao < 0.5\n",
    "    \"\"\").collect()[0]['cnt']\n",
    "    \n",
    "    print(f\"Total com baixa confian√ßa: {baixa_confianca_count:,}\")\n",
    "    \n",
    "    if baixa_confianca_count > 0:\n",
    "        spark.sql(f\"\"\"\n",
    "            SELECT \n",
    "                cd_conta, descr_conta, classificacao_predita,\n",
    "                ROUND(confianca_predicao, 4) as confianca\n",
    "            FROM {DATABASE_DESTINO}.ecd_ml_predicoes\n",
    "            WHERE confianca_predicao < 0.5\n",
    "            ORDER BY confianca_predicao\n",
    "            LIMIT 20\n",
    "        \"\"\").show(truncate=50)\n",
    "\n",
    "    # ================================================================================\n",
    "    # RESUMO FINAL\n",
    "    # ================================================================================\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"üìä RESUMO DAS PREDI√á√ïES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\n‚úÖ Total de contas classificadas por ML: {total_nao_classificadas:,}\")\n",
    "    print(f\"üíæ Tabela: {DATABASE_DESTINO}.ecd_ml_predicoes\")\n",
    "    print(f\"ü§ñ Modelo usado: Random Forest\")\n",
    "    \n",
    "    # Calcular cobertura total estimada\n",
    "    total_contas = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as cnt \n",
    "        FROM {DATABASE_DESTINO}.ecd_contas_classificadas_final\n",
    "        WHERE ano_referencia = {ANO_REFERENCIA}\n",
    "    \"\"\").collect()[0]['cnt']\n",
    "    \n",
    "    contas_ja_classificadas = total_contas - total_nao_classificadas\n",
    "    perc_ja_classificadas = (contas_ja_classificadas * 100.0 / total_contas) if total_contas > 0 else 0\n",
    "    perc_ml = (total_nao_classificadas * 100.0 / total_contas) if total_contas > 0 else 0\n",
    "    \n",
    "    print(f\"\\nüìà COBERTURA TOTAL:\")\n",
    "    print(f\"   Total de contas: {total_contas:,}\")\n",
    "    print(f\"   J√° classificadas (regras): {contas_ja_classificadas:,} ({perc_ja_classificadas:.2f}%)\")\n",
    "    print(f\"   Classificadas por ML: {total_nao_classificadas:,} ({perc_ml:.2f}%)\")\n",
    "    print(f\"   COBERTURA TOTAL: ~100%! üéâ\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PREDI√á√ïES CONCLU√çDAS!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüéØ Pr√≥ximo passo: Validar predi√ß√µes e integrar √† tabela final\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b5c557-892a-40ea-9062-cb1b57921ce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 13: AN√ÅLISE DE FEATURE IMPORTANCE E REFINAMENTO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîç AN√ÅLISE DE FEATURE IMPORTANCE E REFINAMENTO...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "from pyspark.sql.functions import col\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import numpy as np\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR MODELO RANDOM FOREST\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Carregando modelo Random Forest...\")\n",
    "\n",
    "try:\n",
    "    # Caminho do modelo salvo na C√âLULA 10\n",
    "    rf_path = f\"/user/{spark.sparkContext.sparkUser()}/models/rf_{UF_FILTRO}_{ANO_REFERENCIA}\"\n",
    "    rf_model = RandomForestClassificationModel.load(rf_path)\n",
    "    print(\"‚úÖ Modelo Random Forest carregado!\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERRO ao carregar modelo: {e}\")\n",
    "    print(\"‚ö†Ô∏è  Execute a C√âLULA 10 primeiro para treinar o modelo!\")\n",
    "    raise\n",
    "\n",
    "# ================================================================================\n",
    "# DEFINIR NOMES DAS FEATURES\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìã Definindo nomes das features...\")\n",
    "\n",
    "# Features num√©ricas (conforme C√âLULA 9)\n",
    "numeric_features = [\n",
    "    'nivel_conta',\n",
    "    'tamanho_descricao',\n",
    "    'tamanho_codigo',\n",
    "    'tem_ponto_codigo',\n",
    "    'tem_hifen_codigo',\n",
    "    'tem_underscore_codigo',\n",
    "    'valor_absoluto_final',\n",
    "    'valor_absoluto_inicial',\n",
    "    'variacao_valor',\n",
    "    'tem_conta_superior',\n",
    "    'tem_palavra_caixa',\n",
    "    'tem_palavra_banco',\n",
    "    'tem_palavra_estoque',\n",
    "    'tem_palavra_cliente',\n",
    "    'tem_palavra_fornecedor',\n",
    "    'tem_palavra_salario',\n",
    "    'tem_palavra_tributo',\n",
    "    'tem_palavra_receita',\n",
    "    'tem_palavra_despesa',\n",
    "    'tem_palavra_financeiro',\n",
    "    'tem_palavra_imobilizado',\n",
    "    'tem_palavra_capital',\n",
    "    'tem_palavra_lucro'\n",
    "]\n",
    "\n",
    "# Features categ√≥ricas (conforme C√âLULA 9)\n",
    "categorical_cols = [\n",
    "    'origem_demonstrativo',\n",
    "    'ind_grp_bal',\n",
    "    'ind_grp_dre',\n",
    "    'cd_natureza',\n",
    "    'tp_conta_agl',\n",
    "    'tp_conta_pc',\n",
    "    'primeiro_digito_codigo',\n",
    "    'classificacao_nivel1'\n",
    "]\n",
    "\n",
    "print(f\"  Features num√©ricas: {len(numeric_features)}\")\n",
    "print(f\"  Features categ√≥ricas: {len(categorical_cols)}\")\n",
    "\n",
    "# ================================================================================\n",
    "# EXTRAIR FEATURE IMPORTANCE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä FEATURE IMPORTANCE - RANDOM FOREST\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Extrair import√¢ncias do modelo\n",
    "rf_importances = rf_model.featureImportances.toArray()\n",
    "\n",
    "print(f\"Total de import√¢ncias extra√≠das: {len(rf_importances)}\")\n",
    "\n",
    "# Criar nomes para todas as features (num√©ricas + encoded)\n",
    "# As features categ√≥ricas one-hot encoded geram m√∫ltiplas colunas\n",
    "n_numeric = len(numeric_features)\n",
    "n_total_features = len(rf_importances)\n",
    "n_categorical_encoded = n_total_features - n_numeric\n",
    "\n",
    "print(f\"  Features num√©ricas: {n_numeric}\")\n",
    "print(f\"  Features categ√≥ricas encoded: {n_categorical_encoded}\")\n",
    "\n",
    "# Lista de nomes das features\n",
    "feature_names = numeric_features.copy()\n",
    "\n",
    "# Adicionar nomes gen√©ricos para features categ√≥ricas encoded\n",
    "for i in range(n_categorical_encoded):\n",
    "    cat_idx = i % len(categorical_cols)\n",
    "    feature_names.append(f\"{categorical_cols[cat_idx]}_encoded_{i // len(categorical_cols)}\")\n",
    "\n",
    "# Criar DataFrame de import√¢ncia\n",
    "importance_df_rf = pd.DataFrame({\n",
    "    'feature': feature_names,\n",
    "    'importance': rf_importances\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nüìä Top 30 Features Mais Importantes:\")\n",
    "print(importance_df_rf.head(30).to_string())\n",
    "\n",
    "# ================================================================================\n",
    "# VISUALIZA√á√ÉO - TOP 30 FEATURES\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä Gerando gr√°fico de feature importance...\")\n",
    "\n",
    "fig_importance = px.bar(\n",
    "    importance_df_rf.head(30),\n",
    "    x='importance',\n",
    "    y='feature',\n",
    "    orientation='h',\n",
    "    title=f\"Top 30 Features Mais Importantes - Random Forest - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "    labels={'importance': 'Import√¢ncia', 'feature': 'Feature'},\n",
    "    color='importance',\n",
    "    color_continuous_scale='Viridis'\n",
    ")\n",
    "\n",
    "fig_importance.update_layout(\n",
    "    height=800,\n",
    "    yaxis={'categoryorder': 'total ascending'},\n",
    "    showlegend=False\n",
    ")\n",
    "\n",
    "fig_importance.show()\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE DE FEATURES NUM√âRICAS vs CATEG√ìRICAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AN√ÅLISE: Features Num√©ricas vs Categ√≥ricas\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Separar import√¢ncias\n",
    "importance_numeric = importance_df_rf.iloc[:n_numeric].copy()\n",
    "importance_categorical = importance_df_rf.iloc[n_numeric:].copy()\n",
    "\n",
    "total_importance_numeric = importance_numeric['importance'].sum()\n",
    "total_importance_categorical = importance_categorical['importance'].sum()\n",
    "\n",
    "print(f\"\\nImport√¢ncia total - Num√©ricas: {total_importance_numeric:.4f} ({total_importance_numeric*100:.2f}%)\")\n",
    "print(f\"Import√¢ncia total - Categ√≥ricas: {total_importance_categorical:.4f} ({total_importance_categorical*100:.2f}%)\")\n",
    "\n",
    "# Gr√°fico de pizza\n",
    "fig_pie = go.Figure(data=[go.Pie(\n",
    "    labels=['Features Num√©ricas', 'Features Categ√≥ricas (Encoded)'],\n",
    "    values=[total_importance_numeric, total_importance_categorical],\n",
    "    hole=0.3\n",
    ")])\n",
    "\n",
    "fig_pie.update_layout(\n",
    "    title=f\"Distribui√ß√£o de Import√¢ncia: Num√©ricas vs Categ√≥ricas - {UF_FILTRO} {ANO_REFERENCIA}\",\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig_pie.show()\n",
    "\n",
    "# ================================================================================\n",
    "# IDENTIFICAR FEATURES DE BAIXA IMPORT√ÇNCIA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìâ FEATURES DE BAIXA IMPORT√ÇNCIA\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Threshold de baixa import√¢ncia\n",
    "threshold_baixa = 0.01\n",
    "features_baixa_importancia = importance_df_rf[importance_df_rf['importance'] < threshold_baixa]\n",
    "\n",
    "print(f\"\\nFeatures com import√¢ncia < {threshold_baixa}:\")\n",
    "print(f\"  Total: {len(features_baixa_importancia)}\")\n",
    "print(f\"  Percentual: {len(features_baixa_importancia)/len(importance_df_rf)*100:.2f}%\")\n",
    "\n",
    "if len(features_baixa_importancia) > 0:\n",
    "    print(\"\\n  Lista (primeiras 20):\")\n",
    "    for i, row in features_baixa_importancia.head(20).iterrows():\n",
    "        print(f\"   - {row['feature']}: {row['importance']:.6f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# FEATURES MAIS IMPORTANTES (TOP 10%)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìà FEATURES MAIS IMPORTANTES (TOP 10%)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "threshold_alta = importance_df_rf['importance'].quantile(0.9)\n",
    "features_alta_importancia = importance_df_rf[importance_df_rf['importance'] >= threshold_alta]\n",
    "\n",
    "print(f\"\\nFeatures no top 10% (import√¢ncia >= {threshold_alta:.6f}):\")\n",
    "print(f\"  Total: {len(features_alta_importancia)}\")\n",
    "\n",
    "print(\"\\n  Lista:\")\n",
    "for i, row in features_alta_importancia.iterrows():\n",
    "    print(f\"   - {row['feature']}: {row['importance']:.6f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# RECOMENDA√á√ïES DE REFINAMENTO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üí° RECOMENDA√á√ïES DE REFINAMENTO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\n1Ô∏è‚É£ FEATURES PARA REMOVER (baixa import√¢ncia < 0.01):\")\n",
    "if len(features_baixa_importancia) > 0:\n",
    "    print(f\"   ‚úÇÔ∏è  Remover {len(features_baixa_importancia)} features ({len(features_baixa_importancia)/len(importance_df_rf)*100:.1f}% do total)\")\n",
    "    print(\"   üí° Isso pode reduzir overfitting e melhorar generaliza√ß√£o\")\n",
    "    print(\"   ‚ö†Ô∏è  Validar impacto removendo e retreinando o modelo\")\n",
    "else:\n",
    "    print(\"   ‚úÖ N√£o h√° features com import√¢ncia muito baixa\")\n",
    "\n",
    "print(\"\\n2Ô∏è‚É£ FEATURES PARA ADICIONAR:\")\n",
    "print(\"   üí° Considere criar:\")\n",
    "print(\"   üìä Features de Intera√ß√£o:\")\n",
    "print(\"      - razao_valor_final_inicial (vl_cta_fin / vl_cta_ini)\")\n",
    "print(\"      - nivel_x_tamanho_descricao\")\n",
    "print(\"      - nivel_x_tamanho_codigo\")\n",
    "print(\"   üìä Features de Transforma√ß√£o:\")\n",
    "print(\"      - log_valor_absoluto_final (log1p para lidar com outliers)\")\n",
    "print(\"      - log_valor_absoluto_inicial\")\n",
    "print(\"   üìä Features Textuais Avan√ßadas:\")\n",
    "print(\"      - TF-IDF da descri√ß√£o (top N termos)\")\n",
    "print(\"      - N-gramas (bi-gramas, tri-gramas)\")\n",
    "print(\"      - Similaridade com termos do referencial\")\n",
    "\n",
    "print(\"\\n3Ô∏è‚É£ AN√ÅLISE DE CLASSE:\")\n",
    "# Verificar balanceamento\n",
    "df_train = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_train\")\n",
    "class_counts = df_train.groupBy(\"label\").count().orderBy(\"count\", ascending=False).toPandas()\n",
    "\n",
    "ratio_maior_menor = class_counts['count'].max() / class_counts['count'].min()\n",
    "print(f\"   üìä Raz√£o maior/menor classe: {ratio_maior_menor:.2f}\")\n",
    "\n",
    "if ratio_maior_menor > 10:\n",
    "    print(\"   ‚ö†Ô∏è  Classes desbalanceadas detectadas!\")\n",
    "    print(\"   üí° Recomenda√ß√µes:\")\n",
    "    print(\"      - Usar class weights no treinamento\")\n",
    "    print(\"      - Aplicar SMOTE (oversampling)\")\n",
    "    print(\"      - Undersampling da classe majorit√°ria\")\n",
    "else:\n",
    "    print(\"   ‚úÖ Classes razoavelmente balanceadas\")\n",
    "\n",
    "print(\"\\n4Ô∏è‚É£ HIPERPAR√ÇMETROS:\")\n",
    "print(\"   üîß Considere fazer grid search para otimizar:\")\n",
    "print(\"   üìå Random Forest:\")\n",
    "print(\"      - numTrees: testar [100, 150, 200]\")\n",
    "print(\"      - maxDepth: testar [10, 12, 15]\")\n",
    "print(\"      - minInstancesPerNode: testar [5, 10, 15]\")\n",
    "print(\"      - featureSubsetStrategy: testar ['sqrt', 'log2', '0.5']\")\n",
    "\n",
    "# ================================================================================\n",
    "# SALVAR AN√ÅLISE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ Salvando an√°lise de feature importance...\")\n",
    "\n",
    "# Converter para Spark DataFrame\n",
    "df_importance = spark.createDataFrame(importance_df_rf)\n",
    "\n",
    "# Salvar em tabela Hive\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_feature_importance\")\n",
    "\n",
    "df_importance.write.mode(\"overwrite\").saveAsTable(\n",
    "    f\"{DATABASE_DESTINO}.ecd_ml_feature_importance\"\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ An√°lise salva: {DATABASE_DESTINO}.ecd_ml_feature_importance\")\n",
    "\n",
    "# Salvar lista de features para remover\n",
    "if len(features_baixa_importancia) > 0:\n",
    "    df_features_remover = spark.createDataFrame(features_baixa_importancia)\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_features_baixa_importancia\")\n",
    "    \n",
    "    df_features_remover.write.mode(\"overwrite\").saveAsTable(\n",
    "        f\"{DATABASE_DESTINO}.ecd_ml_features_baixa_importancia\"\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Features para remover salvas: {DATABASE_DESTINO}.ecd_ml_features_baixa_importancia\")\n",
    "\n",
    "# ================================================================================\n",
    "# RESUMO FINAL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä RESUMO DA AN√ÅLISE DE FEATURE IMPORTANCE\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüìà Total de features analisadas: {len(importance_df_rf)}\")\n",
    "print(f\"   - Num√©ricas: {n_numeric}\")\n",
    "print(f\"   - Categ√≥ricas (encoded): {n_categorical_encoded}\")\n",
    "\n",
    "print(f\"\\nüîù Top features:\")\n",
    "print(f\"   - Top 10%: {len(features_alta_importancia)} features\")\n",
    "print(f\"   - Import√¢ncia acumulada: {features_alta_importancia['importance'].sum():.4f}\")\n",
    "\n",
    "print(f\"\\nüìâ Features de baixa import√¢ncia:\")\n",
    "print(f\"   - < 0.01: {len(features_baixa_importancia)} features ({len(features_baixa_importancia)/len(importance_df_rf)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüí™ Import√¢ncia por tipo:\")\n",
    "print(f\"   - Num√©ricas: {total_importance_numeric:.4f} ({total_importance_numeric*100:.2f}%)\")\n",
    "print(f\"   - Categ√≥ricas: {total_importance_categorical:.4f} ({total_importance_categorical*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nüìÇ Tabelas criadas:\")\n",
    "print(f\"   - {DATABASE_DESTINO}.ecd_ml_feature_importance\")\n",
    "if len(features_baixa_importancia) > 0:\n",
    "    print(f\"   - {DATABASE_DESTINO}.ecd_ml_features_baixa_importancia\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ AN√ÅLISE DE FEATURE IMPORTANCE CONCLU√çDA!\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüéØ Pr√≥ximo passo: Execute C√âLULA 14 para retreinamento otimizado\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a80ee3-3b80-415b-977b-736d89a19c4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 14 CORRIGIDA: RETREINAMENTO COM ABORDAGEM CONSERVADORA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîÑ RETREINAMENTO COM FEATURES OTIMIZADAS (VERS√ÉO CONSERVADORA)...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when, lit, log1p, udf, abs as spark_abs\n",
    "from pyspark.sql.types import DoubleType\n",
    "import time\n",
    "import pandas as pd\n",
    "\n",
    "# ================================================================================\n",
    "# CONFIGURA√á√ÉO DE RETREINAMENTO - VERS√ÉO CONSERVADORA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n‚öôÔ∏è  CONFIGURA√á√ÉO DE RETREINAMENTO (CONSERVADORA)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# ‚ùå N√ÉO remover features de palavras-chave (s√£o essenciais!)\n",
    "# ‚úÖ Apenas adicionar features de intera√ß√£o e usar class weights\n",
    "REMOVER_FEATURES_BAIXA_IMPORTANCIA = False  # DESLIGADO - manter todas as features\n",
    "ADICIONAR_FEATURES_INTERACAO = True          # Adicionar features de intera√ß√£o\n",
    "USAR_CLASS_WEIGHTS = True                    # Balancear classes com pesos\n",
    "\n",
    "# Hiperpar√¢metros otimizados\n",
    "RF_NUM_TREES = 150        # Aumentado de 100\n",
    "RF_MAX_DEPTH = 12         # Aumentado de 10\n",
    "RF_MIN_INSTANCES = 5      # Reduzido de 10\n",
    "RF_SUBSAMPLING_RATE = 0.8\n",
    "RF_FEATURE_SUBSET = \"sqrt\"\n",
    "\n",
    "print(f\"‚úÖ Configura√ß√µes:\")\n",
    "print(f\"   - Remover features baixa import√¢ncia: {REMOVER_FEATURES_BAIXA_IMPORTANCIA}\")\n",
    "print(f\"   - Adicionar features de intera√ß√£o: {ADICIONAR_FEATURES_INTERACAO}\")\n",
    "print(f\"   - Usar class weights: {USAR_CLASS_WEIGHTS}\")\n",
    "print(f\"   - RF numTrees: {RF_NUM_TREES}\")\n",
    "print(f\"   - RF maxDepth: {RF_MAX_DEPTH}\")\n",
    "print(f\"   - RF minInstancesPerNode: {RF_MIN_INSTANCES}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR DADOS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Carregando dados de treino e valida√ß√£o...\")\n",
    "\n",
    "DATABASE_DESTINO = 'neac'\n",
    "UF_FILTRO = 'SC'\n",
    "ANO_REFERENCIA = 2024\n",
    "\n",
    "df_train = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_train\")\n",
    "df_val = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_val\")\n",
    "df_test = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_test\")\n",
    "\n",
    "print(f\"‚úÖ Treino: {df_train.count():,} registros\")\n",
    "print(f\"‚úÖ Valida√ß√£o: {df_val.count():,} registros\")\n",
    "print(f\"‚úÖ Teste: {df_test.count():,} registros\")\n",
    "\n",
    "# ================================================================================\n",
    "# FEATURE ENGINEERING - APENAS ADICIONAR INTERA√á√ïES\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîß APLICANDO FEATURE ENGINEERING...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Features num√©ricas base (MANTER TODAS - incluindo keywords!)\n",
    "numeric_features_base = [\n",
    "    'nivel_conta',\n",
    "    'tamanho_descricao',\n",
    "    'tamanho_codigo',\n",
    "    'tem_ponto_codigo',\n",
    "    'tem_hifen_codigo',\n",
    "    'tem_underscore_codigo',\n",
    "    'valor_absoluto_final',\n",
    "    'valor_absoluto_inicial',\n",
    "    'variacao_valor',\n",
    "    'tem_conta_superior',\n",
    "    'tem_palavra_caixa',\n",
    "    'tem_palavra_banco',\n",
    "    'tem_palavra_estoque',\n",
    "    'tem_palavra_cliente',\n",
    "    'tem_palavra_fornecedor',\n",
    "    'tem_palavra_salario',\n",
    "    'tem_palavra_tributo',\n",
    "    'tem_palavra_receita',\n",
    "    'tem_palavra_despesa',\n",
    "    'tem_palavra_financeiro',\n",
    "    'tem_palavra_imobilizado',\n",
    "    'tem_palavra_capital',\n",
    "    'tem_palavra_lucro'\n",
    "]\n",
    "\n",
    "# Adicionar features de intera√ß√£o\n",
    "if ADICIONAR_FEATURES_INTERACAO:\n",
    "    print(\"\\n  ‚ûï Criando features de intera√ß√£o...\")\n",
    "    \n",
    "    # Aplicar transforma√ß√µes em todos os datasets\n",
    "    for df_name, df in [(\"Treino\", df_train), (\"Valida√ß√£o\", df_val), (\"Teste\", df_test)]:\n",
    "        print(f\"     - Processando {df_name}...\")\n",
    "        \n",
    "        # 1. Raz√£o entre valores (prote√ß√£o divis√£o por zero)\n",
    "        df = df.withColumn(\n",
    "            \"razao_valor_final_inicial\",\n",
    "            when(col(\"valor_absoluto_inicial\") > 0,\n",
    "                 col(\"valor_absoluto_final\") / col(\"valor_absoluto_inicial\")\n",
    "            ).otherwise(lit(0.0))\n",
    "        )\n",
    "        \n",
    "        # 2. Intera√ß√µes nivel * tamanho\n",
    "        df = df.withColumn(\n",
    "            \"nivel_x_tamanho_descricao\",\n",
    "            col(\"nivel_conta\") * col(\"tamanho_descricao\")\n",
    "        )\n",
    "        \n",
    "        df = df.withColumn(\n",
    "            \"nivel_x_tamanho_codigo\",\n",
    "            col(\"nivel_conta\") * col(\"tamanho_codigo\")\n",
    "        )\n",
    "        \n",
    "        # 3. Log de valores (lidar com outliers)\n",
    "        df = df.withColumn(\n",
    "            \"log_valor_absoluto_final\",\n",
    "            log1p(col(\"valor_absoluto_final\"))\n",
    "        )\n",
    "        \n",
    "        df = df.withColumn(\n",
    "            \"log_valor_absoluto_inicial\",\n",
    "            log1p(col(\"valor_absoluto_inicial\"))\n",
    "        )\n",
    "        \n",
    "        # 4. Diferen√ßa absoluta entre valores\n",
    "        df = df.withColumn(\n",
    "            \"diferenca_absoluta_valores\",\n",
    "            spark_abs(col(\"valor_absoluto_final\") - col(\"valor_absoluto_inicial\"))\n",
    "        )\n",
    "        \n",
    "        # Atualizar refer√™ncia\n",
    "        if df_name == \"Treino\":\n",
    "            df_train = df\n",
    "        elif df_name == \"Valida√ß√£o\":\n",
    "            df_val = df\n",
    "        else:\n",
    "            df_test = df\n",
    "    \n",
    "    # Adicionar novas features √† lista\n",
    "    numeric_features_base.extend([\n",
    "        'razao_valor_final_inicial',\n",
    "        'nivel_x_tamanho_descricao',\n",
    "        'nivel_x_tamanho_codigo',\n",
    "        'log_valor_absoluto_final',\n",
    "        'log_valor_absoluto_inicial',\n",
    "        'diferenca_absoluta_valores'\n",
    "    ])\n",
    "    \n",
    "    print(f\"  ‚úÖ 6 features de intera√ß√£o criadas!\")\n",
    "\n",
    "numeric_features_final = numeric_features_base\n",
    "\n",
    "print(f\"\\n  üìä Total de features num√©ricas: {len(numeric_features_final)}\")\n",
    "\n",
    "# ================================================================================\n",
    "# PREPARAR FEATURES CATEG√ìRICAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n  üî¢ Preparando features categ√≥ricas...\")\n",
    "\n",
    "categorical_cols = [\n",
    "    'origem_demonstrativo',\n",
    "    'ind_grp_bal',\n",
    "    'ind_grp_dre',\n",
    "    'cd_natureza',\n",
    "    'tp_conta_agl',\n",
    "    'tp_conta_pc',\n",
    "    'primeiro_digito_codigo',\n",
    "    'classificacao_nivel1'\n",
    "]\n",
    "\n",
    "# Features categ√≥ricas indexadas\n",
    "categorical_indexed_features = [f\"{col_name}_index\" for col_name in categorical_cols]\n",
    "\n",
    "# Verificar disponibilidade\n",
    "available_cols = df_train.columns\n",
    "categorical_indexed_features = [f for f in categorical_indexed_features if f in available_cols]\n",
    "\n",
    "print(f\"  ‚úÖ Features categ√≥ricas indexadas: {len(categorical_indexed_features)}\")\n",
    "\n",
    "# Todas as features\n",
    "all_features = numeric_features_final + categorical_indexed_features\n",
    "\n",
    "print(f\"\\n  üìä Total de features para o modelo:\")\n",
    "print(f\"     - Num√©ricas: {len(numeric_features_final)}\")\n",
    "print(f\"     - Categ√≥ricas (indexadas): {len(categorical_indexed_features)}\")\n",
    "print(f\"     - TOTAL: {len(all_features)}\")\n",
    "\n",
    "# ================================================================================\n",
    "# BALANCEAMENTO COM CLASS WEIGHTS (VERS√ÉO MELHORADA)\n",
    "# ================================================================================\n",
    "\n",
    "if USAR_CLASS_WEIGHTS:\n",
    "    print(\"\\n‚öñÔ∏è  CALCULANDO PESOS DE CLASSES (VERS√ÉO BALANCEADA)...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Calcular distribui√ß√£o\n",
    "    class_counts = df_train.groupBy(\"label\").count().toPandas()\n",
    "    total = class_counts['count'].sum()\n",
    "    n_classes = len(class_counts)\n",
    "    \n",
    "    # Calcular pesos usando m√©todo BALANCED\n",
    "    # Peso = total / (n_classes * count)\n",
    "    class_counts['weight'] = total / (n_classes * class_counts['count'])\n",
    "    \n",
    "    # LIMITA√á√ÉO: Limitar pesos m√°ximos para evitar overfitting em classes raras\n",
    "    MAX_WEIGHT = 50.0  # Peso m√°ximo permitido\n",
    "    class_counts['weight_limited'] = class_counts['weight'].clip(upper=MAX_WEIGHT)\n",
    "    \n",
    "    print(f\"  üìä Total de classes: {n_classes}\")\n",
    "    print(f\"  üìä Peso m√≠nimo: {class_counts['weight_limited'].min():.4f}\")\n",
    "    print(f\"  üìä Peso m√°ximo: {class_counts['weight_limited'].max():.4f}\")\n",
    "    print(f\"  üìä Peso m√©dio: {class_counts['weight_limited'].mean():.4f}\")\n",
    "    \n",
    "    # Mostrar classes mais desbalanceadas\n",
    "    print(\"\\n  üìã Top 5 classes com maior peso (menos frequentes):\")\n",
    "    for idx, row in class_counts.nlargest(5, 'weight').iterrows():\n",
    "        peso_original = row['weight']\n",
    "        peso_limitado = row['weight_limited']\n",
    "        limitado_str = \" [LIMITADO]\" if peso_original != peso_limitado else \"\"\n",
    "        print(f\"     Label {row['label']:.0f}: peso={peso_limitado:.4f}{limitado_str}, count={row['count']:,}\")\n",
    "    \n",
    "    # Criar mapeamento com pesos limitados\n",
    "    weight_map = dict(zip(class_counts['label'], class_counts['weight_limited']))\n",
    "    \n",
    "    # UDF para aplicar pesos\n",
    "    @udf(returnType=DoubleType())\n",
    "    def get_class_weight(label):\n",
    "        try:\n",
    "            return float(weight_map.get(float(label), 1.0))\n",
    "        except:\n",
    "            return 1.0\n",
    "    \n",
    "    # Adicionar coluna de peso\n",
    "    df_train = df_train.withColumn(\"classWeight\", get_class_weight(col(\"label\")))\n",
    "    \n",
    "    print(\"  ‚úÖ Pesos de classe calculados e aplicados (com limita√ß√£o)!\")\n",
    "\n",
    "# ================================================================================\n",
    "# CRIAR VETOR DE FEATURES\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîó CRIANDO VETOR DE FEATURES...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=all_features,\n",
    "    outputCol=\"features_v2\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "df_train_assembled = assembler.transform(df_train)\n",
    "df_val_assembled = assembler.transform(df_val)\n",
    "df_test_assembled = assembler.transform(df_test)\n",
    "\n",
    "print(\"‚úÖ Vetor de features criado!\")\n",
    "print(f\"   - Total de features: {len(all_features)}\")\n",
    "\n",
    "# ================================================================================\n",
    "# TREINAR MODELO OTIMIZADO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüå≤ TREINANDO RANDOM FOREST OTIMIZADO (V2 - CONSERVADOR)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Configurar modelo\n",
    "if USAR_CLASS_WEIGHTS:\n",
    "    rf_v2 = RandomForestClassifier(\n",
    "        featuresCol=\"features_v2\",\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        probabilityCol=\"probability\",\n",
    "        weightCol=\"classWeight\",\n",
    "        numTrees=RF_NUM_TREES,\n",
    "        maxDepth=RF_MAX_DEPTH,\n",
    "        minInstancesPerNode=RF_MIN_INSTANCES,\n",
    "        subsamplingRate=RF_SUBSAMPLING_RATE,\n",
    "        featureSubsetStrategy=RF_FEATURE_SUBSET,\n",
    "        seed=42\n",
    "    )\n",
    "else:\n",
    "    rf_v2 = RandomForestClassifier(\n",
    "        featuresCol=\"features_v2\",\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        probabilityCol=\"probability\",\n",
    "        numTrees=RF_NUM_TREES,\n",
    "        maxDepth=RF_MAX_DEPTH,\n",
    "        minInstancesPerNode=RF_MIN_INSTANCES,\n",
    "        subsamplingRate=RF_SUBSAMPLING_RATE,\n",
    "        featureSubsetStrategy=RF_FEATURE_SUBSET,\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "print(f\"‚öôÔ∏è  Configura√ß√£o:\")\n",
    "print(f\"   - numTrees: {RF_NUM_TREES}\")\n",
    "print(f\"   - maxDepth: {RF_MAX_DEPTH}\")\n",
    "print(f\"   - minInstancesPerNode: {RF_MIN_INSTANCES}\")\n",
    "print(f\"   - Class weights: {USAR_CLASS_WEIGHTS} (com limita√ß√£o)\")\n",
    "\n",
    "print(\"\\n‚è≥ Treinando modelo... (5-15 minutos)\")\n",
    "inicio = time.time()\n",
    "\n",
    "rf_model_v2 = rf_v2.fit(df_train_assembled)\n",
    "\n",
    "tempo_treino = time.time() - inicio\n",
    "print(f\"\\n‚úÖ Modelo treinado em {tempo_treino:.2f} segundos ({tempo_treino/60:.2f} minutos)\")\n",
    "\n",
    "# ================================================================================\n",
    "# AVALIAR MODELO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AVALIANDO MODELO OTIMIZADO (V2)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(\"\\nüîÆ Fazendo predi√ß√µes...\")\n",
    "df_val_pred = rf_model_v2.transform(df_val_assembled)\n",
    "\n",
    "# M√©tricas\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "accuracy_v2 = evaluator_accuracy.evaluate(df_val_pred)\n",
    "f1_v2 = evaluator_f1.evaluate(df_val_pred)\n",
    "precision_v2 = evaluator_precision.evaluate(df_val_pred)\n",
    "recall_v2 = evaluator_recall.evaluate(df_val_pred)\n",
    "\n",
    "print(f\"\\nüìà M√©tricas do Modelo V2 (Conservador):\")\n",
    "print(f\"   Accuracy:  {accuracy_v2:.4f} ({accuracy_v2*100:.2f}%)\")\n",
    "print(f\"   F1-Score:  {f1_v2:.4f}\")\n",
    "print(f\"   Precision: {precision_v2:.4f}\")\n",
    "print(f\"   Recall:    {recall_v2:.4f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# COMPARA√á√ÉO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä COMPARA√á√ÉO: V1 vs V2 (Conservador)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "try:\n",
    "    metricas_original = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_metricas\") \\\n",
    "        .filter(col(\"modelo\") == \"RandomForest\") \\\n",
    "        .collect()\n",
    "    \n",
    "    if len(metricas_original) > 0:\n",
    "        accuracy_v1 = metricas_original[0]['accuracy']\n",
    "        f1_v1 = metricas_original[0]['f1_score']\n",
    "        \n",
    "        melhoria_accuracy = ((accuracy_v2 - accuracy_v1) / accuracy_v1) * 100\n",
    "        melhoria_f1 = ((f1_v2 - f1_v1) / f1_v1) * 100\n",
    "        \n",
    "        print(f\"\\nüîµ Modelo V1 (Original):\")\n",
    "        print(f\"   Accuracy: {accuracy_v1:.4f} ({accuracy_v1*100:.2f}%)\")\n",
    "        print(f\"   F1-Score: {f1_v1:.4f}\")\n",
    "        \n",
    "        print(f\"\\nüü¢ Modelo V2 (Conservador):\")\n",
    "        print(f\"   Accuracy: {accuracy_v2:.4f} ({accuracy_v2*100:.2f}%) [{melhoria_accuracy:+.2f}%]\")\n",
    "        print(f\"   F1-Score: {f1_v2:.4f} [{melhoria_f1:+.2f}%]\")\n",
    "        \n",
    "        if accuracy_v2 > accuracy_v1:\n",
    "            print(f\"\\nüéâ MELHORIA!\")\n",
    "            print(f\"   ‚úÖ Accuracy: +{melhoria_accuracy:.2f}%\")\n",
    "            print(f\"   ‚úÖ F1-Score: +{melhoria_f1:.2f}%\")\n",
    "        elif accuracy_v2 >= accuracy_v1 * 0.98:  # At√© 2% de queda √© aceit√°vel\n",
    "            print(f\"\\n‚úÖ Desempenho similar (varia√ß√£o: {melhoria_accuracy:.2f}%)\")\n",
    "        else:\n",
    "            print(f\"\\n‚ö†Ô∏è  Desempenho reduziu:\")\n",
    "            print(f\"   ‚¨áÔ∏è  Accuracy: {melhoria_accuracy:.2f}%\")\n",
    "            print(f\"   ‚¨áÔ∏è  F1-Score: {melhoria_f1:.2f}%\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Erro na compara√ß√£o: {e}\")\n",
    "\n",
    "# ================================================================================\n",
    "# SALVAR\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ SALVANDO MODELO...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "try:\n",
    "    rf_v2_path = f\"/user/{spark.sparkContext.sparkUser()}/models/rf_v2_conservador_{UF_FILTRO}_{ANO_REFERENCIA}\"\n",
    "    rf_model_v2.write().overwrite().save(rf_v2_path)\n",
    "    print(f\"‚úÖ Modelo salvo: {rf_v2_path}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Aviso: {e}\")\n",
    "\n",
    "# Salvar predi√ß√µes\n",
    "df_val_pred_final = df_val_pred.select(\n",
    "    \"id_ecd\", \"cnpj\", \"cd_conta\", \"descr_conta\",\n",
    "    \"classificacao_nivel2\", \"label\", \"prediction\"\n",
    ")\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_predictions_rf_v2_conservador_val\")\n",
    "df_val_pred_final.write.mode(\"overwrite\").saveAsTable(\n",
    "    f\"{DATABASE_DESTINO}.ecd_ml_predictions_rf_v2_conservador_val\"\n",
    ")\n",
    "print(f\"‚úÖ Predi√ß√µes salvas: {DATABASE_DESTINO}.ecd_ml_predictions_rf_v2_conservador_val\")\n",
    "\n",
    "# Salvar m√©tricas\n",
    "from pyspark.sql import Row\n",
    "\n",
    "nova_metrica = spark.createDataFrame([Row(\n",
    "    modelo=\"RandomForest_v2_Conservador\",\n",
    "    accuracy=float(accuracy_v2),\n",
    "    f1_score=float(f1_v2),\n",
    "    precision=float(precision_v2),\n",
    "    recall=float(recall_v2),\n",
    "    tempo_treino_segundos=float(tempo_treino),\n",
    "    num_trees=RF_NUM_TREES,\n",
    "    max_depth=RF_MAX_DEPTH,\n",
    "    ano_referencia=ANO_REFERENCIA,\n",
    "    uf=UF_FILTRO\n",
    ")])\n",
    "\n",
    "nova_metrica.write.mode(\"append\").saveAsTable(f\"{DATABASE_DESTINO}.ecd_ml_metricas\")\n",
    "print(f\"‚úÖ M√©tricas salvas: {DATABASE_DESTINO}.ecd_ml_metricas\")\n",
    "\n",
    "# ================================================================================\n",
    "# RESUMO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ RETREINAMENTO CONSERVADOR CONCLU√çDO!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüîß Otimiza√ß√µes aplicadas:\")\n",
    "print(f\"   ‚úÖ 6 features de intera√ß√£o adicionadas\")\n",
    "print(f\"   ‚úÖ Class weights balanceados (com limita√ß√£o MAX={50.0})\")\n",
    "print(f\"   ‚úÖ Hiperpar√¢metros otimizados (150 √°rvores, profundidade 12)\")\n",
    "print(f\"   ‚ÑπÔ∏è  Features de palavras-chave MANTIDAS (essenciais!)\")\n",
    "\n",
    "print(f\"\\nüìä Modelo V2 (Conservador):\")\n",
    "print(f\"   - Features: {len(all_features)} (29 num√©ricas + 8 categ√≥ricas indexadas)\")\n",
    "print(f\"   - Accuracy: {accuracy_v2*100:.2f}%\")\n",
    "print(f\"   - F1-Score: {f1_v2:.4f}\")\n",
    "print(f\"   - Tempo: {tempo_treino/60:.2f} min\")\n",
    "\n",
    "print(\"\\nüí° Li√ß√µes aprendidas:\")\n",
    "print(\"   - Features de palavras-chave s√£o CR√çTICAS (n√£o remover!)\")\n",
    "print(\"   - Class weights funcionam melhor com limita√ß√£o de valor m√°ximo\")\n",
    "print(\"   - Abordagem conservadora > Abordagem agressiva\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a032a4ef-1e82-4643-afbd-61bcb3285ae2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 14.1: TESTE COMPLETO - 3 ABORDAGENS DE OTIMIZA√á√ÉO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüî¨ TESTE COMPLETO: 3 ABORDAGENS DE OTIMIZA√á√ÉO\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nVamos testar:\")\n",
    "print(\"  A) Sem class weights (baseline V1 melhorado)\")\n",
    "print(\"  B) Class weights com MAX_WEIGHT=10.0 (conservador)\")\n",
    "print(\"  C) Filtrar classes raras + class weights moderados\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import col, when, lit, log1p, udf, abs as spark_abs\n",
    "from pyspark.sql.types import DoubleType\n",
    "import time\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# ================================================================================\n",
    "# CONFIGURA√á√ïES GLOBAIS\n",
    "# ================================================================================\n",
    "\n",
    "DATABASE_DESTINO = 'neac'\n",
    "UF_FILTRO = 'SC'\n",
    "ANO_REFERENCIA = 2024\n",
    "\n",
    "# Hiperpar√¢metros otimizados (mantidos para todas as abordagens)\n",
    "RF_NUM_TREES = 150\n",
    "RF_MAX_DEPTH = 12\n",
    "RF_MIN_INSTANCES = 5\n",
    "RF_SUBSAMPLING_RATE = 0.8\n",
    "RF_FEATURE_SUBSET = \"sqrt\"\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR DADOS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüì• Carregando dados...\")\n",
    "df_train = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_train\")\n",
    "df_val = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_val\")\n",
    "df_test = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_test\")\n",
    "\n",
    "print(f\"‚úÖ Treino: {df_train.count():,} registros\")\n",
    "print(f\"‚úÖ Valida√ß√£o: {df_val.count():,} registros\")\n",
    "print(f\"‚úÖ Teste: {df_test.count():,} registros\")\n",
    "\n",
    "# ================================================================================\n",
    "# FEATURE ENGINEERING (COMUM A TODAS AS ABORDAGENS)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîß APLICANDO FEATURE ENGINEERING...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Features num√©ricas base (MANTER TODAS!)\n",
    "numeric_features_base = [\n",
    "    'nivel_conta',\n",
    "    'tamanho_descricao',\n",
    "    'tamanho_codigo',\n",
    "    'tem_ponto_codigo',\n",
    "    'tem_hifen_codigo',\n",
    "    'tem_underscore_codigo',\n",
    "    'valor_absoluto_final',\n",
    "    'valor_absoluto_inicial',\n",
    "    'variacao_valor',\n",
    "    'tem_conta_superior',\n",
    "    'tem_palavra_caixa',\n",
    "    'tem_palavra_banco',\n",
    "    'tem_palavra_estoque',\n",
    "    'tem_palavra_cliente',\n",
    "    'tem_palavra_fornecedor',\n",
    "    'tem_palavra_salario',\n",
    "    'tem_palavra_tributo',\n",
    "    'tem_palavra_receita',\n",
    "    'tem_palavra_despesa',\n",
    "    'tem_palavra_financeiro',\n",
    "    'tem_palavra_imobilizado',\n",
    "    'tem_palavra_capital',\n",
    "    'tem_palavra_lucro'\n",
    "]\n",
    "\n",
    "print(\"\\n  ‚ûï Criando features de intera√ß√£o...\")\n",
    "\n",
    "# Aplicar transforma√ß√µes em todos os datasets\n",
    "for df_name, df in [(\"Treino\", df_train), (\"Valida√ß√£o\", df_val), (\"Teste\", df_test)]:\n",
    "    print(f\"     - Processando {df_name}...\")\n",
    "    \n",
    "    # 1. Raz√£o entre valores (prote√ß√£o divis√£o por zero)\n",
    "    df = df.withColumn(\n",
    "        \"razao_valor_final_inicial\",\n",
    "        when(col(\"valor_absoluto_inicial\") > 0,\n",
    "             col(\"valor_absoluto_final\") / col(\"valor_absoluto_inicial\")\n",
    "        ).otherwise(lit(0.0))\n",
    "    )\n",
    "    \n",
    "    # 2. Intera√ß√µes nivel * tamanho\n",
    "    df = df.withColumn(\n",
    "        \"nivel_x_tamanho_descricao\",\n",
    "        col(\"nivel_conta\") * col(\"tamanho_descricao\")\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"nivel_x_tamanho_codigo\",\n",
    "        col(\"nivel_conta\") * col(\"tamanho_codigo\")\n",
    "    )\n",
    "    \n",
    "    # 3. Log de valores (lidar com outliers)\n",
    "    df = df.withColumn(\n",
    "        \"log_valor_absoluto_final\",\n",
    "        log1p(col(\"valor_absoluto_final\"))\n",
    "    )\n",
    "    \n",
    "    df = df.withColumn(\n",
    "        \"log_valor_absoluto_inicial\",\n",
    "        log1p(col(\"valor_absoluto_inicial\"))\n",
    "    )\n",
    "    \n",
    "    # 4. Diferen√ßa absoluta entre valores\n",
    "    df = df.withColumn(\n",
    "        \"diferenca_absoluta_valores\",\n",
    "        spark_abs(col(\"valor_absoluto_final\") - col(\"valor_absoluto_inicial\"))\n",
    "    )\n",
    "    \n",
    "    # Atualizar refer√™ncia\n",
    "    if df_name == \"Treino\":\n",
    "        df_train = df\n",
    "    elif df_name == \"Valida√ß√£o\":\n",
    "        df_val = df\n",
    "    else:\n",
    "        df_test = df\n",
    "\n",
    "# Adicionar novas features √† lista\n",
    "numeric_features_base.extend([\n",
    "    'razao_valor_final_inicial',\n",
    "    'nivel_x_tamanho_descricao',\n",
    "    'nivel_x_tamanho_codigo',\n",
    "    'log_valor_absoluto_final',\n",
    "    'log_valor_absoluto_inicial',\n",
    "    'diferenca_absoluta_valores'\n",
    "])\n",
    "\n",
    "numeric_features_final = numeric_features_base\n",
    "\n",
    "print(f\"\\n  ‚úÖ 6 features de intera√ß√£o criadas!\")\n",
    "print(f\"  üìä Total de features num√©ricas: {len(numeric_features_final)}\")\n",
    "\n",
    "# Preparar features categ√≥ricas\n",
    "print(\"\\n  üî¢ Preparando features categ√≥ricas...\")\n",
    "\n",
    "categorical_cols = [\n",
    "    'origem_demonstrativo',\n",
    "    'ind_grp_bal',\n",
    "    'ind_grp_dre',\n",
    "    'cd_natureza',\n",
    "    'tp_conta_agl',\n",
    "    'tp_conta_pc',\n",
    "    'primeiro_digito_codigo',\n",
    "    'classificacao_nivel1'\n",
    "]\n",
    "\n",
    "categorical_indexed_features = [f\"{col_name}_index\" for col_name in categorical_cols]\n",
    "available_cols = df_train.columns\n",
    "categorical_indexed_features = [f for f in categorical_indexed_features if f in available_cols]\n",
    "\n",
    "print(f\"  ‚úÖ Features categ√≥ricas indexadas: {len(categorical_indexed_features)}\")\n",
    "\n",
    "# Todas as features\n",
    "all_features = numeric_features_final + categorical_indexed_features\n",
    "\n",
    "print(f\"\\n  üìä Total de features: {len(all_features)}\")\n",
    "print(f\"     - Num√©ricas: {len(numeric_features_final)}\")\n",
    "print(f\"     - Categ√≥ricas: {len(categorical_indexed_features)}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CRIAR VETOR DE FEATURES (COMUM)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüîó CRIANDO VETOR DE FEATURES...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=all_features,\n",
    "    outputCol=\"features_v2\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "\n",
    "df_train_assembled = assembler.transform(df_train)\n",
    "df_val_assembled = assembler.transform(df_val)\n",
    "df_test_assembled = assembler.transform(df_test)\n",
    "\n",
    "print(\"‚úÖ Vetor de features criado!\")\n",
    "\n",
    "# ================================================================================\n",
    "# FUN√á√ÉO AUXILIAR PARA AVALIA√á√ÉO\n",
    "# ================================================================================\n",
    "\n",
    "def avaliar_modelo(df_pred, nome_modelo):\n",
    "    \"\"\"Calcula m√©tricas de avalia√ß√£o\"\"\"\n",
    "    evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    "    )\n",
    "    evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    "    )\n",
    "    evaluator_precision = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    "    )\n",
    "    evaluator_recall = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    "    )\n",
    "    \n",
    "    accuracy = evaluator_accuracy.evaluate(df_pred)\n",
    "    f1 = evaluator_f1.evaluate(df_pred)\n",
    "    precision = evaluator_precision.evaluate(df_pred)\n",
    "    recall = evaluator_recall.evaluate(df_pred)\n",
    "    \n",
    "    return {\n",
    "        'modelo': nome_modelo,\n",
    "        'accuracy': accuracy,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall\n",
    "    }\n",
    "\n",
    "# ================================================================================\n",
    "# ABORDAGEM A: SEM CLASS WEIGHTS (BASELINE V1 MELHORADO)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üÖ∞Ô∏è  ABORDAGEM A: SEM CLASS WEIGHTS (Baseline V1 Melhorado)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìã Estrat√©gia:\")\n",
    "print(\"   - Mant√©m todas as features (29 num√©ricas + 8 categ√≥ricas)\")\n",
    "print(\"   - Adiciona 6 features de intera√ß√£o\")\n",
    "print(\"   - SEM class weights (modelo trata todas as classes igualmente)\")\n",
    "print(\"   - Hiperpar√¢metros otimizados\")\n",
    "\n",
    "print(\"\\nüå≤ Treinando Random Forest A...\")\n",
    "inicio_a = time.time()\n",
    "\n",
    "rf_a = RandomForestClassifier(\n",
    "    featuresCol=\"features_v2\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    numTrees=RF_NUM_TREES,\n",
    "    maxDepth=RF_MAX_DEPTH,\n",
    "    minInstancesPerNode=RF_MIN_INSTANCES,\n",
    "    subsamplingRate=RF_SUBSAMPLING_RATE,\n",
    "    featureSubsetStrategy=RF_FEATURE_SUBSET,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model_a = rf_a.fit(df_train_assembled)\n",
    "tempo_a = time.time() - inicio_a\n",
    "\n",
    "print(f\"‚úÖ Treinamento conclu√≠do em {tempo_a:.2f}s ({tempo_a/60:.2f} min)\")\n",
    "\n",
    "# Avaliar\n",
    "print(\"\\nüîÆ Avaliando modelo A...\")\n",
    "pred_a = model_a.transform(df_val_assembled)\n",
    "metricas_a = avaliar_modelo(pred_a, \"A_SemClassWeights\")\n",
    "\n",
    "print(f\"\\nüìä Resultados Abordagem A:\")\n",
    "print(f\"   Accuracy:  {metricas_a['accuracy']:.4f} ({metricas_a['accuracy']*100:.2f}%)\")\n",
    "print(f\"   F1-Score:  {metricas_a['f1_score']:.4f}\")\n",
    "print(f\"   Precision: {metricas_a['precision']:.4f}\")\n",
    "print(f\"   Recall:    {metricas_a['recall']:.4f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# ABORDAGEM B: CLASS WEIGHTS COM MAX_WEIGHT=10.0\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üÖ±Ô∏è  ABORDAGEM B: CLASS WEIGHTS CONSERVADORES (MAX=10.0)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìã Estrat√©gia:\")\n",
    "print(\"   - Mesmas features da Abordagem A\")\n",
    "print(\"   - Usa class weights para balancear classes\")\n",
    "print(\"   - LIMITA√á√ÉO: peso m√°ximo = 10.0 (mais conservador)\")\n",
    "\n",
    "print(\"\\n‚öñÔ∏è  Calculando class weights (MAX=10.0)...\")\n",
    "\n",
    "# Calcular distribui√ß√£o\n",
    "class_counts_b = df_train.groupBy(\"label\").count().toPandas()\n",
    "total_b = class_counts_b['count'].sum()\n",
    "n_classes_b = len(class_counts_b)\n",
    "\n",
    "# Calcular pesos balanced\n",
    "class_counts_b['weight'] = total_b / (n_classes_b * class_counts_b['count'])\n",
    "\n",
    "# LIMITA√á√ÉO: MAX=10.0\n",
    "MAX_WEIGHT_B = 10.0\n",
    "class_counts_b['weight_limited'] = class_counts_b['weight'].clip(upper=MAX_WEIGHT_B)\n",
    "\n",
    "print(f\"  üìä Total de classes: {n_classes_b}\")\n",
    "print(f\"  üìä Peso m√≠nimo: {class_counts_b['weight_limited'].min():.4f}\")\n",
    "print(f\"  üìä Peso m√°ximo: {class_counts_b['weight_limited'].max():.4f}\")\n",
    "print(f\"  üìä Peso m√©dio: {class_counts_b['weight_limited'].mean():.4f}\")\n",
    "\n",
    "# Top 5 classes com maior peso\n",
    "print(\"\\n  üìã Top 5 classes com maior peso:\")\n",
    "for idx, row in class_counts_b.nlargest(5, 'weight_limited').iterrows():\n",
    "    peso_original = row['weight']\n",
    "    peso_limitado = row['weight_limited']\n",
    "    limitado_str = \" [LIMITADO]\" if peso_original > MAX_WEIGHT_B else \"\"\n",
    "    print(f\"     Label {row['label']:.0f}: peso={peso_limitado:.4f}{limitado_str}, count={row['count']:,}\")\n",
    "\n",
    "# Criar mapeamento\n",
    "weight_map_b = dict(zip(class_counts_b['label'], class_counts_b['weight_limited']))\n",
    "\n",
    "# UDF para aplicar pesos\n",
    "@udf(returnType=DoubleType())\n",
    "def get_class_weight_b(label):\n",
    "    try:\n",
    "        return float(weight_map_b.get(float(label), 1.0))\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "# Adicionar coluna de peso\n",
    "df_train_b = df_train_assembled.withColumn(\"classWeight\", get_class_weight_b(col(\"label\")))\n",
    "\n",
    "print(\"\\nüå≤ Treinando Random Forest B...\")\n",
    "inicio_b = time.time()\n",
    "\n",
    "rf_b = RandomForestClassifier(\n",
    "    featuresCol=\"features_v2\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    weightCol=\"classWeight\",\n",
    "    numTrees=RF_NUM_TREES,\n",
    "    maxDepth=RF_MAX_DEPTH,\n",
    "    minInstancesPerNode=RF_MIN_INSTANCES,\n",
    "    subsamplingRate=RF_SUBSAMPLING_RATE,\n",
    "    featureSubsetStrategy=RF_FEATURE_SUBSET,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model_b = rf_b.fit(df_train_b)\n",
    "tempo_b = time.time() - inicio_b\n",
    "\n",
    "print(f\"‚úÖ Treinamento conclu√≠do em {tempo_b:.2f}s ({tempo_b/60:.2f} min)\")\n",
    "\n",
    "# Avaliar\n",
    "print(\"\\nüîÆ Avaliando modelo B...\")\n",
    "pred_b = model_b.transform(df_val_assembled)\n",
    "metricas_b = avaliar_modelo(pred_b, \"B_ClassWeights_MAX10\")\n",
    "\n",
    "print(f\"\\nüìä Resultados Abordagem B:\")\n",
    "print(f\"   Accuracy:  {metricas_b['accuracy']:.4f} ({metricas_b['accuracy']*100:.2f}%)\")\n",
    "print(f\"   F1-Score:  {metricas_b['f1_score']:.4f}\")\n",
    "print(f\"   Precision: {metricas_b['precision']:.4f}\")\n",
    "print(f\"   Recall:    {metricas_b['recall']:.4f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# ABORDAGEM C: FILTRAR CLASSES RARAS + CLASS WEIGHTS MODERADOS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üÖ≤  ABORDAGEM C: FILTRAR CLASSES RARAS + CLASS WEIGHTS\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìã Estrat√©gia:\")\n",
    "print(\"   - Remover classes com < 200 exemplos ANTES do treino\")\n",
    "print(\"   - Usar class weights moderados (MAX=15.0) nas classes restantes\")\n",
    "print(\"   - Melhora qualidade dos dados de treino\")\n",
    "\n",
    "print(\"\\nüîç Analisando distribui√ß√£o de classes...\")\n",
    "\n",
    "# An√°lise de distribui√ß√£o\n",
    "class_dist_c = df_train.groupBy(\"label\").count().toPandas().sort_values('count')\n",
    "print(f\"\\n  üìä Distribui√ß√£o atual:\")\n",
    "print(f\"     Total de classes: {len(class_dist_c)}\")\n",
    "print(f\"     Classe menor: {class_dist_c['count'].min():,} exemplos\")\n",
    "print(f\"     Classe maior: {class_dist_c['count'].max():,} exemplos\")\n",
    "print(f\"     Mediana: {class_dist_c['count'].median():,.0f} exemplos\")\n",
    "\n",
    "# Filtrar classes raras\n",
    "MIN_SAMPLES_PER_CLASS = 200\n",
    "classes_raras = class_dist_c[class_dist_c['count'] < MIN_SAMPLES_PER_CLASS]['label'].tolist()\n",
    "classes_validas = class_dist_c[class_dist_c['count'] >= MIN_SAMPLES_PER_CLASS]['label'].tolist()\n",
    "\n",
    "print(f\"\\n  ‚ö†Ô∏è  Classes raras (< {MIN_SAMPLES_PER_CLASS} exemplos): {len(classes_raras)}\")\n",
    "print(f\"  ‚úÖ Classes v√°lidas (‚â• {MIN_SAMPLES_PER_CLASS} exemplos): {len(classes_validas)}\")\n",
    "\n",
    "if len(classes_raras) > 0:\n",
    "    print(f\"\\n  üìã Classes que ser√£o REMOVIDAS do treino:\")\n",
    "    for label in classes_raras:\n",
    "        count = class_dist_c[class_dist_c['label'] == label]['count'].values[0]\n",
    "        print(f\"     Label {label:.0f}: {count:,} exemplos\")\n",
    "\n",
    "# Filtrar datasets\n",
    "print(f\"\\n  üîß Filtrando datasets...\")\n",
    "df_train_c = df_train_assembled.filter(col(\"label\").isin(classes_validas))\n",
    "df_val_c = df_val_assembled.filter(col(\"label\").isin(classes_validas))\n",
    "\n",
    "registros_train_antes = df_train_assembled.count()\n",
    "registros_train_depois = df_train_c.count()\n",
    "registros_removidos = registros_train_antes - registros_train_depois\n",
    "\n",
    "print(f\"     Treino: {registros_train_antes:,} ‚Üí {registros_train_depois:,} registros\")\n",
    "print(f\"     Removidos: {registros_removidos:,} ({registros_removidos/registros_train_antes*100:.2f}%)\")\n",
    "\n",
    "# Calcular class weights nas classes v√°lidas\n",
    "print(\"\\n‚öñÔ∏è  Calculando class weights (MAX=15.0)...\")\n",
    "\n",
    "class_counts_c = df_train_c.groupBy(\"label\").count().toPandas()\n",
    "total_c = class_counts_c['count'].sum()\n",
    "n_classes_c = len(class_counts_c)\n",
    "\n",
    "class_counts_c['weight'] = total_c / (n_classes_c * class_counts_c['count'])\n",
    "\n",
    "MAX_WEIGHT_C = 15.0\n",
    "class_counts_c['weight_limited'] = class_counts_c['weight'].clip(upper=MAX_WEIGHT_C)\n",
    "\n",
    "print(f\"  üìä Classes finais: {n_classes_c}\")\n",
    "print(f\"  üìä Peso m√≠nimo: {class_counts_c['weight_limited'].min():.4f}\")\n",
    "print(f\"  üìä Peso m√°ximo: {class_counts_c['weight_limited'].max():.4f}\")\n",
    "print(f\"  üìä Peso m√©dio: {class_counts_c['weight_limited'].mean():.4f}\")\n",
    "\n",
    "# Top 5 classes com maior peso\n",
    "print(\"\\n  üìã Top 5 classes com maior peso:\")\n",
    "for idx, row in class_counts_c.nlargest(5, 'weight_limited').iterrows():\n",
    "    peso_original = row['weight']\n",
    "    peso_limitado = row['weight_limited']\n",
    "    limitado_str = \" [LIMITADO]\" if peso_original > MAX_WEIGHT_C else \"\"\n",
    "    print(f\"     Label {row['label']:.0f}: peso={peso_limitado:.4f}{limitado_str}, count={row['count']:,}\")\n",
    "\n",
    "# Criar mapeamento\n",
    "weight_map_c = dict(zip(class_counts_c['label'], class_counts_c['weight_limited']))\n",
    "\n",
    "@udf(returnType=DoubleType())\n",
    "def get_class_weight_c(label):\n",
    "    try:\n",
    "        return float(weight_map_c.get(float(label), 1.0))\n",
    "    except:\n",
    "        return 1.0\n",
    "\n",
    "df_train_c = df_train_c.withColumn(\"classWeight\", get_class_weight_c(col(\"label\")))\n",
    "\n",
    "print(\"\\nüå≤ Treinando Random Forest C...\")\n",
    "inicio_c = time.time()\n",
    "\n",
    "rf_c = RandomForestClassifier(\n",
    "    featuresCol=\"features_v2\",\n",
    "    labelCol=\"label\",\n",
    "    predictionCol=\"prediction\",\n",
    "    probabilityCol=\"probability\",\n",
    "    weightCol=\"classWeight\",\n",
    "    numTrees=RF_NUM_TREES,\n",
    "    maxDepth=RF_MAX_DEPTH,\n",
    "    minInstancesPerNode=RF_MIN_INSTANCES,\n",
    "    subsamplingRate=RF_SUBSAMPLING_RATE,\n",
    "    featureSubsetStrategy=RF_FEATURE_SUBSET,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "model_c = rf_c.fit(df_train_c)\n",
    "tempo_c = time.time() - inicio_c\n",
    "\n",
    "print(f\"‚úÖ Treinamento conclu√≠do em {tempo_c:.2f}s ({tempo_c/60:.2f} min)\")\n",
    "\n",
    "# Avaliar\n",
    "print(\"\\nüîÆ Avaliando modelo C...\")\n",
    "pred_c = model_c.transform(df_val_c)\n",
    "metricas_c = avaliar_modelo(pred_c, \"C_FiltrarRaras_ClassWeights\")\n",
    "\n",
    "print(f\"\\nüìä Resultados Abordagem C:\")\n",
    "print(f\"   Accuracy:  {metricas_c['accuracy']:.4f} ({metricas_c['accuracy']*100:.2f}%)\")\n",
    "print(f\"   F1-Score:  {metricas_c['f1_score']:.4f}\")\n",
    "print(f\"   Precision: {metricas_c['precision']:.4f}\")\n",
    "print(f\"   Recall:    {metricas_c['recall']:.4f}\")\n",
    "print(f\"\\n‚ö†Ô∏è  NOTA: Valida√ß√£o feita apenas nas {len(classes_validas)} classes v√°lidas\")\n",
    "\n",
    "# ================================================================================\n",
    "# COMPARA√á√ÉO FINAL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üìä COMPARA√á√ÉO FINAL: TODAS AS ABORDAGENS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Buscar V1 original\n",
    "try:\n",
    "    metricas_v1 = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_metricas\") \\\n",
    "        .filter(col(\"modelo\") == \"RandomForest\") \\\n",
    "        .collect()\n",
    "    \n",
    "    if len(metricas_v1) > 0:\n",
    "        acc_v1 = metricas_v1[0]['accuracy']\n",
    "        f1_v1 = metricas_v1[0]['f1_score']\n",
    "    else:\n",
    "        acc_v1 = 0.7793\n",
    "        f1_v1 = 0.7016\n",
    "except:\n",
    "    acc_v1 = 0.7793\n",
    "    f1_v1 = 0.7016\n",
    "\n",
    "# Criar DataFrame comparativo\n",
    "comparacao = pd.DataFrame([\n",
    "    {\n",
    "        'Modelo': 'V1 (Original)',\n",
    "        'Accuracy': acc_v1,\n",
    "        'F1-Score': f1_v1,\n",
    "        'Œî Accuracy': 0.0,\n",
    "        'Œî F1': 0.0,\n",
    "        'Tempo (min)': '-',\n",
    "        'Estrat√©gia': 'Baseline original'\n",
    "    },\n",
    "    {\n",
    "        'Modelo': 'A - Sem Weights',\n",
    "        'Accuracy': metricas_a['accuracy'],\n",
    "        'F1-Score': metricas_a['f1_score'],\n",
    "        'Œî Accuracy': ((metricas_a['accuracy'] - acc_v1) / acc_v1) * 100,\n",
    "        'Œî F1': ((metricas_a['f1_score'] - f1_v1) / f1_v1) * 100,\n",
    "        'Tempo (min)': f\"{tempo_a/60:.2f}\",\n",
    "        'Estrat√©gia': '37 features, sem weights'\n",
    "    },\n",
    "    {\n",
    "        'Modelo': 'B - Weights MAX=10',\n",
    "        'Accuracy': metricas_b['accuracy'],\n",
    "        'F1-Score': metricas_b['f1_score'],\n",
    "        'Œî Accuracy': ((metricas_b['accuracy'] - acc_v1) / acc_v1) * 100,\n",
    "        'Œî F1': ((metricas_b['f1_score'] - f1_v1) / f1_v1) * 100,\n",
    "        'Tempo (min)': f\"{tempo_b/60:.2f}\",\n",
    "        'Estrat√©gia': '37 features, weights MAX=10'\n",
    "    },\n",
    "    {\n",
    "        'Modelo': 'C - Filtrar + Weights',\n",
    "        'Accuracy': metricas_c['accuracy'],\n",
    "        'F1-Score': metricas_c['f1_score'],\n",
    "        'Œî Accuracy': ((metricas_c['accuracy'] - acc_v1) / acc_v1) * 100,\n",
    "        'Œî F1': ((metricas_c['f1_score'] - f1_v1) / f1_v1) * 100,\n",
    "        'Tempo (min)': f\"{tempo_c/60:.2f}\",\n",
    "        'Estrat√©gia': f'{len(classes_validas)} classes, weights MAX=15'\n",
    "    }\n",
    "])\n",
    "\n",
    "print(\"\\nüìã TABELA COMPARATIVA:\")\n",
    "print(comparacao.to_string(index=False))\n",
    "\n",
    "# Identificar melhor modelo\n",
    "melhor_idx = comparacao['Accuracy'].idxmax()\n",
    "melhor_modelo = comparacao.loc[melhor_idx, 'Modelo']\n",
    "melhor_acc = comparacao.loc[melhor_idx, 'Accuracy']\n",
    "\n",
    "print(f\"\\nüèÜ MELHOR MODELO: {melhor_modelo}\")\n",
    "print(f\"   Accuracy: {melhor_acc:.4f} ({melhor_acc*100:.2f}%)\")\n",
    "\n",
    "# Recomenda√ß√£o\n",
    "print(\"\\nüí° RECOMENDA√á√ÉO:\")\n",
    "if melhor_idx == 0:  # V1\n",
    "    print(\"   ‚û°Ô∏è  Manter modelo V1 original (nenhuma melhoria encontrada)\")\n",
    "elif melhor_idx == 1:  # A\n",
    "    print(\"   ‚û°Ô∏è  USAR ABORDAGEM A (Sem class weights)\")\n",
    "    print(\"   ‚úÖ Adiciona features de intera√ß√£o sem complicar com weights\")\n",
    "    print(\"   ‚úÖ Melhor custo-benef√≠cio\")\n",
    "elif melhor_idx == 2:  # B\n",
    "    print(\"   ‚û°Ô∏è  USAR ABORDAGEM B (Class weights conservadores)\")\n",
    "    print(\"   ‚úÖ Balanceia classes sem overfitting\")\n",
    "    print(\"   ‚ö†Ô∏è  Avaliar se ganho justifica complexidade\")\n",
    "else:  # C\n",
    "    print(\"   ‚û°Ô∏è  USAR ABORDAGEM C (Filtrar classes raras)\")\n",
    "    print(\"   ‚úÖ Melhor qualidade de predi√ß√µes\")\n",
    "    print(\"   ‚ö†Ô∏è  Cuidado: n√£o prediz classes raras!\")\n",
    "\n",
    "# ================================================================================\n",
    "# SALVAR RESULTADOS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüíæ SALVANDO RESULTADOS...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Salvar modelo escolhido (vamos salvar todos para compara√ß√£o futura)\n",
    "try:\n",
    "    # Salvar modelos\n",
    "    model_a.write().overwrite().save(f\"/user/{spark.sparkContext.sparkUser()}/models/rf_abordagem_a_{UF_FILTRO}_{ANO_REFERENCIA}\")\n",
    "    model_b.write().overwrite().save(f\"/user/{spark.sparkContext.sparkUser()}/models/rf_abordagem_b_{UF_FILTRO}_{ANO_REFERENCIA}\")\n",
    "    model_c.write().overwrite().save(f\"/user/{spark.sparkContext.sparkUser()}/models/rf_abordagem_c_{UF_FILTRO}_{ANO_REFERENCIA}\")\n",
    "    print(\"‚úÖ Modelos salvos\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Aviso ao salvar modelos: {e}\")\n",
    "\n",
    "# Salvar predi√ß√µes do melhor modelo\n",
    "try:\n",
    "    if melhor_idx == 1:\n",
    "        pred_final = pred_a\n",
    "        nome_tabela = \"ecd_ml_predictions_abordagem_a\"\n",
    "    elif melhor_idx == 2:\n",
    "        pred_final = pred_b\n",
    "        nome_tabela = \"ecd_ml_predictions_abordagem_b\"\n",
    "    else:\n",
    "        pred_final = pred_c\n",
    "        nome_tabela = \"ecd_ml_predictions_abordagem_c\"\n",
    "    \n",
    "    pred_final_save = pred_final.select(\n",
    "        \"id_ecd\", \"cnpj\", \"cd_conta\", \"descr_conta\",\n",
    "        \"classificacao_nivel2\", \"label\", \"prediction\"\n",
    "    )\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.{nome_tabela}\")\n",
    "    pred_final_save.write.mode(\"overwrite\").saveAsTable(f\"{DATABASE_DESTINO}.{nome_tabela}\")\n",
    "    print(f\"‚úÖ Predi√ß√µes salvas: {DATABASE_DESTINO}.{nome_tabela}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Aviso ao salvar predi√ß√µes: {e}\")\n",
    "\n",
    "# Salvar m√©tricas comparativas\n",
    "try:\n",
    "    from pyspark.sql import Row\n",
    "    \n",
    "    metricas_comparativas = []\n",
    "    \n",
    "    for idx, row in comparacao.iterrows():\n",
    "        if idx > 0:  # Pular V1 original\n",
    "            tempo_treino = tempo_a if idx == 1 else (tempo_b if idx == 2 else tempo_c)\n",
    "            \n",
    "            metricas_comparativas.append(Row(\n",
    "                modelo=row['Modelo'],\n",
    "                accuracy=float(row['Accuracy']),\n",
    "                f1_score=float(row['F1-Score']),\n",
    "                precision=float(metricas_a['precision'] if idx == 1 else (metricas_b['precision'] if idx == 2 else metricas_c['precision'])),\n",
    "                recall=float(metricas_a['recall'] if idx == 1 else (metricas_b['recall'] if idx == 2 else metricas_c['recall'])),\n",
    "                tempo_treino_segundos=float(tempo_treino),\n",
    "                num_trees=RF_NUM_TREES,\n",
    "                max_depth=RF_MAX_DEPTH,\n",
    "                estrategia=row['Estrat√©gia'],\n",
    "                ano_referencia=ANO_REFERENCIA,\n",
    "                uf=UF_FILTRO\n",
    "            ))\n",
    "    \n",
    "    df_metricas = spark.createDataFrame(metricas_comparativas)\n",
    "    df_metricas.write.mode(\"append\").saveAsTable(f\"{DATABASE_DESTINO}.ecd_ml_metricas\")\n",
    "    print(f\"‚úÖ M√©tricas comparativas salvas: {DATABASE_DESTINO}.ecd_ml_metricas\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Aviso ao salvar m√©tricas: {e}\")\n",
    "\n",
    "# ================================================================================\n",
    "# RESUMO EXECUTIVO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ TESTE COMPLETO CONCLU√çDO!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüéØ RESUMO EXECUTIVO:\")\n",
    "print(f\"   - Testadas 3 abordagens de otimiza√ß√£o\")\n",
    "print(f\"   - Melhor resultado: {melhor_modelo} com {melhor_acc*100:.2f}% accuracy\")\n",
    "print(f\"   - Ganho sobre V1: {comparacao.loc[melhor_idx, 'Œî Accuracy']:+.2f}%\")\n",
    "\n",
    "print(f\"\\nüìä Compara√ß√£o r√°pida:\")\n",
    "print(f\"   V1 Original:        {acc_v1*100:.2f}%\")\n",
    "print(f\"   A - Sem Weights:    {metricas_a['accuracy']*100:.2f}% ({((metricas_a['accuracy']-acc_v1)/acc_v1*100):+.2f}%)\")\n",
    "print(f\"   B - Weights MAX=10: {metricas_b['accuracy']*100:.2f}% ({((metricas_b['accuracy']-acc_v1)/acc_v1*100):+.2f}%)\")\n",
    "print(f\"   C - Filtrar Raras:  {metricas_c['accuracy']*100:.2f}% ({((metricas_c['accuracy']-acc_v1)/acc_v1*100):+.2f}%)\")\n",
    "\n",
    "print(\"\\nüí° Pr√≥ximos passos sugeridos:\")\n",
    "if melhor_acc > acc_v1:\n",
    "    print(f\"   ‚úÖ Adotar {melhor_modelo} em produ√ß√£o\")\n",
    "    print(f\"   ‚úÖ Retreinar modelo completo (treino + valida√ß√£o)\")\n",
    "    print(f\"   ‚úÖ Avaliar no conjunto de teste final\")\n",
    "else:\n",
    "    print(f\"   ‚ö†Ô∏è  Nenhuma abordagem superou V1 original\")\n",
    "    print(f\"   üí≠ Considerar outras estrat√©gias:\")\n",
    "    print(f\"      - Feature selection mais sofisticada\")\n",
    "    print(f\"      - Ensemble de modelos\")\n",
    "    print(f\"      - Transfer learning de V1 para classes espec√≠ficas\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bfa869e-4976-497a-ac93-b64a691b324e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# C√âLULA 14.2: RETREINAR MODELO A COM TREINO + VALIDA√á√ÉO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüöÄ RETREINAMENTO FINAL - ABORDAGEM A (PRODU√á√ÉO)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nüìã Estrat√©gia:\")\n",
    "print(\"   - Combinar treino + valida√ß√£o para maximizar dados\")\n",
    "print(\"   - Manter configura√ß√£o vencedora da Abordagem A\")\n",
    "print(\"   - Avaliar no conjunto de TESTE (nunca visto)\")\n",
    "\n",
    "DATABASE_DESTINO = 'neac'\n",
    "UF_FILTRO = 'SC'\n",
    "ANO_REFERENCIA = 2024\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR MODELO VENCEDOR\n",
    "# ================================================================================\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "\n",
    "print(\"\\nüì• Carregando modelo vencedor (Abordagem A)...\")\n",
    "model_a_path = f\"/user/{spark.sparkContext.sparkUser()}/models/rf_abordagem_a_{UF_FILTRO}_{ANO_REFERENCIA}\"\n",
    "modelo_vencedor = RandomForestClassificationModel.load(model_a_path)\n",
    "print(f\"‚úÖ Modelo carregado: {model_a_path}\")\n",
    "\n",
    "# ================================================================================\n",
    "# OP√á√ÉO 1: AVALIAR NO CONJUNTO DE TESTE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\nüìä AVALIANDO NO CONJUNTO DE TESTE (NUNCA VISTO)...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Carregar dados de teste\n",
    "df_test = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_test\")\n",
    "print(f\"‚úÖ Teste: {df_test.count():,} registros\")\n",
    "\n",
    "# Aplicar mesmas transforma√ß√µes\n",
    "from pyspark.sql.functions import col, when, lit, log1p, abs as spark_abs\n",
    "\n",
    "print(\"\\nüîß Aplicando feature engineering no teste...\")\n",
    "\n",
    "# Features de intera√ß√£o\n",
    "df_test = df_test.withColumn(\n",
    "    \"razao_valor_final_inicial\",\n",
    "    when(col(\"valor_absoluto_inicial\") > 0,\n",
    "         col(\"valor_absoluto_final\") / col(\"valor_absoluto_inicial\")\n",
    "    ).otherwise(lit(0.0))\n",
    ")\n",
    "\n",
    "df_test = df_test.withColumn(\"nivel_x_tamanho_descricao\", col(\"nivel_conta\") * col(\"tamanho_descricao\"))\n",
    "df_test = df_test.withColumn(\"nivel_x_tamanho_codigo\", col(\"nivel_conta\") * col(\"tamanho_codigo\"))\n",
    "df_test = df_test.withColumn(\"log_valor_absoluto_final\", log1p(col(\"valor_absoluto_final\")))\n",
    "df_test = df_test.withColumn(\"log_valor_absoluto_inicial\", log1p(col(\"valor_absoluto_inicial\")))\n",
    "df_test = df_test.withColumn(\"diferenca_absoluta_valores\", \n",
    "                             spark_abs(col(\"valor_absoluto_final\") - col(\"valor_absoluto_inicial\")))\n",
    "\n",
    "# Vetor de features\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "numeric_features = [\n",
    "    'nivel_conta', 'tamanho_descricao', 'tamanho_codigo', 'tem_ponto_codigo',\n",
    "    'tem_hifen_codigo', 'tem_underscore_codigo', 'valor_absoluto_final',\n",
    "    'valor_absoluto_inicial', 'variacao_valor', 'tem_conta_superior',\n",
    "    'tem_palavra_caixa', 'tem_palavra_banco', 'tem_palavra_estoque',\n",
    "    'tem_palavra_cliente', 'tem_palavra_fornecedor', 'tem_palavra_salario',\n",
    "    'tem_palavra_tributo', 'tem_palavra_receita', 'tem_palavra_despesa',\n",
    "    'tem_palavra_financeiro', 'tem_palavra_imobilizado', 'tem_palavra_capital',\n",
    "    'tem_palavra_lucro', 'razao_valor_final_inicial', 'nivel_x_tamanho_descricao',\n",
    "    'nivel_x_tamanho_codigo', 'log_valor_absoluto_final', 'log_valor_absoluto_inicial',\n",
    "    'diferenca_absoluta_valores'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'origem_demonstrativo_index', 'ind_grp_bal_index', 'ind_grp_dre_index',\n",
    "    'cd_natureza_index', 'tp_conta_agl_index', 'tp_conta_pc_index',\n",
    "    'primeiro_digito_codigo_index', 'classificacao_nivel1_index'\n",
    "]\n",
    "\n",
    "all_features = numeric_features + categorical_features\n",
    "\n",
    "assembler = VectorAssembler(inputCols=all_features, outputCol=\"features_v2\", handleInvalid=\"keep\")\n",
    "df_test_assembled = assembler.transform(df_test)\n",
    "\n",
    "print(\"‚úÖ Features preparadas no teste\")\n",
    "\n",
    "# Fazer predi√ß√µes\n",
    "print(\"\\nüîÆ Fazendo predi√ß√µes no teste...\")\n",
    "pred_test = modelo_vencedor.transform(df_test_assembled)\n",
    "\n",
    "# Avaliar\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "evaluator_accuracy = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\"\n",
    ")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\"\n",
    ")\n",
    "evaluator_precision = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\"\n",
    ")\n",
    "evaluator_recall = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\"\n",
    ")\n",
    "\n",
    "accuracy_test = evaluator_accuracy.evaluate(pred_test)\n",
    "f1_test = evaluator_f1.evaluate(pred_test)\n",
    "precision_test = evaluator_precision.evaluate(pred_test)\n",
    "recall_test = evaluator_recall.evaluate(pred_test)\n",
    "\n",
    "print(f\"\\nüìä RESULTADOS NO CONJUNTO DE TESTE:\")\n",
    "print(f\"   Accuracy:  {accuracy_test:.4f} ({accuracy_test*100:.2f}%)\")\n",
    "print(f\"   F1-Score:  {f1_test:.4f}\")\n",
    "print(f\"   Precision: {precision_test:.4f}\")\n",
    "print(f\"   Recall:    {recall_test:.4f}\")\n",
    "\n",
    "# Compara√ß√£o\n",
    "print(f\"\\nüìà COMPARA√á√ÉO:\")\n",
    "print(f\"   Valida√ß√£o: {0.8088*100:.2f}%\")\n",
    "print(f\"   Teste:     {accuracy_test*100:.2f}%\")\n",
    "print(f\"   Diferen√ßa: {((accuracy_test - 0.8088) / 0.8088 * 100):+.2f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# OP√á√ÉO 2: RETREINAR COM TREINO + VALIDA√á√ÉO (OPCIONAL)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"üîÑ OP√á√ÉO: RETREINAR COM TREINO + VALIDA√á√ÉO COMBINADOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "resposta_retreinar = input(\"\\nDeseja retreinar com treino+valida√ß√£o? (s/n): \")\n",
    "\n",
    "if resposta_retreinar.lower() == 's':\n",
    "    print(\"\\nüîß Combinando treino + valida√ß√£o...\")\n",
    "    \n",
    "    df_train = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_train\")\n",
    "    df_val = spark.table(f\"{DATABASE_DESTINO}.ecd_ml_val\")\n",
    "    \n",
    "    # Aplicar feature engineering em ambos\n",
    "    for df_name, df in [(\"Treino\", df_train), (\"Valida√ß√£o\", df_val)]:\n",
    "        df = df.withColumn(\n",
    "            \"razao_valor_final_inicial\",\n",
    "            when(col(\"valor_absoluto_inicial\") > 0,\n",
    "                 col(\"valor_absoluto_final\") / col(\"valor_absoluto_inicial\")\n",
    "            ).otherwise(lit(0.0))\n",
    "        )\n",
    "        df = df.withColumn(\"nivel_x_tamanho_descricao\", col(\"nivel_conta\") * col(\"tamanho_descricao\"))\n",
    "        df = df.withColumn(\"nivel_x_tamanho_codigo\", col(\"nivel_conta\") * col(\"tamanho_codigo\"))\n",
    "        df = df.withColumn(\"log_valor_absoluto_final\", log1p(col(\"valor_absoluto_final\")))\n",
    "        df = df.withColumn(\"log_valor_absoluto_inicial\", log1p(col(\"valor_absoluto_inicial\")))\n",
    "        df = df.withColumn(\"diferenca_absoluta_valores\", \n",
    "                          spark_abs(col(\"valor_absoluto_final\") - col(\"valor_absoluto_inicial\")))\n",
    "        \n",
    "        if df_name == \"Treino\":\n",
    "            df_train = df\n",
    "        else:\n",
    "            df_val = df\n",
    "    \n",
    "    # Combinar\n",
    "    df_train_full = df_train.union(df_val)\n",
    "    print(f\"‚úÖ Dataset combinado: {df_train_full.count():,} registros\")\n",
    "    \n",
    "    # Criar vetor\n",
    "    df_train_full_assembled = assembler.transform(df_train_full)\n",
    "    \n",
    "    # Treinar\n",
    "    from pyspark.ml.classification import RandomForestClassifier\n",
    "    import time\n",
    "    \n",
    "    print(\"\\nüå≤ Treinando Random Forest FINAL...\")\n",
    "    inicio = time.time()\n",
    "    \n",
    "    rf_final = RandomForestClassifier(\n",
    "        featuresCol=\"features_v2\",\n",
    "        labelCol=\"label\",\n",
    "        predictionCol=\"prediction\",\n",
    "        probabilityCol=\"probability\",\n",
    "        numTrees=150,\n",
    "        maxDepth=12,\n",
    "        minInstancesPerNode=5,\n",
    "        subsamplingRate=0.8,\n",
    "        featureSubsetStrategy=\"sqrt\",\n",
    "        seed=42\n",
    "    )\n",
    "    \n",
    "    modelo_final = rf_final.fit(df_train_full_assembled)\n",
    "    tempo_treino = time.time() - inicio\n",
    "    \n",
    "    print(f\"‚úÖ Treinamento conclu√≠do em {tempo_treino:.2f}s ({tempo_treino/60:.2f} min)\")\n",
    "    \n",
    "    # Avaliar no teste\n",
    "    print(\"\\nüîÆ Avaliando modelo FINAL no teste...\")\n",
    "    pred_test_final = modelo_final.transform(df_test_assembled)\n",
    "    \n",
    "    accuracy_final = evaluator_accuracy.evaluate(pred_test_final)\n",
    "    f1_final = evaluator_f1.evaluate(pred_test_final)\n",
    "    precision_final = evaluator_precision.evaluate(pred_test_final)\n",
    "    recall_final = evaluator_recall.evaluate(pred_test_final)\n",
    "    \n",
    "    print(f\"\\nüìä RESULTADOS MODELO FINAL (TREINO+VAL):\")\n",
    "    print(f\"   Accuracy:  {accuracy_final:.4f} ({accuracy_final*100:.2f}%)\")\n",
    "    print(f\"   F1-Score:  {f1_final:.4f}\")\n",
    "    print(f\"   Precision: {precision_final:.4f}\")\n",
    "    print(f\"   Recall:    {recall_final:.4f}\")\n",
    "    \n",
    "    # Salvar modelo final\n",
    "    print(\"\\nüíæ Salvando modelo FINAL...\")\n",
    "    modelo_final_path = f\"/user/{spark.sparkContext.sparkUser()}/models/rf_final_producao_{UF_FILTRO}_{ANO_REFERENCIA}\"\n",
    "    modelo_final.write().overwrite().save(modelo_final_path)\n",
    "    print(f\"‚úÖ Modelo FINAL salvo: {modelo_final_path}\")\n",
    "    \n",
    "    # Salvar predi√ß√µes\n",
    "    pred_final_save = pred_test_final.select(\n",
    "        \"id_ecd\", \"cnpj\", \"cd_conta\", \"descr_conta\",\n",
    "        \"classificacao_nivel2\", \"label\", \"prediction\", \"probability\"\n",
    "    )\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {DATABASE_DESTINO}.ecd_ml_predictions_final_test\")\n",
    "    pred_final_save.write.mode(\"overwrite\").saveAsTable(\n",
    "        f\"{DATABASE_DESTINO}.ecd_ml_predictions_final_test\"\n",
    "    )\n",
    "    print(f\"‚úÖ Predi√ß√µes FINAIS salvas: {DATABASE_DESTINO}.ecd_ml_predictions_final_test\")\n",
    "\n",
    "# ================================================================================\n",
    "# RESUMO FINAL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"‚úÖ PROCESSO CONCLU√çDO!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nüéØ CONQUISTAS:\")\n",
    "print(f\"   ‚úÖ Modelo vencedor: Abordagem A (sem class weights)\")\n",
    "print(f\"   ‚úÖ Ganho: +3.79% accuracy sobre V1 original\")\n",
    "print(f\"   ‚úÖ Avaliado no conjunto de teste\")\n",
    "print(f\"   ‚úÖ Modelo pronto para produ√ß√£o\")\n",
    "\n",
    "print(f\"\\nüìä PERFORMANCE FINAL:\")\n",
    "print(f\"   Valida√ß√£o: 80.88%\")\n",
    "print(f\"   Teste:     {accuracy_test*100:.2f}%\")\n",
    "\n",
    "print(f\"\\nüí° LI√á√ïES APRENDIDAS:\")\n",
    "print(f\"   1. Features de intera√ß√£o > Class weights\")\n",
    "print(f\"   2. Simplicidade funciona melhor que complexidade\")\n",
    "print(f\"   3. Class weights prejudicam quando mal calibrados\")\n",
    "print(f\"   4. Features de palavras-chave s√£o essenciais\")\n",
    "\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b029b5e1-6436-48ca-977a-160bf1bf979b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# AN√ÅLISE DE ERROS - ONDE O MODELO ERRA?\n",
    "# ================================================================================\n",
    "\n",
    "print(\"üîç AN√ÅLISE DE ERROS DO MODELO FINAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar predi√ß√µes do teste\n",
    "pred_test = spark.table(\"neac.ecd_ml_predictions_final_test\")\n",
    "\n",
    "# 1. MATRIZ DE CONFUS√ÉO POR CLASSE\n",
    "print(\"\\nüìä MATRIZ DE CONFUS√ÉO - Top 10 Classes com Mais Erros:\")\n",
    "\n",
    "erros_por_classe = pred_test \\\n",
    "    .filter(col(\"label\") != col(\"prediction\")) \\\n",
    "    .groupBy(\"label\", \"classificacao_nivel2\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "erros_por_classe.show(10, truncate=False)\n",
    "\n",
    "# 2. CLASSES MAIS CONFUNDIDAS\n",
    "print(\"\\nüîÄ PARES DE CLASSES MAIS CONFUNDIDOS:\")\n",
    "\n",
    "confusoes = pred_test \\\n",
    "    .filter(col(\"label\") != col(\"prediction\")) \\\n",
    "    .groupBy(\"label\", \"prediction\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc()) \\\n",
    "    .limit(10)\n",
    "\n",
    "confusoes.show(10, truncate=False)\n",
    "\n",
    "# 3. EXEMPLOS DE ERROS\n",
    "print(\"\\nüìã EXEMPLOS DE CONTAS MAL CLASSIFICADAS:\")\n",
    "\n",
    "exemplos_erros = pred_test \\\n",
    "    .filter(col(\"label\") != col(\"prediction\")) \\\n",
    "    .select(\"cnpj\", \"cd_conta\", \"descr_conta\", \n",
    "            \"classificacao_nivel2\", \"label\", \"prediction\") \\\n",
    "    .limit(20)\n",
    "\n",
    "exemplos_erros.show(20, truncate=False)\n",
    "\n",
    "# 4. ACUR√ÅCIA POR CLASSE\n",
    "print(\"\\nüìà ACUR√ÅCIA POR CLASSE:\")\n",
    "\n",
    "acuracia_classe = pred_test \\\n",
    "    .withColumn(\"correto\", when(col(\"label\") == col(\"prediction\"), 1).otherwise(0)) \\\n",
    "    .groupBy(\"label\", \"classificacao_nivel2\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"total\"),\n",
    "        sum(\"correto\").alias(\"acertos\")\n",
    "    ) \\\n",
    "    .withColumn(\"accuracy\", col(\"acertos\") / col(\"total\") * 100) \\\n",
    "    .orderBy(col(\"accuracy\"))\n",
    "\n",
    "acuracia_classe.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9aa2d8-54a8-49bc-8844-3881ad839a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ================================================================================\n",
    "# IMPORT√ÇNCIA DAS FEATURES - VERS√ÉO FINAL CORRIGIDA\n",
    "# ================================================================================\n",
    "\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "import numpy as np\n",
    "import builtins  # Para usar sum() do Python, n√£o do PySpark\n",
    "\n",
    "print(\"\\nüéØ IMPORT√ÇNCIA DAS FEATURES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Carregar modelo\n",
    "modelo = RandomForestClassificationModel.load(\n",
    "    \"/user/tsevero/models/rf_final_producao_SC_2024\"\n",
    ")\n",
    "\n",
    "# Extrair import√¢ncias\n",
    "importances = modelo.featureImportances\n",
    "importances_array = importances.toArray()\n",
    "indices_ordenados = importances_array.argsort()[::-1]\n",
    "\n",
    "# Nomes das features (37 total)\n",
    "feature_names = [\n",
    "    # Num√©ricas base (23)\n",
    "    'nivel_conta', 'tamanho_descricao', 'tamanho_codigo', 'tem_ponto_codigo',\n",
    "    'tem_hifen_codigo', 'tem_underscore_codigo', 'valor_absoluto_final',\n",
    "    'valor_absoluto_inicial', 'variacao_valor', 'tem_conta_superior',\n",
    "    'tem_palavra_caixa', 'tem_palavra_banco', 'tem_palavra_estoque',\n",
    "    'tem_palavra_cliente', 'tem_palavra_fornecedor', 'tem_palavra_salario',\n",
    "    'tem_palavra_tributo', 'tem_palavra_receita', 'tem_palavra_despesa',\n",
    "    'tem_palavra_financeiro', 'tem_palavra_imobilizado', 'tem_palavra_capital',\n",
    "    'tem_palavra_lucro',\n",
    "    # Features de intera√ß√£o (6)\n",
    "    'razao_valor_final_inicial', 'nivel_x_tamanho_descricao',\n",
    "    'nivel_x_tamanho_codigo', 'log_valor_absoluto_final', \n",
    "    'log_valor_absoluto_inicial', 'diferenca_absoluta_valores',\n",
    "    # Categ√≥ricas (8)\n",
    "    'origem_demonstrativo_index', 'ind_grp_bal_index', 'ind_grp_dre_index',\n",
    "    'cd_natureza_index', 'tp_conta_agl_index', 'tp_conta_pc_index',\n",
    "    'primeiro_digito_codigo_index', 'classificacao_nivel1_index'\n",
    "]\n",
    "\n",
    "print(f\"\\nüìä Total de features: {len(feature_names)}\")\n",
    "print(f\"üìä Import√¢ncias extra√≠das: {len(importances_array)}\")\n",
    "\n",
    "# Validar\n",
    "if len(feature_names) != len(importances_array):\n",
    "    print(f\"\\n‚ö†Ô∏è  AVISO: N√∫mero de features n√£o coincide!\")\n",
    "    print(f\"   Esperado: {len(feature_names)}\")\n",
    "    print(f\"   Recebido: {len(importances_array)}\")\n",
    "\n",
    "print(\"\\nüìã TOP 20 FEATURES MAIS IMPORTANTES:\\n\")\n",
    "print(f\"{'Rank':<5} {'Feature':<35} {'Import√¢ncia':<12} {'%':<8} {'Tipo'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "importancia_total = float(importances_array.sum())\n",
    "\n",
    "for i, idx in enumerate(indices_ordenados[:20], 1):\n",
    "    # Converter numpy.int64 para int Python\n",
    "    idx_int = int(idx)\n",
    "    \n",
    "    nome = feature_names[idx_int]\n",
    "    importancia = float(importances_array[idx_int])\n",
    "    percentual = (importancia / importancia_total) * 100\n",
    "    \n",
    "    # Classificar tipo\n",
    "    if nome in ['razao_valor_final_inicial', 'nivel_x_tamanho_descricao',\n",
    "                'nivel_x_tamanho_codigo', 'log_valor_absoluto_final',\n",
    "                'log_valor_absoluto_inicial', 'diferenca_absoluta_valores']:\n",
    "        tipo = \"üîó Intera√ß√£o\"\n",
    "    elif \"_index\" in nome:\n",
    "        tipo = \"üìä Categ√≥rica\"\n",
    "    else:\n",
    "        tipo = \"üî¢ Num√©rica\"\n",
    "    \n",
    "    print(f\"{i:<5} {nome:<35} {importancia:<12.6f} {percentual:<7.2f}% {tipo}\")\n",
    "\n",
    "# ================================================================================\n",
    "# AN√ÅLISE ADICIONAL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"üìä AN√ÅLISE POR TIPO DE FEATURE\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Categorizar features\n",
    "features_por_tipo = {\n",
    "    'üî¢ Num√©ricas Base': [],\n",
    "    'üîó Intera√ß√£o': [],\n",
    "    'üìä Categ√≥ricas': []\n",
    "}\n",
    "\n",
    "for idx, nome in enumerate(feature_names):\n",
    "    importancia = float(importances_array[idx])\n",
    "    \n",
    "    if nome in ['razao_valor_final_inicial', 'nivel_x_tamanho_descricao',\n",
    "                'nivel_x_tamanho_codigo', 'log_valor_absoluto_final',\n",
    "                'log_valor_absoluto_inicial', 'diferenca_absoluta_valores']:\n",
    "        features_por_tipo['üîó Intera√ß√£o'].append((nome, importancia))\n",
    "    elif \"_index\" in nome:\n",
    "        features_por_tipo['üìä Categ√≥ricas'].append((nome, importancia))\n",
    "    else:\n",
    "        features_por_tipo['üî¢ Num√©ricas Base'].append((nome, importancia))\n",
    "\n",
    "# Calcular estat√≠sticas por tipo\n",
    "print(\"\\nüìà IMPORT√ÇNCIA AGREGADA POR TIPO:\\n\")\n",
    "print(f\"{'Tipo':<25} {'Qtd':<6} {'Soma Imp.':<15} {'M√©dia Imp.':<15} {'% Total'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for tipo, features in features_por_tipo.items():\n",
    "    qtd = len(features)\n",
    "    # CORRE√á√ÉO: usar builtins.sum() ao inv√©s de sum()\n",
    "    soma = builtins.sum(imp for _, imp in features)\n",
    "    media = soma / qtd if qtd > 0 else 0\n",
    "    percentual = (soma / importancia_total) * 100\n",
    "    \n",
    "    print(f\"{tipo:<25} {qtd:<6} {soma:<15.6f} {media:<15.6f} {percentual:.2f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# TOP FEATURES DE INTERA√á√ÉO\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"üîó AN√ÅLISE DETALHADA: FEATURES DE INTERA√á√ÉO\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "interacoes = [(nome, float(importances_array[idx])) \n",
    "              for idx, nome in enumerate(feature_names) \n",
    "              if nome in ['razao_valor_final_inicial', 'nivel_x_tamanho_descricao',\n",
    "                         'nivel_x_tamanho_codigo', 'log_valor_absoluto_final',\n",
    "                         'log_valor_absoluto_inicial', 'diferenca_absoluta_valores']]\n",
    "\n",
    "interacoes_ordenadas = sorted(interacoes, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n{'Feature de Intera√ß√£o':<35} {'Import√¢ncia':<15} {'% Total':<10} {'Rank Geral'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for nome, importancia in interacoes_ordenadas:\n",
    "    percentual = (importancia / importancia_total) * 100\n",
    "    \n",
    "    # Encontrar rank geral\n",
    "    rank_geral = None\n",
    "    for i, idx in enumerate(indices_ordenados, 1):\n",
    "        if feature_names[int(idx)] == nome:\n",
    "            rank_geral = i\n",
    "            break\n",
    "    \n",
    "    print(f\"{nome:<35} {importancia:<15.6f} {percentual:<9.2f}% #{rank_geral}\")\n",
    "\n",
    "# ================================================================================\n",
    "# TOP PALAVRAS-CHAVE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"üî§ AN√ÅLISE DETALHADA: FEATURES DE PALAVRAS-CHAVE\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "palavras_chave = [(nome, float(importances_array[idx])) \n",
    "                  for idx, nome in enumerate(feature_names) \n",
    "                  if nome.startswith('tem_palavra_')]\n",
    "\n",
    "palavras_ordenadas = sorted(palavras_chave, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n{'Palavra-Chave':<35} {'Import√¢ncia':<15} {'% Total':<10} {'Rank Geral'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for nome, importancia in palavras_ordenadas:\n",
    "    percentual = (importancia / importancia_total) * 100\n",
    "    palavra = nome.replace('tem_palavra_', '').upper()\n",
    "    \n",
    "    # Encontrar rank geral\n",
    "    rank_geral = None\n",
    "    for i, idx in enumerate(indices_ordenados, 1):\n",
    "        if feature_names[int(idx)] == nome:\n",
    "            rank_geral = i\n",
    "            break\n",
    "    \n",
    "    print(f\"{palavra:<35} {importancia:<15.6f} {percentual:<9.2f}% #{rank_geral}\")\n",
    "\n",
    "# ================================================================================\n",
    "# VISUALIZA√á√ÉO: IMPORT√ÇNCIAS ACUMULADAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"üìà IMPORT√ÇNCIA ACUMULADA (TOP FEATURES)\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "print(f\"\\n{'Top N Features':<20} {'Import√¢ncia Acumulada':<25} {'% do Total'}\")\n",
    "print(\"-\" * 90)\n",
    "\n",
    "for n in [5, 10, 15, 20, 30]:\n",
    "    if n <= len(indices_ordenados):\n",
    "        # CORRE√á√ÉO: usar builtins.sum()\n",
    "        importancia_acumulada = builtins.sum(float(importances_array[int(idx)]) \n",
    "                                            for idx in indices_ordenados[:n])\n",
    "        percentual = (importancia_acumulada / importancia_total) * 100\n",
    "        print(f\"Top {n:<17} {importancia_acumulada:<25.6f} {percentual:.2f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# INSIGHTS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"üí° INSIGHTS E DESCOBERTAS\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Calcular percentuais (CORRE√á√ÉO: usar builtins.sum())\n",
    "total_interacao = builtins.sum(imp for _, imp in interacoes)\n",
    "total_palavras = builtins.sum(imp for _, imp in palavras_chave)\n",
    "perc_interacao = (total_interacao / importancia_total) * 100\n",
    "perc_palavras = (total_palavras / importancia_total) * 100\n",
    "\n",
    "print(f\"\\n1Ô∏è‚É£  Features de INTERA√á√ÉO representam {perc_interacao:.2f}% da import√¢ncia total\")\n",
    "if perc_interacao > 20:\n",
    "    print(f\"   ‚úÖ EXCELENTE! Intera√ß√µes foram fundamentais para o ganho de +3.79%\")\n",
    "elif perc_interacao > 10:\n",
    "    print(f\"   ‚úÖ BOM! Intera√ß√µes contribu√≠ram significativamente\")\n",
    "else:\n",
    "    print(f\"   ‚ÑπÔ∏è  Contribui√ß√£o moderada das intera√ß√µes\")\n",
    "\n",
    "print(f\"\\n2Ô∏è‚É£  Features de PALAVRAS-CHAVE representam {perc_palavras:.2f}% da import√¢ncia total\")\n",
    "if perc_palavras > 30:\n",
    "    print(f\"   ‚úÖ CONFIRMADO! Palavras-chave s√£o ESSENCIAIS (por isso V2 falhou ao remov√™-las)\")\n",
    "elif perc_palavras > 20:\n",
    "    print(f\"   ‚úÖ BOM! Palavras-chave s√£o muito importantes\")\n",
    "else:\n",
    "    print(f\"   ‚ÑπÔ∏è  Palavras-chave t√™m import√¢ncia moderada\")\n",
    "\n",
    "# Top feature de intera√ß√£o\n",
    "if len(interacoes_ordenadas) > 0:\n",
    "    top_interacao = interacoes_ordenadas[0]\n",
    "    print(f\"\\n3Ô∏è‚É£  Melhor feature de intera√ß√£o: '{top_interacao[0]}'\")\n",
    "    print(f\"   Import√¢ncia: {top_interacao[1]:.6f} ({(top_interacao[1]/importancia_total)*100:.2f}%)\")\n",
    "\n",
    "# Top palavra-chave\n",
    "if len(palavras_ordenadas) > 0:\n",
    "    top_palavra = palavras_ordenadas[0]\n",
    "    palavra_limpa = top_palavra[0].replace('tem_palavra_', '').upper()\n",
    "    print(f\"\\n4Ô∏è‚É£  Palavra-chave mais importante: '{palavra_limpa}'\")\n",
    "    print(f\"   Import√¢ncia: {top_palavra[1]:.6f} ({(top_palavra[1]/importancia_total)*100:.2f}%)\")\n",
    "\n",
    "# Top 5 features gerais\n",
    "print(f\"\\n5Ô∏è‚É£  Top 5 features mais importantes do modelo:\")\n",
    "for i, idx in enumerate(indices_ordenados[:5], 1):\n",
    "    idx_int = int(idx)\n",
    "    nome = feature_names[idx_int]\n",
    "    importancia = float(importances_array[idx_int])\n",
    "    print(f\"   {i}. {nome}: {(importancia/importancia_total)*100:.2f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# COMPARA√á√ÉO: ANTES vs DEPOIS DAS INTERA√á√ïES\n",
    "# ================================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"‚öñÔ∏è  IMPACTO DAS FEATURES DE INTERA√á√ÉO\")\n",
    "print(\"=\" * 90)\n",
    "\n",
    "# Features base originais vs features de intera√ß√£o\n",
    "features_originais = features_por_tipo['üî¢ Num√©ricas Base'] + features_por_tipo['üìä Categ√≥ricas']\n",
    "features_novas = features_por_tipo['üîó Intera√ß√£o']\n",
    "\n",
    "total_originais = builtins.sum(imp for _, imp in features_originais)\n",
    "total_novas = builtins.sum(imp for _, imp in features_novas)\n",
    "\n",
    "perc_originais = (total_originais / importancia_total) * 100\n",
    "perc_novas = (total_novas / importancia_total) * 100\n",
    "\n",
    "print(f\"\\nüìä Distribui√ß√£o de import√¢ncia:\")\n",
    "print(f\"   Features ORIGINAIS (31):  {perc_originais:.2f}%\")\n",
    "print(f\"   Features INTERA√á√ÉO (6):   {perc_novas:.2f}%\")\n",
    "print(f\"\\nüí° Interpreta√ß√£o:\")\n",
    "\n",
    "if perc_novas > 15:\n",
    "    print(f\"   ‚úÖ As 6 features de intera√ß√£o (16.2% das features) representam {perc_novas:.1f}% da import√¢ncia!\")\n",
    "    print(f\"   ‚úÖ ROI excelente: cada feature de intera√ß√£o vale {perc_novas/6:.2f}% em m√©dia\")\n",
    "    print(f\"   ‚úÖ JUSTIFICA o ganho de +3.79% accuracy!\")\n",
    "else:\n",
    "    print(f\"   ‚ÑπÔ∏è  Features de intera√ß√£o contribuem moderadamente ({perc_novas:.1f}%)\")\n",
    "\n",
    "# Efici√™ncia por feature\n",
    "media_original = total_originais / len(features_originais) if len(features_originais) > 0 else 0\n",
    "media_interacao = total_novas / len(features_novas) if len(features_novas) > 0 else 0\n",
    "\n",
    "print(f\"\\nüìà Import√¢ncia m√©dia por feature:\")\n",
    "print(f\"   Originais:  {media_original:.6f}\")\n",
    "print(f\"   Intera√ß√£o:  {media_interacao:.6f}\")\n",
    "\n",
    "if media_interacao > media_original:\n",
    "    ratio = media_interacao / media_original\n",
    "    print(f\"   ‚úÖ Features de intera√ß√£o s√£o {ratio:.2f}x mais importantes em m√©dia!\")\n",
    "else:\n",
    "    print(f\"   ‚ÑπÔ∏è  Features originais t√™m maior import√¢ncia m√©dia individual\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 90)\n",
    "print(\"‚úÖ AN√ÅLISE DE FEATURE IMPORTANCE CONCLU√çDA!\")\n",
    "print(\"=\" * 90)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Pipeline)",
   "language": "python",
   "name": "conda_data_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
