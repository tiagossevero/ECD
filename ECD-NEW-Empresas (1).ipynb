{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "70aea681-2477-45f5-9ffe-1cb8f6cb1a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'.env_file' loaded!\n",
      "ENV 'PROD' configured!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins\")\n",
    "sys.path.append(\"/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags\")\n",
    "\n",
    "#Import libs python\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import date\n",
    "\n",
    "#Import libs internas\n",
    "from utils import spark_utils_session as utils\n",
    "\n",
    "from hooks.hdfs.hdfs_helper import HdfsHelper\n",
    "from jobs.job_base_config import BaseETLJobClass\n",
    "\n",
    "import poc_helper\n",
    "poc_helper.load_env(\"PROD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4faf8a1f-48e4-4c57-a162-c839c8e3a940",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[2m2025-11-15T23:47:44.442485Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mUsing json file settings.     \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:301\u001b[0m\n",
      "\u001b[2m2025-11-15T23:47:44.443611Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mExporting default ENV.        \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:305\u001b[0m\n",
      "\u001b[2m2025-11-15T23:47:44.444153Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mExporting custom ENVs.        \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:338\u001b[0m\n",
      "\u001b[2m2025-11-15T23:47:44.444498Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mBuilding profile 'efd_t2'.    \u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:221\u001b[0m\n",
      "\u001b[2m2025-11-15T23:47:44.444869Z\u001b[0m [\u001b[32m\u001b[1minfo     \u001b[0m] \u001b[1mNot enough info for building the kerberos client. Ignoring it\u001b[0m [\u001b[0m\u001b[1m\u001b[34mroot\u001b[0m]\u001b[0m \u001b[36mloc\u001b[0m=\u001b[35mspark_utils_session.py:284\u001b[0m\n",
      "Warning: Ignoring non-Spark config property: hive.metastore.uris\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/cloudera/parcels/SPARK3-3.5.4.3.5.7191000.0-30-1.p0.68499982/lib/spark3/jars/ivy-2.5.2.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/tsevero/.ivy2/cache\n",
      "The jars for the packages stored in: /home/tsevero/.ivy2/jars\n",
      "com.databricks#spark-xml_2.12 added as a dependency\n",
      "org.apache.iceberg#iceberg-spark-runtime-3.5_2.12 added as a dependency\n",
      "com.oracle.database.security#oraclepki added as a dependency\n",
      "com.oracle.database.security#osdt_core added as a dependency\n",
      "com.oracle.database.security#osdt_cert added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-dbc19d31-37de-4541-bae2-95906e93eb37;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.databricks#spark-xml_2.12;0.18.0 in central\n",
      "\tfound commons-io#commons-io;2.11.0 in central\n",
      "\tfound org.glassfish.jaxb#txw2;3.0.2 in central\n",
      "\tfound org.apache.ws.xmlschema#xmlschema-core;2.3.0 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.9.0 in central\n",
      "\tfound org.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 in central\n",
      "\tfound com.oracle.database.security#oraclepki;21.19.0.0 in central\n",
      "\tfound com.oracle.database.security#osdt_core;21.19.0.0 in central\n",
      "\tfound com.oracle.database.security#osdt_cert;21.19.0.0 in central\n",
      ":: resolution report :: resolve 169ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.databricks#spark-xml_2.12;0.18.0 from central in [default]\n",
      "\tcom.oracle.database.security#oraclepki;21.19.0.0 from central in [default]\n",
      "\tcom.oracle.database.security#osdt_cert;21.19.0.0 from central in [default]\n",
      "\tcom.oracle.database.security#osdt_core;21.19.0.0 from central in [default]\n",
      "\tcommons-io#commons-io;2.11.0 from central in [default]\n",
      "\torg.apache.iceberg#iceberg-spark-runtime-3.5_2.12;1.9.2 from central in [default]\n",
      "\torg.apache.ws.xmlschema#xmlschema-core;2.3.0 from central in [default]\n",
      "\torg.glassfish.jaxb#txw2;3.0.2 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.9.0 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   9   |   0   |   0   |   0   ||   9   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-dbc19d31-37de-4541-bae2-95906e93eb37\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 9 already retrieved (0kB/6ms)\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/15 20:47:48 WARN  conf.HiveConf: [Thread-9]: HiveConf of name hive.metastore.runworker.in does not exist\n",
      "25/11/15 20:47:48 WARN  conf.HiveConf: [Thread-9]: HiveConf of name hive.masking.algo does not exist\n",
      "25/11/15 20:47:49 WARN  util.Utils: [Thread-9]: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n",
      "25/11/15 20:47:49 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.databricks_spark-xml_2.12-0.18.0.jar added multiple times to distributed cache.\n",
      "25/11/15 20:47:49 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.apache.iceberg_iceberg-spark-runtime-3.5_2.12-1.9.2.jar added multiple times to distributed cache.\n",
      "25/11/15 20:47:49 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.oracle.database.security_oraclepki-21.19.0.0.jar added multiple times to distributed cache.\n",
      "25/11/15 20:47:49 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.oracle.database.security_osdt_core-21.19.0.0.jar added multiple times to distributed cache.\n",
      "25/11/15 20:47:49 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/com.oracle.database.security_osdt_cert-21.19.0.0.jar added multiple times to distributed cache.\n",
      "25/11/15 20:47:49 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/commons-io_commons-io-2.11.0.jar added multiple times to distributed cache.\n",
      "25/11/15 20:47:49 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.glassfish.jaxb_txw2-3.0.2.jar added multiple times to distributed cache.\n",
      "25/11/15 20:47:49 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.apache.ws.xmlschema_xmlschema-core-2.3.0.jar added multiple times to distributed cache.\n",
      "25/11/15 20:47:49 WARN  yarn.Client: [Thread-9]: Same path resource file:///home/tsevero/.ivy2/jars/org.scala-lang.modules_scala-collection-compat_2.12-2.9.0.jar added multiple times to distributed cache.\n",
      "25/11/15 20:47:54 WARN  util.Utils: [Thread-9]: spark.executor.instances less than spark.dynamicAllocation.minExecutors is invalid, ignoring its setting, please update your configs.\n"
     ]
    }
   ],
   "source": [
    "def get_session(profile: str, dynamic_allocation_enabled: bool = True) -> utils.DBASparkAppSession:\n",
    "    \"\"\"Generates DBASparkAppSession.\"\"\"\n",
    "    \n",
    "    app_name = \"tsevero_ecd_new_empresas\"\n",
    "    \n",
    "    spark_builder = (utils.DBASparkAppSession\n",
    "                     .builder\n",
    "                     .setAppName(app_name)\n",
    "                     .usingProcessProfile(profile)\n",
    "                    )\n",
    "    \n",
    "    if dynamic_allocation_enabled:\n",
    "        spark_builder.autoResourceManagement()\n",
    "\n",
    "    return spark_builder.build()\n",
    "\n",
    "session = get_session(profile='efd_t2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00777b24-69dc-4a6e-ba6f-4e11cd534937",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = cda11462-c57d-4a52-a7a8-d9c77dd6e76e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|namespace         |\n",
      "+------------------+\n",
      "|anac              |\n",
      "|bcadastro         |\n",
      "|bpe               |\n",
      "|c115              |\n",
      "|ccc               |\n",
      "|ccg               |\n",
      "|cte               |\n",
      "|default           |\n",
      "|destda            |\n",
      "|detran_share      |\n",
      "|dime              |\n",
      "|due               |\n",
      "|efd               |\n",
      "|fci               |\n",
      "|gecob             |\n",
      "|gescol            |\n",
      "|gessimples        |\n",
      "|gplam             |\n",
      "|information_schema|\n",
      "|malhas            |\n",
      "+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "session.sparkSession.sql(\"SHOW DATABASES\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7be2b9e6-be5f-4120-99be-8d828e7bbc98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SessÃ£o Spark jÃ¡ ativa!\n",
      "\n",
      "================================================================================\n",
      "ðŸš€ PROJETO ECD - ANÃLISE EXPLORATÃ“RIA E MACHINE LEARNING\n",
      "================================================================================\n",
      "ðŸ“… Data de execuÃ§Ã£o: 15/11/2025 20:48:03\n",
      "ðŸ“Š Bibliotecas carregadas com sucesso!\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ============================================================================\n",
    "# CÃ‰LULA 1: CONFIGURAÃ‡ÃƒO INICIAL - PROJETO ECD\n",
    "# ============================================================================\n",
    "# EscrituraÃ§Ã£o ContÃ¡bil Digital - AnÃ¡lise Completa\n",
    "# Auditor Fiscal da Receita Estadual de SC\n",
    "# ============================================================================\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# PySpark imports com aliases para evitar conflitos\n",
    "from pyspark.sql.functions import (\n",
    "    col as spark_col, \n",
    "    sum as spark_sum, \n",
    "    avg as spark_avg,\n",
    "    count as spark_count,\n",
    "    when as spark_when,\n",
    "    desc as spark_desc,\n",
    "    asc as spark_asc,\n",
    "    round as spark_round,\n",
    "    coalesce as spark_coalesce,\n",
    "    max as spark_max,\n",
    "    min as spark_min,\n",
    "    stddev as spark_stddev,\n",
    "    lit as spark_lit,\n",
    "    concat as spark_concat,\n",
    "    expr as spark_expr,\n",
    "    abs as spark_abs\n",
    ")\n",
    "\n",
    "# Machine Learning imports\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.metrics import (\n",
    "    classification_report, \n",
    "    confusion_matrix, \n",
    "    roc_auc_score, \n",
    "    silhouette_score,\n",
    "    davies_bouldin_score,\n",
    "    calinski_harabasz_score\n",
    ")\n",
    "import xgboost as xgb\n",
    "\n",
    "# Acesso ao SparkSession\n",
    "spark = session.sparkSession\n",
    "\n",
    "# ConfiguraÃ§Ãµes\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: f'{x:,.2f}')\n",
    "\n",
    "# ConfiguraÃ§Ã£o de estilo para grÃ¡ficos\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Verificar se spark existe na sessÃ£o\n",
    "try:\n",
    "    spark\n",
    "    print(\"âœ… SessÃ£o Spark jÃ¡ ativa!\")\n",
    "except NameError:\n",
    "    print(\"âš ï¸ SessÃ£o Spark nÃ£o encontrada. Por favor, inicialize a sessÃ£o Spark.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸš€ PROJETO ECD - ANÃLISE EXPLORATÃ“RIA E MACHINE LEARNING\")\n",
    "print(\"=\"*80)\n",
    "print(f\"ðŸ“… Data de execuÃ§Ã£o: {datetime.now().strftime('%d/%m/%Y %H:%M:%S')}\")\n",
    "print(\"ðŸ“Š Bibliotecas carregadas com sucesso!\")\n",
    "print(\"=\"*80 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6a58aa4c-87f9-49f2-8b9a-c333ee9992c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ APLICANDO MODELO ML EM TODAS AS CONTAS DO ECD\n",
      "================================================================================\n",
      "\n",
      "âš™ï¸  ConfiguraÃ§Ãµes:\n",
      "   Database ML: neac\n",
      "   Database ECD: usr_sat_ecd\n",
      "   UF: SC\n",
      "   Ano: 2024\n",
      "\n",
      "ðŸ“¥ Carregando modelo treinado...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Modelo FINAL carregado: /user/tsevero/models/rf_final_producao_SC_2024\n",
      "\n",
      "ðŸ“¥ Carregando TODAS as contas do ECD (BalanÃ§o Patrimonial)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Total de contas carregadas: 144,602,905\n",
      "\n",
      "ðŸ”§ Criando features...\n",
      "âœ… Features criadas em 0.16s\n",
      "\n",
      "ðŸ”§ Criando classificaÃ§Ã£o NÃ­vel 1...\n",
      "\n",
      "ðŸ”§ Indexando variÃ¡veis categÃ³ricas...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… VariÃ¡veis indexadas\n",
      "\n",
      "ðŸ”§ Montando vetor de features...\n",
      "âœ… Vetor de features montado\n",
      "\n",
      "ðŸ”® Aplicando modelo em TODAS as contas...\n",
      "   Processando em lotes para evitar timeout...\n",
      "âœ… PrediÃ§Ãµes concluÃ­das em 0.20s (0.00 min)\n",
      "\n",
      "ðŸ”§ Mapeando labels para classificaÃ§Ãµes...\n",
      "âœ… Labels mapeadas\n",
      "\n",
      "ðŸ’¾ Salvando prediÃ§Ãµes COMPLETAS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Salvo: neac.ecd_ml_predictions_ALL\n",
      "   Total: 144,602,905\n",
      "\n",
      "ðŸ“Š ESTATÃSTICAS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Top 20 classificaÃ§Ãµes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2370:>                                                       (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------+--------+\n",
      "|classificacao_nivel2    |count   |\n",
      "+------------------------+--------+\n",
      "|ATIVO_CIRCULANTE        |74873652|\n",
      "|PASSIVO_CIRCULANTE      |60477546|\n",
      "|PASSIVO                 |3749043 |\n",
      "|ATIVO                   |1886263 |\n",
      "|DISPONIBILIDADES        |1001849 |\n",
      "|CAPITAL_SOCIAL          |907726  |\n",
      "|FORNECEDORES            |679718  |\n",
      "|ESTOQUES                |286137  |\n",
      "|TRIBUTOS_A_PAGAR        |216264  |\n",
      "|IMOBILIZADO             |161367  |\n",
      "|CREDITOS                |155556  |\n",
      "|INVESTIMENTOS           |140849  |\n",
      "|RECEITA_BRUTA           |35910   |\n",
      "|RESERVAS                |20245   |\n",
      "|CUSTOS                  |7126    |\n",
      "|PATRIMONIO_LIQUIDO      |3608    |\n",
      "|OUTRAS_RECEITAS_DESPESAS|46      |\n",
      "+------------------------+--------+\n",
      "\n",
      "\n",
      "ðŸ“Š COMPARAÃ‡ÃƒO:\n",
      "   Antes (teste): 246,359\n",
      "   Depois (ALL):  144,602,905\n",
      "   GANHO:         +144,356,546\n",
      "\n",
      "================================================================================\n",
      "âœ… CONCLUÃDO!\n",
      "================================================================================\n",
      "\n",
      "ðŸŽ¯ Agora execute os Scripts 1, 2 e 3 usando:\n",
      "   neac.ecd_ml_predictions_ALL\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# APLICAR MODELO ML EM TODAS AS CONTAS DO ECD - VERSÃƒO CORRIGIDA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ðŸš€ APLICANDO MODELO ML EM TODAS AS CONTAS DO ECD\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, log1p, abs as spark_abs, length, lower,\n",
    "    regexp_extract, substring, coalesce\n",
    ")\n",
    "from pyspark.ml.feature import VectorAssembler, StringIndexer\n",
    "from pyspark.ml.classification import RandomForestClassificationModel\n",
    "import time\n",
    "\n",
    "# ================================================================================\n",
    "# CONFIGURAÃ‡Ã•ES\n",
    "# ================================================================================\n",
    "\n",
    "DATABASE = 'neac'\n",
    "DATABASE_ECD = 'usr_sat_ecd'\n",
    "UF_FILTRO = 'SC'\n",
    "ANO_REFERENCIA = 2024\n",
    "\n",
    "print(f\"\\nâš™ï¸  ConfiguraÃ§Ãµes:\")\n",
    "print(f\"   Database ML: {DATABASE}\")\n",
    "print(f\"   Database ECD: {DATABASE_ECD}\")\n",
    "print(f\"   UF: {UF_FILTRO}\")\n",
    "print(f\"   Ano: {ANO_REFERENCIA}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 1. CARREGAR MODELO TREINADO\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“¥ Carregando modelo treinado...\")\n",
    "\n",
    "try:\n",
    "    model_path = f\"/user/{spark.sparkContext.sparkUser()}/models/rf_final_producao_{UF_FILTRO}_{ANO_REFERENCIA}\"\n",
    "    modelo = RandomForestClassificationModel.load(model_path)\n",
    "    print(f\"âœ… Modelo FINAL carregado: {model_path}\")\n",
    "except:\n",
    "    try:\n",
    "        model_path = f\"/user/{spark.sparkContext.sparkUser()}/models/rf_abordagem_a_{UF_FILTRO}_{ANO_REFERENCIA}\"\n",
    "        modelo = RandomForestClassificationModel.load(model_path)\n",
    "        print(f\"âœ… Modelo Abordagem A carregado: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Erro ao carregar modelo: {e}\")\n",
    "        raise\n",
    "\n",
    "# ================================================================================\n",
    "# 2. CARREGAR TODAS AS CONTAS DO ECD - SEM TABELA DE IDENTIFICAÃ‡ÃƒO\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“¥ Carregando TODAS as contas do ECD (BalanÃ§o Patrimonial)...\")\n",
    "\n",
    "df_todas_contas = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        bp.id_ecd,\n",
    "        CAST(bp.id_ecd AS STRING) as cnpj,  -- Usar id_ecd como proxy temporÃ¡rio\n",
    "        bp.cod_agl as cd_conta,\n",
    "        bp.descr_cod_agl as descr_conta,\n",
    "        bp.cod_agl_sup as cd_conta_sup,\n",
    "        bp.nivel_agl as nivel_conta,\n",
    "        bp.ind_grp_bal,\n",
    "        'BP' as origem_demonstrativo,\n",
    "        CAST(NULL AS STRING) as ind_grp_dre,  -- BP nÃ£o tem ind_grp_dre\n",
    "        bp.vl_cta_fin as vl_conta_final,\n",
    "        bp.vl_cta_ini as vl_conta_inicial,\n",
    "        bp.ind_dc_cta_fin,\n",
    "        bp.ind_dc_cta_ini,\n",
    "        pc.cd_natureza,\n",
    "        pc.tp_conta as tp_conta_pc,\n",
    "        CASE \n",
    "            WHEN bp.nivel_agl = 1 THEN 'S'\n",
    "            WHEN bp.nivel_agl > 1 THEN 'A'\n",
    "            ELSE 'A'\n",
    "        END as tp_conta_agl\n",
    "    FROM {DATABASE_ECD}.ecd_rj100_balanco_patrimonial bp\n",
    "    LEFT JOIN {DATABASE_ECD}.ecd_ri050_plano_contas pc\n",
    "        ON bp.id_ecd = pc.id_ecd\n",
    "        AND bp.cod_agl = pc.cd_conta_anl\n",
    "\"\"\")\n",
    "\n",
    "total_contas = df_todas_contas.count()\n",
    "print(f\"âœ… Total de contas carregadas: {total_contas:,}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 3. CRIAR FEATURES\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ”§ Criando features...\")\n",
    "inicio_features = time.time()\n",
    "\n",
    "# Features bÃ¡sicas de texto\n",
    "df_features = df_todas_contas.withColumn(\n",
    "    'tamanho_descricao',\n",
    "    length(col('descr_conta'))\n",
    ").withColumn(\n",
    "    'tamanho_codigo',\n",
    "    length(col('cd_conta'))\n",
    ").withColumn(\n",
    "    'tem_ponto_codigo',\n",
    "    when(col('cd_conta').contains('.'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_hifen_codigo',\n",
    "    when(col('cd_conta').contains('-'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_underscore_codigo',\n",
    "    when(col('cd_conta').contains('_'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'primeiro_digito_codigo',\n",
    "    substring(col('cd_conta'), 1, 1)\n",
    ")\n",
    "\n",
    "# Features de valores\n",
    "df_features = df_features.withColumn(\n",
    "    'valor_absoluto_final',\n",
    "    spark_abs(coalesce(col('vl_conta_final'), lit(0)))\n",
    ").withColumn(\n",
    "    'valor_absoluto_inicial',\n",
    "    spark_abs(coalesce(col('vl_conta_inicial'), lit(0)))\n",
    ").withColumn(\n",
    "    'variacao_valor',\n",
    "    coalesce(col('vl_conta_final'), lit(0)) - coalesce(col('vl_conta_inicial'), lit(0))\n",
    ")\n",
    "\n",
    "# Features de hierarquia\n",
    "df_features = df_features.withColumn(\n",
    "    'tem_conta_superior',\n",
    "    when(col('cd_conta_sup').isNotNull(), 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Features de palavras-chave\n",
    "descr_lower = lower(col('descr_conta'))\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    'tem_palavra_caixa',\n",
    "    when(descr_lower.contains('caixa'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_banco',\n",
    "    when(descr_lower.contains('banco'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_estoque',\n",
    "    when(descr_lower.contains('estoque'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_cliente',\n",
    "    when(descr_lower.contains('cliente'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_fornecedor',\n",
    "    when(descr_lower.contains('fornecedor'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_salario',\n",
    "    when(descr_lower.contains('salario') | descr_lower.contains('salÃ¡rio'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_tributo',\n",
    "    when(descr_lower.contains('tributo') | descr_lower.contains('imposto'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_receita',\n",
    "    when(descr_lower.contains('receita'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_despesa',\n",
    "    when(descr_lower.contains('despesa'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_financeiro',\n",
    "    when(descr_lower.contains('financeiro') | descr_lower.contains('financeira'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_imobilizado',\n",
    "    when(descr_lower.contains('imobilizado'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_capital',\n",
    "    when(descr_lower.contains('capital'), 1.0).otherwise(0.0)\n",
    ").withColumn(\n",
    "    'tem_palavra_lucro',\n",
    "    when(descr_lower.contains('lucro') | descr_lower.contains('prejuÃ­zo') | descr_lower.contains('prejuizo'), 1.0).otherwise(0.0)\n",
    ")\n",
    "\n",
    "# Features de interaÃ§Ã£o\n",
    "df_features = df_features.withColumn(\n",
    "    'razao_valor_final_inicial',\n",
    "    when(col('valor_absoluto_inicial') > 0,\n",
    "         col('valor_absoluto_final') / col('valor_absoluto_inicial')\n",
    "    ).otherwise(lit(0.0))\n",
    ").withColumn(\n",
    "    'nivel_x_tamanho_descricao',\n",
    "    col('nivel_conta') * col('tamanho_descricao')\n",
    ").withColumn(\n",
    "    'nivel_x_tamanho_codigo',\n",
    "    col('nivel_conta') * col('tamanho_codigo')\n",
    ").withColumn(\n",
    "    'log_valor_absoluto_final',\n",
    "    log1p(col('valor_absoluto_final'))\n",
    ").withColumn(\n",
    "    'log_valor_absoluto_inicial',\n",
    "    log1p(col('valor_absoluto_inicial'))\n",
    ").withColumn(\n",
    "    'diferenca_absoluta_valores',\n",
    "    spark_abs(col('valor_absoluto_final') - col('valor_absoluto_inicial'))\n",
    ")\n",
    "\n",
    "tempo_features = time.time() - inicio_features\n",
    "print(f\"âœ… Features criadas em {tempo_features:.2f}s\")\n",
    "\n",
    "# ================================================================================\n",
    "# 4. CRIAR CLASSIFICAÃ‡ÃƒO NIVEL 1\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ”§ Criando classificaÃ§Ã£o NÃ­vel 1...\")\n",
    "\n",
    "df_features = df_features.withColumn(\n",
    "    'classificacao_nivel1',\n",
    "    when(col('ind_grp_bal') == 'A', 'ATIVO')\n",
    "    .when(col('ind_grp_bal') == 'P', 'PASSIVO')\n",
    "    .otherwise('OUTROS')\n",
    ")\n",
    "\n",
    "# Preencher ind_grp_dre como 'N/A' se nulo\n",
    "df_features = df_features.withColumn(\n",
    "    'ind_grp_dre',\n",
    "    coalesce(col('ind_grp_dre'), lit('N/A'))\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# 5. STRING INDEXERS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ”§ Indexando variÃ¡veis categÃ³ricas...\")\n",
    "\n",
    "categorical_cols = [\n",
    "    'origem_demonstrativo', 'ind_grp_bal', 'ind_grp_dre',\n",
    "    'cd_natureza', 'tp_conta_agl', 'tp_conta_pc',\n",
    "    'primeiro_digito_codigo', 'classificacao_nivel1'\n",
    "]\n",
    "\n",
    "df_indexed = df_features\n",
    "\n",
    "for col_name in categorical_cols:\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=col_name,\n",
    "        outputCol=f'{col_name}_index',\n",
    "        handleInvalid='keep'\n",
    "    )\n",
    "    df_indexed = indexer.fit(df_indexed).transform(df_indexed)\n",
    "\n",
    "print(f\"âœ… VariÃ¡veis indexadas\")\n",
    "\n",
    "# ================================================================================\n",
    "# 6. VETOR DE FEATURES\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ”§ Montando vetor de features...\")\n",
    "\n",
    "numeric_features = [\n",
    "    'nivel_conta', 'tamanho_descricao', 'tamanho_codigo',\n",
    "    'tem_ponto_codigo', 'tem_hifen_codigo', 'tem_underscore_codigo',\n",
    "    'valor_absoluto_final', 'valor_absoluto_inicial', 'variacao_valor',\n",
    "    'tem_conta_superior',\n",
    "    'tem_palavra_caixa', 'tem_palavra_banco', 'tem_palavra_estoque',\n",
    "    'tem_palavra_cliente', 'tem_palavra_fornecedor', 'tem_palavra_salario',\n",
    "    'tem_palavra_tributo', 'tem_palavra_receita', 'tem_palavra_despesa',\n",
    "    'tem_palavra_financeiro', 'tem_palavra_imobilizado', 'tem_palavra_capital',\n",
    "    'tem_palavra_lucro',\n",
    "    'razao_valor_final_inicial', 'nivel_x_tamanho_descricao', 'nivel_x_tamanho_codigo',\n",
    "    'log_valor_absoluto_final', 'log_valor_absoluto_inicial', 'diferenca_absoluta_valores'\n",
    "]\n",
    "\n",
    "categorical_features = [f'{col}_index' for col in categorical_cols]\n",
    "all_features = numeric_features + categorical_features\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=all_features,\n",
    "    outputCol='features_v2',\n",
    "    handleInvalid='keep'\n",
    ")\n",
    "\n",
    "df_assembled = assembler.transform(df_indexed)\n",
    "print(f\"âœ… Vetor de features montado\")\n",
    "\n",
    "# ================================================================================\n",
    "# 7. FAZER PREDIÃ‡Ã•ES EM LOTES (PARA EVITAR TIMEOUT)\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ”® Aplicando modelo em TODAS as contas...\")\n",
    "print(f\"   Processando em lotes para evitar timeout...\")\n",
    "\n",
    "# Criar partiÃ§Ãµes menores\n",
    "df_assembled = df_assembled.repartition(200)  # Ajuste conforme necessÃ¡rio\n",
    "\n",
    "inicio_pred = time.time()\n",
    "df_predicoes = modelo.transform(df_assembled)\n",
    "\n",
    "# Persistir para acelerar operaÃ§Ãµes futuras\n",
    "df_predicoes = df_predicoes.persist()\n",
    "\n",
    "tempo_pred = time.time() - inicio_pred\n",
    "print(f\"âœ… PrediÃ§Ãµes concluÃ­das em {tempo_pred:.2f}s ({tempo_pred/60:.2f} min)\")\n",
    "\n",
    "# ================================================================================\n",
    "# 8. MAPEAR LABELS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ”§ Mapeando labels para classificaÃ§Ãµes...\")\n",
    "\n",
    "# Carregar mapeamento\n",
    "mapeamento = spark.sql(f\"\"\"\n",
    "    SELECT DISTINCT label, classificacao_nivel2\n",
    "    FROM {DATABASE}.ecd_ml_dataset\n",
    "    ORDER BY label\n",
    "\"\"\").collect()\n",
    "\n",
    "label_to_class = {row['label']: row['classificacao_nivel2'] for row in mapeamento}\n",
    "\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "@udf(returnType=StringType())\n",
    "def map_label_to_class(label):\n",
    "    if label is None:\n",
    "        return None\n",
    "    return label_to_class.get(int(label), 'NAO_CLASSIFICADO')\n",
    "\n",
    "df_predicoes_final = df_predicoes.withColumn(\n",
    "    'classificacao_nivel2_pred',\n",
    "    map_label_to_class(col('prediction'))\n",
    ")\n",
    "\n",
    "print(f\"âœ… Labels mapeadas\")\n",
    "\n",
    "# ================================================================================\n",
    "# 9. SALVAR\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ’¾ Salvando prediÃ§Ãµes COMPLETAS...\")\n",
    "\n",
    "df_save = df_predicoes_final.select(\n",
    "    'id_ecd',\n",
    "    'cnpj',\n",
    "    'cd_conta',\n",
    "    'descr_conta',\n",
    "    'classificacao_nivel2_pred',\n",
    "    'prediction',\n",
    "    'probability'\n",
    ").withColumnRenamed('classificacao_nivel2_pred', 'classificacao_nivel2')\n",
    "\n",
    "df_save.createOrReplaceTempView(\"temp_predictions_all\")\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_predictions_ALL\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {DATABASE}.ecd_ml_predictions_ALL\n",
    "    STORED AS PARQUET\n",
    "    AS SELECT * FROM temp_predictions_all\n",
    "\"\"\")\n",
    "\n",
    "total_pred_salvas = df_save.count()\n",
    "print(f\"âœ… Salvo: {DATABASE}.ecd_ml_predictions_ALL\")\n",
    "print(f\"   Total: {total_pred_salvas:,}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 10. ESTATÃSTICAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š ESTATÃSTICAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“Š Top 20 classificaÃ§Ãµes:\")\n",
    "df_save.groupBy('classificacao_nivel2').count() \\\n",
    "    .orderBy(col('count').desc()) \\\n",
    "    .limit(20) \\\n",
    "    .show(truncate=False)\n",
    "\n",
    "# ComparaÃ§Ã£o\n",
    "pred_antes = 246359\n",
    "pred_depois = total_pred_salvas\n",
    "\n",
    "print(f\"\\nðŸ“Š COMPARAÃ‡ÃƒO:\")\n",
    "print(f\"   Antes (teste): {pred_antes:,}\")\n",
    "print(f\"   Depois (ALL):  {pred_depois:,}\")\n",
    "print(f\"   GANHO:         +{pred_depois - pred_antes:,}\")\n",
    "\n",
    "# Unpersist\n",
    "df_predicoes.unpersist()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… CONCLUÃDO!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nðŸŽ¯ Agora execute os Scripts 1, 2 e 3 usando:\")\n",
    "print(f\"   {DATABASE}.ecd_ml_predictions_ALL\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e62ba0a0-5a7a-4ae2-adaa-cf9c57a29574",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” ANÃLISE DE QUALIDADE DAS CLASSIFICAÃ‡Ã•ES POR EMPRESA\n",
      "================================================================================\n",
      "\n",
      "âš™ï¸  ConfiguraÃ§Ãµes:\n",
      "   Database ML: neac\n",
      "   Database ECD: usr_sat_ecd\n",
      "   Ano: 2024\n",
      "\n",
      "ðŸ“¥ Carregando prediÃ§Ãµes...\n",
      "âœ… PrediÃ§Ãµes: 144,602,905 registros\n",
      "\n",
      "ðŸ“¥ Carregando valores do ECD...\n",
      "âœ… BalanÃ§o: 144,602,899\n",
      "âœ… DRE: 40,260,048\n",
      "\n",
      "ðŸ”— JOIN...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… BalanÃ§o: 460,360,611\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… DRE: 106,068\n",
      "\n",
      "ðŸ“Š ESTATÃSTICAS...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Salvo: neac.ecd_ml_stats_classificacao_empresa\n",
      "\n",
      "ðŸ’° CALCULANDO BALANÃ‡O...\n",
      "\n",
      "ðŸ” DistribuiÃ§Ã£o:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+---------+\n",
      "|     grupo_balanco|    count|\n",
      "+------------------+---------+\n",
      "|           PASSIVO|213044367|\n",
      "|            OUTROS|   172419|\n",
      "|PATRIMONIO_LIQUIDO|  3484536|\n",
      "|             ATIVO|243659289|\n",
      "+------------------+---------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Salvo: neac.ecd_ml_valores_balanco_empresa\n",
      "\n",
      "âš–ï¸  Erro na equaÃ§Ã£o:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+\n",
      "| faixa| count|\n",
      "+------+------+\n",
      "|0.5-1%|     3|\n",
      "|  1-5%|    11|\n",
      "| 5-10%|    28|\n",
      "| > 10%|380999|\n",
      "|â‰¤ 0.5%|  3452|\n",
      "+------+------+\n",
      "\n",
      "\n",
      "ðŸ“ˆ CALCULANDO DRE...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Salvo: neac.ecd_ml_valores_dre_empresa\n",
      "\n",
      "ðŸ”— CONSOLIDANDO...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Salvo: neac.ecd_ml_empresas_consolidado\n",
      "\n",
      "ðŸ“Š ESTATÃSTICAS FINAIS\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Total: 384,493 empresas\n",
      "   âœ… Aptas: 0 (0.00%)\n",
      "   âŒ NÃ£o aptas: 384,493\n",
      "\n",
      "ðŸ“‹ Motivos:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                ]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - EquaÃ§Ã£o > 10%: 380,999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2809:==============================================>       (25 + 4) / 29]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   - Dados incompletos: 384,490\n",
      "\n",
      "================================================================================\n",
      "âœ… SCRIPT 1 CONCLUÃDO!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# SCRIPT 1: ANÃLISE COMPLETA - VERSÃƒO FINAL CORRIGIDA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ðŸ” ANÃLISE DE QUALIDADE DAS CLASSIFICAÃ‡Ã•ES POR EMPRESA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, count, sum as spark_sum, when, lit, abs as spark_abs,\n",
    "    round as spark_round, coalesce\n",
    ")\n",
    "\n",
    "DATABASE = 'neac'\n",
    "DATABASE_ECD = 'usr_sat_ecd'\n",
    "ANO_REFERENCIA = 2024\n",
    "\n",
    "print(f\"\\nâš™ï¸  ConfiguraÃ§Ãµes:\")\n",
    "print(f\"   Database ML: {DATABASE}\")\n",
    "print(f\"   Database ECD: {DATABASE_ECD}\")\n",
    "print(f\"   Ano: {ANO_REFERENCIA}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR PREDIÃ‡Ã•ES\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“¥ Carregando prediÃ§Ãµes...\")\n",
    "df_pred = spark.table(f\"{DATABASE}.ecd_ml_predictions_ALL\")\n",
    "print(f\"âœ… PrediÃ§Ãµes: {df_pred.count():,} registros\")\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR VALORES\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“¥ Carregando valores do ECD...\")\n",
    "\n",
    "df_balanco_valores = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        bp.id_ecd,\n",
    "        bp.cod_agl as cd_conta,\n",
    "        bp.vl_cta_fin as vl_conta_final,\n",
    "        bp.ind_dc_cta_fin as ind_dc_final\n",
    "    FROM {DATABASE_ECD}.ecd_rj100_balanco_patrimonial bp\n",
    "\"\"\")\n",
    "\n",
    "df_dre_valores = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        dre.id_ecd,\n",
    "        dre.cod_agl as cd_conta,\n",
    "        dre.vl_cta_fin as vl_conta_final,\n",
    "        dre.ind_dc_cta_fin as ind_dc_final\n",
    "    FROM {DATABASE_ECD}.ecd_rj150_demonstracao_resultado_exercicio dre\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… BalanÃ§o: {df_balanco_valores.count():,}\")\n",
    "print(f\"âœ… DRE: {df_dre_valores.count():,}\")\n",
    "\n",
    "# ================================================================================\n",
    "# JOIN\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ”— JOIN...\")\n",
    "\n",
    "df_balanco_completo = df_pred.join(df_balanco_valores, on=['id_ecd', 'cd_conta'], how='inner')\n",
    "df_dre_completo = df_pred.join(df_dre_valores, on=['id_ecd', 'cd_conta'], how='inner')\n",
    "\n",
    "print(f\"âœ… BalanÃ§o: {df_balanco_completo.count():,}\")\n",
    "print(f\"âœ… DRE: {df_dre_completo.count():,}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 1. ESTATÃSTICAS (SEM LABEL - NÃƒO HÃ VALIDAÃ‡ÃƒO)\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š ESTATÃSTICAS...\")\n",
    "\n",
    "stats_classificacao = df_balanco_completo.groupBy('cnpj', 'id_ecd').agg(\n",
    "    count('*').alias('total_contas')\n",
    ").withColumn('ano_calendario', lit(ANO_REFERENCIA)) \\\n",
    " .withColumn('contas_corretas', col('total_contas')) \\\n",
    " .withColumn('perc_corretas', lit(100.0))\n",
    "\n",
    "stats_classificacao.createOrReplaceTempView(\"temp_stats\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_stats_classificacao_empresa\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {DATABASE}.ecd_ml_stats_classificacao_empresa\n",
    "    STORED AS PARQUET  \n",
    "    AS SELECT * FROM temp_stats\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Salvo: {DATABASE}.ecd_ml_stats_classificacao_empresa\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. VALORES DO BALANÃ‡O\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ’° CALCULANDO BALANÃ‡O...\")\n",
    "\n",
    "df_balanco_ajustado = df_balanco_completo.withColumn(\n",
    "    'vl_ajustado',\n",
    "    when(col('ind_dc_final') == 'D', col('vl_conta_final'))\n",
    "    .when(col('ind_dc_final') == 'C', -col('vl_conta_final'))\n",
    "    .otherwise(col('vl_conta_final'))\n",
    ")\n",
    "\n",
    "df_balanco_classificado = df_balanco_ajustado.withColumn(\n",
    "    'grupo_balanco',\n",
    "    when(col('classificacao_nivel2').isin([\n",
    "        'ATIVO', 'ATIVO_CIRCULANTE', 'ATIVO_NAO_CIRCULANTE',\n",
    "        'DISPONIBILIDADES', 'CREDITOS', 'ESTOQUES',\n",
    "        'IMOBILIZADO', 'INTANGIVEL', 'INVESTIMENTOS'\n",
    "    ]), 'ATIVO')\n",
    "    .when(col('classificacao_nivel2').isin([\n",
    "        'PATRIMONIO_LIQUIDO', 'CAPITAL_SOCIAL',\n",
    "        'RESERVAS', 'LUCROS_PREJUIZOS_ACUMULADOS'\n",
    "    ]), 'PATRIMONIO_LIQUIDO')\n",
    "    .when(col('classificacao_nivel2').isin([\n",
    "        'PASSIVO', 'PASSIVO_CIRCULANTE', 'PASSIVO_NAO_CIRCULANTE',\n",
    "        'EMPRESTIMOS_CP', 'FORNECEDORES',\n",
    "        'SALARIOS_A_PAGAR', 'TRIBUTOS_A_PAGAR'\n",
    "    ]), 'PASSIVO')\n",
    "    .otherwise('OUTROS')\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ” DistribuiÃ§Ã£o:\")\n",
    "df_balanco_classificado.groupBy('grupo_balanco').count().show()\n",
    "\n",
    "valores_balanco = df_balanco_classificado.filter(\n",
    "    col('grupo_balanco').isin(['ATIVO', 'PASSIVO', 'PATRIMONIO_LIQUIDO'])\n",
    ").groupBy('id_ecd', 'grupo_balanco').agg(\n",
    "    spark_sum('vl_ajustado').alias('vl_saldo_final')\n",
    ")\n",
    "\n",
    "valores_pivot = valores_balanco.groupBy('id_ecd').pivot(\n",
    "    'grupo_balanco', ['ATIVO', 'PASSIVO', 'PATRIMONIO_LIQUIDO']\n",
    ").agg(spark_sum('vl_saldo_final'))\n",
    "\n",
    "df_empresas = df_pred.select('id_ecd', 'cnpj').distinct() \\\n",
    "    .withColumn('ano_calendario', lit(ANO_REFERENCIA))\n",
    "\n",
    "valores_com_cnpj = valores_pivot.join(df_empresas, on='id_ecd', how='inner')\n",
    "\n",
    "valores_com_equacao = valores_com_cnpj.withColumn(\n",
    "    'ativo', coalesce(col('ATIVO'), lit(0))\n",
    ").withColumn(\n",
    "    'passivo', coalesce(col('PASSIVO'), lit(0))\n",
    ").withColumn(\n",
    "    'patrimonio_liquido', coalesce(col('PATRIMONIO_LIQUIDO'), lit(0))\n",
    ").withColumn(\n",
    "    'equacao_contabil', col('ativo') - (col('passivo') + col('patrimonio_liquido'))\n",
    ").withColumn(\n",
    "    'equacao_contabil_perc',\n",
    "    when(spark_abs(col('ativo')) > 0, \n",
    "         spark_round(spark_abs(col('equacao_contabil')) / spark_abs(col('ativo')) * 100, 4)\n",
    "    ).otherwise(0)\n",
    ").withColumn(\n",
    "    'equacao_valida', when(col('equacao_contabil_perc') <= 10.0, 'SIM').otherwise('NAO')\n",
    ")\n",
    "\n",
    "valores_com_equacao.createOrReplaceTempView(\"temp_balanco\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_valores_balanco_empresa\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {DATABASE}.ecd_ml_valores_balanco_empresa\n",
    "    STORED AS PARQUET\n",
    "    AS SELECT * FROM temp_balanco\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Salvo: {DATABASE}.ecd_ml_valores_balanco_empresa\")\n",
    "\n",
    "print(f\"\\nâš–ï¸  Erro na equaÃ§Ã£o:\")\n",
    "valores_com_equacao.groupBy(\n",
    "    when(col('equacao_contabil_perc') <= 0.5, 'â‰¤ 0.5%')\n",
    "    .when(col('equacao_contabil_perc') <= 1.0, '0.5-1%')\n",
    "    .when(col('equacao_contabil_perc') <= 5.0, '1-5%')\n",
    "    .when(col('equacao_contabil_perc') <= 10.0, '5-10%')\n",
    "    .otherwise('> 10%').alias('faixa')\n",
    ").count().orderBy('faixa').show()\n",
    "\n",
    "# ================================================================================\n",
    "# 3. DRE\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“ˆ CALCULANDO DRE...\")\n",
    "\n",
    "df_dre_ajustado = df_dre_completo.withColumn(\n",
    "    'vl_ajustado',\n",
    "    when(col('ind_dc_final') == 'D', col('vl_conta_final'))\n",
    "    .when(col('ind_dc_final') == 'C', -col('vl_conta_final'))\n",
    "    .otherwise(col('vl_conta_final'))\n",
    ")\n",
    "\n",
    "df_dre_classificado = df_dre_ajustado.withColumn(\n",
    "    'grupo_dre',\n",
    "    when(col('classificacao_nivel2').isin([\n",
    "        'RECEITA_BRUTA', 'DEDUCOES_RECEITA', 'RECEITAS_FINANCEIRAS'\n",
    "    ]), 'RECEITA')\n",
    "    .when(col('classificacao_nivel2').isin([\n",
    "        'CUSTOS', 'DESPESAS_OPERACIONAIS', 'DESPESAS_FINANCEIRAS',\n",
    "        'IMPOSTOS_RESULTADO', 'OUTRAS_RECEITAS_DESPESAS'\n",
    "    ]), 'DESPESA')\n",
    "    .otherwise('OUTROS')\n",
    ")\n",
    "\n",
    "valores_dre = df_dre_classificado.filter(\n",
    "    col('grupo_dre').isin(['RECEITA', 'DESPESA'])\n",
    ").groupBy('id_ecd', 'grupo_dre').agg(\n",
    "    spark_sum('vl_ajustado').alias('valor')\n",
    ")\n",
    "\n",
    "valores_dre_pivot = valores_dre.groupBy('id_ecd').pivot(\n",
    "    'grupo_dre', ['RECEITA', 'DESPESA']\n",
    ").agg(spark_sum('valor'))\n",
    "\n",
    "valores_dre_final = valores_dre_pivot.join(df_empresas, on='id_ecd', how='inner') \\\n",
    "    .withColumn('receitas', spark_abs(coalesce(col('RECEITA'), lit(0)))) \\\n",
    "    .withColumn('despesas', spark_abs(coalesce(col('DESPESA'), lit(0)))) \\\n",
    "    .withColumn('resultado', col('receitas') - col('despesas')) \\\n",
    "    .select('id_ecd', 'cnpj', 'ano_calendario', 'receitas', 'despesas', 'resultado')\n",
    "\n",
    "valores_dre_final.createOrReplaceTempView(\"temp_dre\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_valores_dre_empresa\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {DATABASE}.ecd_ml_valores_dre_empresa\n",
    "    STORED AS PARQUET\n",
    "    AS SELECT * FROM temp_dre\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Salvo: {DATABASE}.ecd_ml_valores_dre_empresa\")\n",
    "\n",
    "# ================================================================================\n",
    "# 4. CONSOLIDAR\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ”— CONSOLIDANDO...\")\n",
    "\n",
    "df_final = stats_classificacao \\\n",
    "    .join(valores_com_equacao.drop('ano_calendario'), on=['cnpj', 'id_ecd'], how='left') \\\n",
    "    .join(valores_dre_final.drop('ano_calendario'), on=['cnpj', 'id_ecd'], how='left') \\\n",
    "    .withColumn('classificacao_boa', lit('SIM')) \\\n",
    "    .withColumn('dados_completos', \n",
    "        when((col('ativo').isNotNull()) & (col('passivo').isNotNull()) & \n",
    "             (col('patrimonio_liquido').isNotNull()) & (col('receitas').isNotNull()) & \n",
    "             (col('despesas').isNotNull()), 'SIM').otherwise('NAO')) \\\n",
    "    .withColumn('apta_indicadores',\n",
    "        when((col('equacao_valida') == 'SIM') & \n",
    "             (col('dados_completos') == 'SIM') & (spark_abs(col('ativo')) > 10000), \n",
    "             'SIM').otherwise('NAO'))\n",
    "\n",
    "df_final.createOrReplaceTempView(\"temp_consolidado\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_empresas_consolidado\")\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {DATABASE}.ecd_ml_empresas_consolidado\n",
    "    STORED AS PARQUET\n",
    "    AS SELECT * FROM temp_consolidado\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Salvo: {DATABASE}.ecd_ml_empresas_consolidado\")\n",
    "\n",
    "# ================================================================================\n",
    "# 5. ESTATÃSTICAS FINAIS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š ESTATÃSTICAS FINAIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "total = df_final.count()\n",
    "aptas = df_final.filter(col('apta_indicadores') == 'SIM').count()\n",
    "\n",
    "print(f\"\\nâœ… Total: {total:,} empresas\")\n",
    "print(f\"   âœ… Aptas: {aptas:,} ({aptas/total*100:.2f}%)\")\n",
    "print(f\"   âŒ NÃ£o aptas: {total-aptas:,}\")\n",
    "\n",
    "print(f\"\\nðŸ“‹ Motivos:\")\n",
    "print(f\"   - EquaÃ§Ã£o > 10%: {df_final.filter(col('equacao_valida') == 'NAO').count():,}\")\n",
    "print(f\"   - Dados incompletos: {df_final.filter(col('dados_completos') == 'NAO').count():,}\")\n",
    "\n",
    "if aptas > 0:\n",
    "    print(f\"\\nðŸ† TOP 20 APTAS:\")\n",
    "    df_final.filter(col('apta_indicadores') == 'SIM') \\\n",
    "        .select('cnpj', 'total_contas', 'equacao_contabil_perc', \n",
    "                'ativo', 'patrimonio_liquido', 'resultado') \\\n",
    "        .orderBy(col('equacao_contabil_perc').asc()) \\\n",
    "        .limit(20) \\\n",
    "        .show(truncate=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… SCRIPT 1 CONCLUÃDO!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "40a023bb-e90d-48b3-aa24-2b13e5154347",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š ANÃLISE DE TOLERÃ‚NCIAS NA EQUAÃ‡ÃƒO CONTÃBIL\n",
      "================================================================================\n",
      "\n",
      "âš™ï¸  ConfiguraÃ§Ãµes:\n",
      "   Database: neac\n",
      "   Ano: 2024\n",
      "   TolerÃ¢ncias a testar: [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 15.0, 20.0]\n",
      "\n",
      "ðŸ“¥ Carregando dados consolidados...\n",
      "âœ… Total de empresas: 384,493\n",
      "\n",
      "ðŸ“Š TESTANDO DIFERENTES TOLERÃ‚NCIAS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š TolerÃ¢ncia: 0.1%\n",
      "   Empresas aptas: 0 (0.00%)\n",
      "   Erro mÃ©dio: 0.0000%\n",
      "   Desvio padrÃ£o: 0.0000%\n",
      "   Erro mÃ­n/mÃ¡x: 0.0000% / 0.0000%\n",
      "\n",
      "ðŸ“Š TolerÃ¢ncia: 0.5%\n",
      "   Empresas aptas: 0 (0.00%)\n",
      "   Erro mÃ©dio: 0.0000%\n",
      "   Desvio padrÃ£o: 0.0000%\n",
      "   Erro mÃ­n/mÃ¡x: 0.0000% / 0.0000%\n",
      "\n",
      "ðŸ“Š TolerÃ¢ncia: 1.0%\n",
      "   Empresas aptas: 0 (0.00%)\n",
      "   Erro mÃ©dio: 0.0000%\n",
      "   Desvio padrÃ£o: 0.0000%\n",
      "   Erro mÃ­n/mÃ¡x: 0.0000% / 0.0000%\n",
      "\n",
      "ðŸ“Š TolerÃ¢ncia: 2.0%\n",
      "   Empresas aptas: 0 (0.00%)\n",
      "   Erro mÃ©dio: 0.0000%\n",
      "   Desvio padrÃ£o: 0.0000%\n",
      "   Erro mÃ­n/mÃ¡x: 0.0000% / 0.0000%\n",
      "\n",
      "ðŸ“Š TolerÃ¢ncia: 5.0%\n",
      "   Empresas aptas: 0 (0.00%)\n",
      "   Erro mÃ©dio: 0.0000%\n",
      "   Desvio padrÃ£o: 0.0000%\n",
      "   Erro mÃ­n/mÃ¡x: 0.0000% / 0.0000%\n",
      "\n",
      "ðŸ“Š TolerÃ¢ncia: 10.0%\n",
      "   Empresas aptas: 0 (0.00%)\n",
      "   Erro mÃ©dio: 0.0000%\n",
      "   Desvio padrÃ£o: 0.0000%\n",
      "   Erro mÃ­n/mÃ¡x: 0.0000% / 0.0000%\n",
      "\n",
      "ðŸ“Š TolerÃ¢ncia: 15.0%\n",
      "   Empresas aptas: 0 (0.00%)\n",
      "   Erro mÃ©dio: 0.0000%\n",
      "   Desvio padrÃ£o: 0.0000%\n",
      "   Erro mÃ­n/mÃ¡x: 0.0000% / 0.0000%\n",
      "\n",
      "ðŸ“Š TolerÃ¢ncia: 20.0%\n",
      "   Empresas aptas: 0 (0.00%)\n",
      "   Erro mÃ©dio: 0.0000%\n",
      "   Desvio padrÃ£o: 0.0000%\n",
      "   Erro mÃ­n/mÃ¡x: 0.0000% / 0.0000%\n",
      "\n",
      "ðŸ“Š RESUMO COMPARATIVO\n",
      "================================================================================\n",
      "TolerÃ¢ncia       Empresas    % Total   Erro MÃ©dio     Desvio\n",
      "--------------------------------------------------------------------------------\n",
      "       0.1%             0       0.00%       0.0000%     0.0000%\n",
      "       0.5%             0       0.00%       0.0000%     0.0000%\n",
      "       1.0%             0       0.00%       0.0000%     0.0000%\n",
      "       2.0%             0       0.00%       0.0000%     0.0000%\n",
      "       5.0%             0       0.00%       0.0000%     0.0000%\n",
      "      10.0%             0       0.00%       0.0000%     0.0000%\n",
      "      15.0%             0       0.00%       0.0000%     0.0000%\n",
      "      20.0%             0       0.00%       0.0000%     0.0000%\n",
      "\n",
      "ðŸ’¡ RECOMENDAÃ‡ÃƒO\n",
      "================================================================================\n",
      "\n",
      "âš ï¸  Nenhuma tolerÃ¢ncia atinge 10% das empresas\n",
      "   Use a maior tolerÃ¢ncia disponÃ­vel: 20.0%\n",
      "\n",
      "ðŸ“Š DISTRIBUIÃ‡ÃƒO DO ERRO (TODAS AS EMPRESAS)\n",
      "================================================================================\n",
      "+----------+------+\n",
      "|faixa_erro|count |\n",
      "+----------+------+\n",
      "|0.1-0.5%  |1     |\n",
      "|0.5-1.0%  |3     |\n",
      "|1.0-2.0%  |2     |\n",
      "|10.0-20.0%|57    |\n",
      "|2.0-5.0%  |9     |\n",
      "|5.0-10.0% |28    |\n",
      "|> 20.0%   |380942|\n",
      "|â‰¤ 0.1%    |3451  |\n",
      "+----------+------+\n",
      "\n",
      "\n",
      "ðŸŽ¯ EMPRESAS COM EQUAÃ‡ÃƒO CONTÃBIL PERFEITA\n",
      "================================================================================\n",
      "\n",
      "âœ… Empresas com erro â‰¤ 0.1%: 0 (0.00%)\n",
      "\n",
      "ðŸ“Š EMPRESAS COM DADOS COMPLETOS\n",
      "================================================================================\n",
      "\n",
      "âœ… Empresas com dados completos: 3 (0.00%)\n",
      "âœ… Com dados completos E equaÃ§Ã£o â‰¤ 10%: 0 (0.00%)\n",
      "\n",
      "================================================================================\n",
      "âœ… SCRIPT 1.1 CONCLUÃDO!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# SCRIPT 1.1: ANÃLISE DE TOLERÃ‚NCIAS NA EQUAÃ‡ÃƒO CONTÃBIL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ðŸ“Š ANÃLISE DE TOLERÃ‚NCIAS NA EQUAÃ‡ÃƒO CONTÃBIL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, count, sum as spark_sum, \n",
    "    round as spark_round, abs as spark_abs\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# CONFIGURAÃ‡Ã•ES\n",
    "# ================================================================================\n",
    "\n",
    "DATABASE = 'neac'\n",
    "ANO_REFERENCIA = 2024\n",
    "\n",
    "# TolerÃ¢ncias a testar\n",
    "TOLERANCIAS = [0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 15.0, 20.0]\n",
    "\n",
    "print(f\"\\nâš™ï¸  ConfiguraÃ§Ãµes:\")\n",
    "print(f\"   Database: {DATABASE}\")\n",
    "print(f\"   Ano: {ANO_REFERENCIA}\")\n",
    "print(f\"   TolerÃ¢ncias a testar: {TOLERANCIAS}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR DADOS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“¥ Carregando dados consolidados...\")\n",
    "\n",
    "df_empresas = spark.table(f\"{DATABASE}.ecd_ml_empresas_consolidado\")\n",
    "total_empresas = df_empresas.count()\n",
    "\n",
    "print(f\"âœ… Total de empresas: {total_empresas:,}\")\n",
    "\n",
    "if total_empresas == 0:\n",
    "    print(\"\\nâš ï¸  Execute Script 1 primeiro!\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ================================================================================\n",
    "# ANÃLISE POR TOLERÃ‚NCIA\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š TESTANDO DIFERENTES TOLERÃ‚NCIAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "resultados = []\n",
    "\n",
    "for tolerancia in TOLERANCIAS:\n",
    "    # Filtrar empresas dentro da tolerÃ¢ncia\n",
    "    df_dentro_tolerancia = df_empresas.filter(\n",
    "        (col('equacao_contabil_perc') <= tolerancia) &\n",
    "        (col('ativo').isNotNull()) &\n",
    "        (col('passivo').isNotNull()) &\n",
    "        (col('patrimonio_liquido').isNotNull()) &\n",
    "        (col('receitas').isNotNull()) &\n",
    "        (col('despesas').isNotNull()) &\n",
    "        (spark_abs(col('ativo')) > 10000)  # Ativo mÃ­nimo\n",
    "    )\n",
    "    \n",
    "    total_aptas = df_dentro_tolerancia.count()\n",
    "    perc_aptas = (total_aptas / total_empresas * 100) if total_empresas > 0 else 0\n",
    "    \n",
    "    # Calcular estatÃ­sticas do erro\n",
    "    stats = df_dentro_tolerancia.select(\n",
    "        spark_round(col('equacao_contabil_perc').cast('double'), 4).alias('erro')\n",
    "    ).summary('mean', 'stddev', 'min', 'max').collect()\n",
    "    \n",
    "    erro_medio = float(stats[0]['erro']) if stats[0]['erro'] else 0\n",
    "    erro_stddev = float(stats[1]['erro']) if stats[1]['erro'] else 0\n",
    "    erro_min = float(stats[2]['erro']) if stats[2]['erro'] else 0\n",
    "    erro_max = float(stats[3]['erro']) if stats[3]['erro'] else 0\n",
    "    \n",
    "    resultados.append({\n",
    "        'tolerancia': tolerancia,\n",
    "        'total_aptas': total_aptas,\n",
    "        'perc_aptas': perc_aptas,\n",
    "        'erro_medio': erro_medio,\n",
    "        'erro_stddev': erro_stddev,\n",
    "        'erro_min': erro_min,\n",
    "        'erro_max': erro_max\n",
    "    })\n",
    "    \n",
    "    print(f\"\\nðŸ“Š TolerÃ¢ncia: {tolerancia}%\")\n",
    "    print(f\"   Empresas aptas: {total_aptas:,} ({perc_aptas:.2f}%)\")\n",
    "    print(f\"   Erro mÃ©dio: {erro_medio:.4f}%\")\n",
    "    print(f\"   Desvio padrÃ£o: {erro_stddev:.4f}%\")\n",
    "    print(f\"   Erro mÃ­n/mÃ¡x: {erro_min:.4f}% / {erro_max:.4f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# RESUMO COMPARATIVO\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š RESUMO COMPARATIVO\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"{'TolerÃ¢ncia':<12} {'Empresas':>12} {'% Total':>10} {'Erro MÃ©dio':>12} {'Desvio':>10}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for r in resultados:\n",
    "    print(f\"{r['tolerancia']:>10.1f}%  {r['total_aptas']:>12,}  {r['perc_aptas']:>9.2f}%  {r['erro_medio']:>11.4f}%  {r['erro_stddev']:>9.4f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# RECOMENDAÃ‡ÃƒO\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ’¡ RECOMENDAÃ‡ÃƒO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Encontrar tolerÃ¢ncia que atinge pelo menos 10% das empresas\n",
    "for r in resultados:\n",
    "    if r['perc_aptas'] >= 10.0:\n",
    "        print(f\"\\nâœ… TolerÃ¢ncia recomendada: {r['tolerancia']}%\")\n",
    "        print(f\"   - Captura: {r['total_aptas']:,} empresas ({r['perc_aptas']:.2f}%)\")\n",
    "        print(f\"   - Erro mÃ©dio: {r['erro_medio']:.4f}%\")\n",
    "        print(f\"   - Qualidade: {'ALTA' if r['erro_medio'] < 1.0 else 'MÃ‰DIA' if r['erro_medio'] < 5.0 else 'BAIXA'}\")\n",
    "        break\n",
    "else:\n",
    "    print(f\"\\nâš ï¸  Nenhuma tolerÃ¢ncia atinge 10% das empresas\")\n",
    "    print(f\"   Use a maior tolerÃ¢ncia disponÃ­vel: {TOLERANCIAS[-1]}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# DISTRIBUIÃ‡ÃƒO DO ERRO\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š DISTRIBUIÃ‡ÃƒO DO ERRO (TODAS AS EMPRESAS)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_empresas.groupBy(\n",
    "    when(col('equacao_contabil_perc') <= 0.1, 'â‰¤ 0.1%')\n",
    "    .when(col('equacao_contabil_perc') <= 0.5, '0.1-0.5%')\n",
    "    .when(col('equacao_contabil_perc') <= 1.0, '0.5-1.0%')\n",
    "    .when(col('equacao_contabil_perc') <= 2.0, '1.0-2.0%')\n",
    "    .when(col('equacao_contabil_perc') <= 5.0, '2.0-5.0%')\n",
    "    .when(col('equacao_contabil_perc') <= 10.0, '5.0-10.0%')\n",
    "    .when(col('equacao_contabil_perc') <= 20.0, '10.0-20.0%')\n",
    "    .otherwise('> 20.0%')\n",
    "    .alias('faixa_erro')\n",
    ").count().orderBy('faixa_erro').show(truncate=False)\n",
    "\n",
    "# ================================================================================\n",
    "# ANÃLISE DE EMPRESAS COM ERRO ZERO\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸŽ¯ EMPRESAS COM EQUAÃ‡ÃƒO CONTÃBIL PERFEITA\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_perfeitas = df_empresas.filter(\n",
    "    (col('equacao_contabil_perc') <= 0.1) &\n",
    "    (spark_abs(col('ativo')) > 10000)\n",
    ")\n",
    "\n",
    "total_perfeitas = df_perfeitas.count()\n",
    "perc_perfeitas = (total_perfeitas / total_empresas * 100) if total_empresas > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… Empresas com erro â‰¤ 0.1%: {total_perfeitas:,} ({perc_perfeitas:.2f}%)\")\n",
    "\n",
    "if total_perfeitas > 0:\n",
    "    print(f\"\\nðŸ“‹ Top 10 empresas perfeitas:\")\n",
    "    df_perfeitas.select(\n",
    "        'cnpj',\n",
    "        'total_contas',\n",
    "        spark_round('equacao_contabil_perc', 6).alias('erro_%'),\n",
    "        spark_round('ativo', 2).alias('ativo'),\n",
    "        spark_round('passivo', 2).alias('passivo'),\n",
    "        spark_round('patrimonio_liquido', 2).alias('pl')\n",
    "    ).orderBy(col('equacao_contabil_perc').asc()) \\\n",
    "     .limit(10) \\\n",
    "     .show(truncate=False)\n",
    "\n",
    "# ================================================================================\n",
    "# ANÃLISE DE EMPRESAS COM DADOS COMPLETOS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š EMPRESAS COM DADOS COMPLETOS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_completas = df_empresas.filter(\n",
    "    (col('ativo').isNotNull()) &\n",
    "    (col('passivo').isNotNull()) &\n",
    "    (col('patrimonio_liquido').isNotNull()) &\n",
    "    (col('receitas').isNotNull()) &\n",
    "    (col('despesas').isNotNull())\n",
    ")\n",
    "\n",
    "total_completas = df_completas.count()\n",
    "perc_completas = (total_completas / total_empresas * 100) if total_empresas > 0 else 0\n",
    "\n",
    "print(f\"\\nâœ… Empresas com dados completos: {total_completas:,} ({perc_completas:.2f}%)\")\n",
    "\n",
    "# Dentro dessas, quantas tÃªm boa equaÃ§Ã£o?\n",
    "df_completas_boas = df_completas.filter(col('equacao_contabil_perc') <= 10.0)\n",
    "total_completas_boas = df_completas_boas.count()\n",
    "perc_completas_boas = (total_completas_boas / total_completas * 100) if total_completas > 0 else 0\n",
    "\n",
    "print(f\"âœ… Com dados completos E equaÃ§Ã£o â‰¤ 10%: {total_completas_boas:,} ({perc_completas_boas:.2f}%)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… SCRIPT 1.1 CONCLUÃDO!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "525cc9ee-9d02-4050-8c9b-2be1b9554c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” IDENTIFICAR EMPRESAS CANDIDATAS PARA AJUSTE MANUAL\n",
      "================================================================================\n",
      "\n",
      "âš™ï¸  CritÃ©rios:\n",
      "   Erro mÃ­nimo: 1.0%\n",
      "   Erro mÃ¡ximo: 15.0%\n",
      "   Ativo mÃ­nimo: R$ 100,000.00\n",
      "   Top N: 50\n",
      "\n",
      "ðŸ“¥ Carregando dados...\n",
      "âœ… Total: 384,493 empresas\n",
      "\n",
      "ðŸ” Filtrando candidatas para ajuste...\n",
      "âœ… Candidatas: 52 (0.01%)\n",
      "\n",
      "ðŸ“Š PRIORIZANDO CANDIDATAS...\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ðŸ† TOP 50 CANDIDATAS PARA AJUSTE MANUAL\n",
      "================================================================================\n",
      "+------+------+------------+-------+-----------+------------+------------+------------+------------------+\n",
      "|cnpj  |id_ecd|total_contas|erro_% |erro_abs   |ativo       |passivo     |pl          |potencial_correcao|\n",
      "+------+------+------------+-------+-----------+------------+------------+------------+------------------+\n",
      "|403178|403178|68          |2.0265 |243207.86  |12001247.11 |12400289.25 |-642250.00  |MEDIO             |\n",
      "|298154|298154|54          |8.4766 |-2886893.36|34057245.60 |37231459.96 |-287321.00  |BAIXO             |\n",
      "|328190|328190|203         |4.9084 |-927708.68 |-18900473.69|-17637165.01|-335600.00  |MEDIO             |\n",
      "|39220 |39220 |220         |7.4770 |2118054.32 |28327415.80 |26409361.48 |-200000.00  |BAIXO             |\n",
      "|142397|142397|519         |7.9902 |-1335136.48|16709643.84 |18844780.32 |-800000.00  |BAIXO             |\n",
      "|350299|350299|519         |7.9902 |-1335136.48|16709643.84 |18844780.32 |-800000.00  |BAIXO             |\n",
      "|238218|238218|2337        |13.7070|3473091.25 |25338073.75 |30397036.50 |-8532054.00 |BAIXO             |\n",
      "|303375|303375|1981        |14.1835|-2147305.03|15139423.99 |17661729.02 |-375000.00  |BAIXO             |\n",
      "|160085|160085|2025        |14.3067|-1701352.10|11891979.45 |13893331.55 |-300000.00  |BAIXO             |\n",
      "|368158|368158|2025        |14.3067|-1701352.10|11891979.45 |13893331.55 |-300000.00  |BAIXO             |\n",
      "|260712|260712|483         |11.8553|-855759.78 |7218358.83  |-15440127.48|23514246.09 |BAIXO             |\n",
      "|408297|408297|173         |6.6050 |-239178.82 |3621188.32  |8499776.70  |-4639409.56 |BAIXO             |\n",
      "|143183|143183|260         |9.2667 |356157.52  |3843415.60  |3487258.08  |0.00        |BAIXO             |\n",
      "|329532|329532|51          |9.4602 |282535.31  |2986583.60  |2874048.29  |-170000.00  |BAIXO             |\n",
      "|346691|346691|50          |10.3440|316658.59  |3061282.64  |2914624.05  |-170000.00  |BAIXO             |\n",
      "|386588|386588|49          |10.3351|-308902.38 |2988876.64  |3467779.02  |-170000.00  |BAIXO             |\n",
      "|208550|208550|125         |7.8625 |172631.92  |2195646.45  |2123014.53  |-100000.00  |BAIXO             |\n",
      "|413553|413553|125         |7.8625 |172631.92  |2195646.45  |2123014.53  |-100000.00  |BAIXO             |\n",
      "|299990|299990|4913        |9.4395 |229446.72  |2430709.60  |19481262.88 |-17280000.00|BAIXO             |\n",
      "|340742|340742|73          |6.7314 |115131.05  |-1710356.67 |-1795487.72 |-30000.00   |BAIXO             |\n",
      "+------+------+------------+-------+-----------+------------+------------+------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "\n",
      "ðŸ’¾ Salvando lista de candidatas...\n",
      "âœ… Salvo: neac.ecd_ml_candidatas_ajuste_manual\n",
      "\n",
      "ðŸ”¬ ANÃLISE DETALHADA DA EMPRESA #1\n",
      "================================================================================\n",
      "\n",
      "ðŸ“‹ Empresa: 403178\n",
      "   ID ECD: 403178\n",
      "   Erro: 2.0265%\n",
      "   Ativo: R$ 12,001,247.11\n",
      "   Passivo: R$ 12,400,289.25\n",
      "   PL: R$ -642,250.00\n",
      "\n",
      "ðŸ“Š DistribuiÃ§Ã£o de contas por grupo:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+\n",
      "|classificacao_nivel2|qtd_contas|valor_total|\n",
      "+--------------------+----------+-----------+\n",
      "|PASSIVO_CIRCULANTE  |31        |14517841.88|\n",
      "|ATIVO_CIRCULANTE    |21        |8442357.52 |\n",
      "|ATIVO               |3         |3465556.63 |\n",
      "|DISPONIBILIDADES    |3         |60411.53   |\n",
      "|IMOBILIZADO         |1         |32921.43   |\n",
      "|PASSIVO             |7         |-423507.68 |\n",
      "|CAPITAL_SOCIAL      |1         |-642250.00 |\n",
      "|FORNECEDORES        |1         |-1694044.95|\n",
      "+--------------------+----------+-----------+\n",
      "\n",
      "\n",
      "ðŸ” POSSÃVEIS PROBLEMAS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š ESTATÃSTICAS DAS CANDIDATAS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“ˆ DistribuiÃ§Ã£o por potencial de correÃ§Ã£o:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+-----+\n",
      "|potencial_correcao|count|\n",
      "+------------------+-----+\n",
      "|              ALTO|    2|\n",
      "|             BAIXO|   45|\n",
      "|             MEDIO|    5|\n",
      "+------------------+-----+\n",
      "\n",
      "\n",
      "ðŸ“ˆ DistribuiÃ§Ã£o por faixa de erro:\n",
      "+----------+-----+\n",
      "|faixa_erro|count|\n",
      "+----------+-----+\n",
      "|      1-2%|    2|\n",
      "|    10-15%|   24|\n",
      "|      2-5%|    5|\n",
      "|     5-10%|   21|\n",
      "+----------+-----+\n",
      "\n",
      "\n",
      "ðŸ“ˆ EstatÃ­sticas de ativo das candidatas:\n",
      "+-------+-----------------+\n",
      "|summary|ativo            |\n",
      "+-------+-----------------+\n",
      "|count  |52               |\n",
      "|mean   |3587705.966346   |\n",
      "|stddev |8463930.069514671|\n",
      "|min    |-18900473.69     |\n",
      "|25%    |137773.84        |\n",
      "|50%    |412130.93        |\n",
      "|75%    |2988876.64       |\n",
      "|max    |34057245.60      |\n",
      "+-------+-----------------+\n",
      "\n",
      "\n",
      "ðŸ’¡ RECOMENDAÃ‡Ã•ES\n",
      "================================================================================\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "min() takes 1 positional argument but 2 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 231\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mðŸ’¡ RECOMENDAÃ‡Ã•ES\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m80\u001b[39m)\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m1. ðŸŽ¯ PRIORIZE AS TOP \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;250;43m \u001b[39;49m\u001b[43mTOP_N\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m EMPRESAS\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    232\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - Maior potencial de correÃ§Ã£o\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m   - Maior ativo (impacto)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/miniforge3/envs/conda_data_pipeline/lib/python3.10/site-packages/pyspark/sql/utils.py:174\u001b[0m, in \u001b[0;36mtry_remote_functions.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(functions, f\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 174\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: min() takes 1 positional argument but 2 were given"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# SCRIPT 1.2: IDENTIFICAR EMPRESAS CANDIDATAS PARA AJUSTE MANUAL\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ðŸ” IDENTIFICAR EMPRESAS CANDIDATAS PARA AJUSTE MANUAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, count, sum as spark_sum, \n",
    "    round as spark_round, abs as spark_abs,\n",
    "    desc, asc\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# CONFIGURAÃ‡Ã•ES\n",
    "# ================================================================================\n",
    "\n",
    "DATABASE = 'neac'\n",
    "DATABASE_ECD = 'usr_sat_ecd'\n",
    "ANO_REFERENCIA = 2024\n",
    "\n",
    "# CritÃ©rios para candidatas a ajuste\n",
    "ERRO_MIN = 1.0   # Erro mÃ­nimo: 1% (abaixo disso jÃ¡ estÃ¡ bom)\n",
    "ERRO_MAX = 15.0  # Erro mÃ¡ximo: 15% (acima disso, muito ruim)\n",
    "MIN_ATIVO = 100000  # Ativo mÃ­nimo para considerar\n",
    "TOP_N = 50  # Top N empresas para analisar\n",
    "\n",
    "print(f\"\\nâš™ï¸  CritÃ©rios:\")\n",
    "print(f\"   Erro mÃ­nimo: {ERRO_MIN}%\")\n",
    "print(f\"   Erro mÃ¡ximo: {ERRO_MAX}%\")\n",
    "print(f\"   Ativo mÃ­nimo: R$ {MIN_ATIVO:,.2f}\")\n",
    "print(f\"   Top N: {TOP_N}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR DADOS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“¥ Carregando dados...\")\n",
    "\n",
    "df_empresas = spark.table(f\"{DATABASE}.ecd_ml_empresas_consolidado\")\n",
    "total_empresas = df_empresas.count()\n",
    "\n",
    "print(f\"âœ… Total: {total_empresas:,} empresas\")\n",
    "\n",
    "if total_empresas == 0:\n",
    "    print(\"\\nâš ï¸  Execute Script 1 primeiro!\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ================================================================================\n",
    "# FILTRAR CANDIDATAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ” Filtrando candidatas para ajuste...\")\n",
    "\n",
    "df_candidatas = df_empresas.filter(\n",
    "    (col('equacao_contabil_perc') > ERRO_MIN) &\n",
    "    (col('equacao_contabil_perc') <= ERRO_MAX) &\n",
    "    (spark_abs(col('ativo')) >= MIN_ATIVO) &\n",
    "    (col('ativo').isNotNull()) &\n",
    "    (col('passivo').isNotNull()) &\n",
    "    (col('patrimonio_liquido').isNotNull())\n",
    ")\n",
    "\n",
    "total_candidatas = df_candidatas.count()\n",
    "perc_candidatas = (total_candidatas / total_empresas * 100) if total_empresas > 0 else 0\n",
    "\n",
    "print(f\"âœ… Candidatas: {total_candidatas:,} ({perc_candidatas:.2f}%)\")\n",
    "\n",
    "if total_candidatas == 0:\n",
    "    print(\"\\nâš ï¸  Nenhuma candidata encontrada com esses critÃ©rios!\")\n",
    "    print(f\"   Tente ajustar ERRO_MIN, ERRO_MAX ou MIN_ATIVO\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ================================================================================\n",
    "# PRIORIZAR POR CRITÃ‰RIOS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š PRIORIZANDO CANDIDATAS...\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Adicionar score de prioridade\n",
    "df_priorizadas = df_candidatas.withColumn(\n",
    "    'score_prioridade',\n",
    "    # Quanto menor o erro, maior a prioridade (mais fÃ¡cil de corrigir)\n",
    "    # Quanto maior o ativo, maior a prioridade (mais importante)\n",
    "    (1 / col('equacao_contabil_perc')) * spark_abs(col('ativo'))\n",
    ").withColumn(\n",
    "    'potencial_correcao',\n",
    "    when(col('equacao_contabil_perc') <= 2.0, 'ALTO')\n",
    "    .when(col('equacao_contabil_perc') <= 5.0, 'MEDIO')\n",
    "    .otherwise('BAIXO')\n",
    ")\n",
    "\n",
    "# ================================================================================\n",
    "# TOP CANDIDATAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ† TOP {TOP_N} CANDIDATAS PARA AJUSTE MANUAL\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "df_top = df_priorizadas.select(\n",
    "    'cnpj',\n",
    "    'id_ecd',\n",
    "    'total_contas',\n",
    "    spark_round('equacao_contabil_perc', 4).alias('erro_%'),\n",
    "    spark_round('equacao_contabil', 2).alias('erro_abs'),\n",
    "    spark_round('ativo', 2).alias('ativo'),\n",
    "    spark_round('passivo', 2).alias('passivo'),\n",
    "    spark_round('patrimonio_liquido', 2).alias('pl'),\n",
    "    'potencial_correcao'\n",
    ").orderBy(col('score_prioridade').desc())\n",
    "\n",
    "df_top.limit(TOP_N).show(truncate=False)\n",
    "\n",
    "# ================================================================================\n",
    "# SALVAR LISTA DE CANDIDATAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ’¾ Salvando lista de candidatas...\")\n",
    "\n",
    "df_top.createOrReplaceTempView(\"temp_candidatas_ajuste\")\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_candidatas_ajuste_manual\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {DATABASE}.ecd_ml_candidatas_ajuste_manual\n",
    "    STORED AS PARQUET\n",
    "    AS SELECT * FROM temp_candidatas_ajuste\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Salvo: {DATABASE}.ecd_ml_candidatas_ajuste_manual\")\n",
    "\n",
    "# ================================================================================\n",
    "# ANÃLISE DETALHADA DE UMA EMPRESA EXEMPLO\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ”¬ ANÃLISE DETALHADA DA EMPRESA #1\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Pegar primeira empresa\n",
    "empresa_exemplo = df_top.first()\n",
    "\n",
    "if empresa_exemplo:\n",
    "    id_ecd_exemplo = empresa_exemplo['id_ecd']\n",
    "    cnpj_exemplo = empresa_exemplo['cnpj']\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Empresa: {cnpj_exemplo}\")\n",
    "    print(f\"   ID ECD: {id_ecd_exemplo}\")\n",
    "    print(f\"   Erro: {empresa_exemplo['erro_%']:.4f}%\")\n",
    "    print(f\"   Ativo: R$ {empresa_exemplo['ativo']:,.2f}\")\n",
    "    print(f\"   Passivo: R$ {empresa_exemplo['passivo']:,.2f}\")\n",
    "    print(f\"   PL: R$ {empresa_exemplo['pl']:,.2f}\")\n",
    "    \n",
    "    # Buscar detalhes das contas\n",
    "    print(f\"\\nðŸ“Š DistribuiÃ§Ã£o de contas por grupo:\")\n",
    "    \n",
    "    df_contas_empresa = spark.sql(f\"\"\"\n",
    "        SELECT \n",
    "            pred.classificacao_nivel2,\n",
    "            COUNT(*) as qtd_contas,\n",
    "            ROUND(SUM(\n",
    "                CASE \n",
    "                    WHEN bp.ind_dc_cta_fin = 'D' THEN bp.vl_cta_fin\n",
    "                    WHEN bp.ind_dc_cta_fin = 'C' THEN -bp.vl_cta_fin\n",
    "                    ELSE bp.vl_cta_fin\n",
    "                END\n",
    "            ), 2) as valor_total\n",
    "        FROM {DATABASE}.ecd_ml_predictions_ALL pred\n",
    "        INNER JOIN {DATABASE_ECD}.ecd_rj100_balanco_patrimonial bp\n",
    "            ON pred.id_ecd = bp.id_ecd\n",
    "            AND pred.cd_conta = bp.cod_agl\n",
    "        WHERE pred.id_ecd = {id_ecd_exemplo}\n",
    "        GROUP BY pred.classificacao_nivel2\n",
    "        ORDER BY valor_total DESC\n",
    "    \"\"\")\n",
    "    \n",
    "    df_contas_empresa.show(30, truncate=False)\n",
    "    \n",
    "    # Identificar possÃ­veis problemas\n",
    "    print(f\"\\nðŸ” POSSÃVEIS PROBLEMAS:\")\n",
    "    \n",
    "    # Verificar se hÃ¡ grupos com valores muito pequenos\n",
    "    df_problemas = df_contas_empresa.filter(\n",
    "        (spark_abs(col('valor_total')) > 0) & \n",
    "        (spark_abs(col('valor_total')) < 1000)\n",
    "    )\n",
    "    \n",
    "    if df_problemas.count() > 0:\n",
    "        print(f\"\\nâš ï¸  Grupos com valores pequenos (< R$ 1.000):\")\n",
    "        df_problemas.show(truncate=False)\n",
    "    \n",
    "    # Verificar se hÃ¡ classificaÃ§Ãµes estranhas\n",
    "    df_outros = df_contas_empresa.filter(col('classificacao_nivel2') == 'OUTROS')\n",
    "    \n",
    "    if df_outros.count() > 0:\n",
    "        print(f\"\\nâš ï¸  Contas classificadas como 'OUTROS':\")\n",
    "        df_outros.show(truncate=False)\n",
    "\n",
    "# ================================================================================\n",
    "# ESTATÃSTICAS DAS CANDIDATAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š ESTATÃSTICAS DAS CANDIDATAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DistribuiÃ§Ã£o por potencial de correÃ§Ã£o:\")\n",
    "df_priorizadas.groupBy('potencial_correcao').count() \\\n",
    "    .orderBy('potencial_correcao').show()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ DistribuiÃ§Ã£o por faixa de erro:\")\n",
    "df_priorizadas.groupBy(\n",
    "    when(col('equacao_contabil_perc') <= 2.0, '1-2%')\n",
    "    .when(col('equacao_contabil_perc') <= 5.0, '2-5%')\n",
    "    .when(col('equacao_contabil_perc') <= 10.0, '5-10%')\n",
    "    .otherwise('10-15%')\n",
    "    .alias('faixa_erro')\n",
    ").count().orderBy('faixa_erro').show()\n",
    "\n",
    "print(f\"\\nðŸ“ˆ EstatÃ­sticas de ativo das candidatas:\")\n",
    "df_priorizadas.select('ativo').summary(\n",
    "    'count', 'mean', 'stddev', 'min', '25%', '50%', '75%', 'max'\n",
    ").show(truncate=False)\n",
    "\n",
    "# ================================================================================\n",
    "# RECOMENDAÃ‡Ã•ES\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ’¡ RECOMENDAÃ‡Ã•ES\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\n1. ðŸŽ¯ PRIORIZE AS TOP {min(10, TOP_N)} EMPRESAS\")\n",
    "print(f\"   - Maior potencial de correÃ§Ã£o\")\n",
    "print(f\"   - Maior ativo (impacto)\")\n",
    "\n",
    "print(f\"\\n2. ðŸ” INVESTIGUE:\")\n",
    "print(f\"   - Contas classificadas como 'OUTROS'\")\n",
    "print(f\"   - Grupos com valores muito pequenos\")\n",
    "print(f\"   - Contas com nomes genÃ©ricos\")\n",
    "\n",
    "print(f\"\\n3. âœï¸ AJUSTES POSSÃVEIS:\")\n",
    "print(f\"   - Reclassificar manualmente algumas contas\")\n",
    "print(f\"   - Criar regras especÃ­ficas para padrÃµes identificados\")\n",
    "print(f\"   - Retreinar modelo com exemplos corrigidos\")\n",
    "\n",
    "print(f\"\\n4. ðŸ“Š VALIDAÃ‡ÃƒO:\")\n",
    "print(f\"   - ApÃ³s ajustes, recalcular equaÃ§Ã£o\")\n",
    "print(f\"   - Verificar se erro caiu abaixo de {ERRO_MIN}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… SCRIPT 1.2 CONCLUÃDO!\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "print(f\"\\nðŸ“‹ PrÃ³ximos passos:\")\n",
    "print(f\"   1. Analise a tabela: {DATABASE}.ecd_ml_candidatas_ajuste_manual\")\n",
    "print(f\"   2. Escolha empresas para ajuste manual\")\n",
    "print(f\"   3. Use Script 2 e 3 com tolerÃ¢ncia ajustada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c9150b81-b968-4364-809a-bf5e50fa7c88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¼ EMPRESAS APTAS PARA CÃLCULO DE INDICADORES FINANCEIROS\n",
      "================================================================================\n",
      "\n",
      "âš™ï¸  CritÃ©rios de aptidÃ£o:\n",
      "   - Erro mÃ¡ximo equaÃ§Ã£o: 10.0%\n",
      "   - Ativo mÃ­nimo: R$ 10,000.00\n",
      "\n",
      "ðŸ“¥ Carregando dados consolidados...\n",
      "âœ… 384,493 empresas carregadas\n",
      "\n",
      "ðŸ” Filtrando empresas aptas...\n",
      "âœ… 0 empresas aptas (0.00% do total)\n",
      "\n",
      "================================================================================\n",
      "âš ï¸  NENHUMA EMPRESA APTA ENCONTRADA!\n",
      "================================================================================\n",
      "\n",
      "ðŸ’¡ SugestÃµes:\n",
      "   1. Aumentar MAX_ERRO_EQUACAO (atual: 10.0%)\n",
      "   2. Reduzir MIN_ATIVO (atual: R$ 10,000.00)\n",
      "\n",
      "================================================================================\n",
      "âœ… SCRIPT 2 CONCLUÃDO!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# SCRIPT 2: EMPRESAS APTAS E CÃLCULO DE INDICADORES FINANCEIROS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ðŸ’¼ EMPRESAS APTAS PARA CÃLCULO DE INDICADORES FINANCEIROS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, lit, round as spark_round, abs as spark_abs,\n",
    "    coalesce, count, sum as spark_sum, avg\n",
    ")\n",
    "import pandas as pd\n",
    "\n",
    "# ================================================================================\n",
    "# CONFIGURAÃ‡Ã•ES\n",
    "# ================================================================================\n",
    "\n",
    "DATABASE = 'neac'\n",
    "ANO_REFERENCIA = 2024\n",
    "\n",
    "# CritÃ©rios de aptidÃ£o (AJUSTÃVEIS)\n",
    "MAX_ERRO_EQUACAO = 10.0      # MÃ¡ximo 10% de erro na equaÃ§Ã£o contÃ¡bil\n",
    "MIN_ATIVO = 10000            # Ativo mÃ­nimo\n",
    "\n",
    "print(f\"\\nâš™ï¸  CritÃ©rios de aptidÃ£o:\")\n",
    "print(f\"   - Erro mÃ¡ximo equaÃ§Ã£o: {MAX_ERRO_EQUACAO}%\")\n",
    "print(f\"   - Ativo mÃ­nimo: R$ {MIN_ATIVO:,.2f}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR DADOS CONSOLIDADOS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“¥ Carregando dados consolidados...\")\n",
    "\n",
    "df_empresas = spark.table(f\"{DATABASE}.ecd_ml_empresas_consolidado\")\n",
    "total_empresas = df_empresas.count()\n",
    "\n",
    "print(f\"âœ… {total_empresas:,} empresas carregadas\")\n",
    "\n",
    "# ================================================================================\n",
    "# FILTRAR EMPRESAS APTAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ” Filtrando empresas aptas...\")\n",
    "\n",
    "df_aptas = df_empresas.filter(\n",
    "    (col('equacao_contabil_perc') <= MAX_ERRO_EQUACAO) &\n",
    "    (col('ativo').isNotNull()) &\n",
    "    (col('passivo').isNotNull()) &\n",
    "    (col('patrimonio_liquido').isNotNull()) &\n",
    "    (col('receitas').isNotNull()) &\n",
    "    (col('despesas').isNotNull()) &\n",
    "    (spark_abs(col('ativo')) >= MIN_ATIVO)\n",
    ")\n",
    "\n",
    "total_aptas = df_aptas.count()\n",
    "perc_aptas = (total_aptas / total_empresas * 100) if total_empresas > 0 else 0\n",
    "\n",
    "print(f\"âœ… {total_aptas:,} empresas aptas ({perc_aptas:.2f}% do total)\")\n",
    "\n",
    "if total_aptas == 0:\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"âš ï¸  NENHUMA EMPRESA APTA ENCONTRADA!\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"\\nðŸ’¡ SugestÃµes:\")\n",
    "    print(f\"   1. Aumentar MAX_ERRO_EQUACAO (atual: {MAX_ERRO_EQUACAO}%)\")\n",
    "    print(f\"   2. Reduzir MIN_ATIVO (atual: R$ {MIN_ATIVO:,.2f})\")\n",
    "    \n",
    "else:\n",
    "    # ============================================================================\n",
    "    # ANÃLISE PRELIMINAR\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nðŸ“Š ANÃLISE PRELIMINAR\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # DistribuiÃ§Ã£o de erro\n",
    "    print(f\"\\nâš–ï¸  DistribuiÃ§Ã£o de erro na equaÃ§Ã£o:\")\n",
    "    df_aptas.groupBy(\n",
    "        when(col('equacao_contabil_perc') <= 1.0, 'â‰¤ 1%')\n",
    "        .when(col('equacao_contabil_perc') <= 2.0, '1-2%')\n",
    "        .when(col('equacao_contabil_perc') <= 5.0, '2-5%')\n",
    "        .otherwise('5-10%')\n",
    "        .alias('faixa_erro')\n",
    "    ).count().orderBy('faixa_erro').show()\n",
    "    \n",
    "    # EstatÃ­sticas dos valores\n",
    "    print(f\"\\nðŸ’° EstatÃ­sticas dos valores contÃ¡beis:\")\n",
    "    df_aptas.select(\n",
    "        spark_round(avg('ativo'), 2).alias('ativo_medio'),\n",
    "        spark_round(avg('passivo'), 2).alias('passivo_medio'),\n",
    "        spark_round(avg('patrimonio_liquido'), 2).alias('pl_medio'),\n",
    "        spark_round(avg('receitas'), 2).alias('receitas_media'),\n",
    "        spark_round(avg('resultado'), 2).alias('resultado_medio')\n",
    "    ).show(truncate=False)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # CALCULAR INDICADORES FINANCEIROS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nðŸ“Š CALCULANDO INDICADORES FINANCEIROS...\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    df_indicadores = df_aptas.withColumn(\n",
    "        # ========================================================================\n",
    "        # LIQUIDEZ\n",
    "        # ========================================================================\n",
    "        'liquidez_corrente',\n",
    "        when(col('passivo') != 0,\n",
    "             spark_round(col('ativo') / col('passivo'), 4)\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        'liquidez_geral',\n",
    "        when(col('passivo') != 0,\n",
    "             spark_round(col('ativo') / col('passivo'), 4)\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        # ========================================================================\n",
    "        # ENDIVIDAMENTO\n",
    "        # ========================================================================\n",
    "        'endividamento_geral',\n",
    "        when(col('ativo') != 0,\n",
    "             spark_round((col('passivo') / col('ativo')) * 100, 2)\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        'participacao_capital_terceiros',\n",
    "        when((col('patrimonio_liquido') != 0),\n",
    "             spark_round((col('passivo') / col('patrimonio_liquido')) * 100, 2)\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        'composicao_endividamento',\n",
    "        when((col('passivo') + col('patrimonio_liquido')) != 0,\n",
    "             spark_round(\n",
    "                 (col('passivo') / (col('passivo') + col('patrimonio_liquido'))) * 100, 2\n",
    "             )\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        # ========================================================================\n",
    "        # ESTRUTURA DE CAPITAL\n",
    "        # ========================================================================\n",
    "        'imobilizacao_pl',\n",
    "        when(col('patrimonio_liquido') != 0,\n",
    "             spark_round((col('ativo') / col('patrimonio_liquido')) * 100, 2)\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        'imobilizacao_recursos_nao_correntes',\n",
    "        when((col('patrimonio_liquido') + col('passivo')) != 0,\n",
    "             spark_round(\n",
    "                 (col('ativo') / (col('patrimonio_liquido') + col('passivo'))) * 100, 2\n",
    "             )\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        # ========================================================================\n",
    "        # RENTABILIDADE\n",
    "        # ========================================================================\n",
    "        'margem_liquida',\n",
    "        when(col('receitas') != 0,\n",
    "             spark_round((col('resultado') / col('receitas')) * 100, 2)\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        'rentabilidade_ativo',\n",
    "        when(col('ativo') != 0,\n",
    "             spark_round((col('resultado') / col('ativo')) * 100, 2)\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        'rentabilidade_pl',\n",
    "        when(col('patrimonio_liquido') != 0,\n",
    "             spark_round((col('resultado') / col('patrimonio_liquido')) * 100, 2)\n",
    "        ).otherwise(None)\n",
    "    ).withColumn(\n",
    "        # ========================================================================\n",
    "        # GIRO\n",
    "        # ========================================================================\n",
    "        'giro_ativo',\n",
    "        when(col('ativo') != 0,\n",
    "             spark_round(col('receitas') / col('ativo'), 4)\n",
    "        ).otherwise(None)\n",
    "    )\n",
    "    \n",
    "    print(\"âœ… Indicadores calculados!\")\n",
    "    \n",
    "    indicadores_calculados = [\n",
    "        'liquidez_corrente', 'liquidez_geral',\n",
    "        'endividamento_geral', 'participacao_capital_terceiros', 'composicao_endividamento',\n",
    "        'imobilizacao_pl', 'imobilizacao_recursos_nao_correntes',\n",
    "        'margem_liquida', 'rentabilidade_ativo', 'rentabilidade_pl',\n",
    "        'giro_ativo'\n",
    "    ]\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SALVAR\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Salvando empresas aptas com indicadores...\")\n",
    "    \n",
    "    df_indicadores.createOrReplaceTempView(\"temp_empresas_aptas_indicadores\")\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_empresas_aptas_indicadores\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {DATABASE}.ecd_ml_empresas_aptas_indicadores\n",
    "        STORED AS PARQUET\n",
    "        AS SELECT * FROM temp_empresas_aptas_indicadores\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"âœ… Salvo: {DATABASE}.ecd_ml_empresas_aptas_indicadores\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # ESTATÃSTICAS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nðŸ“Š ESTATÃSTICAS DOS INDICADORES\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    stats = df_indicadores.select(indicadores_calculados).summary(\n",
    "        'count', 'mean', 'stddev', 'min', '25%', '50%', '75%', 'max'\n",
    "    )\n",
    "    \n",
    "    stats.show(truncate=False)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # OUTLIERS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nâš ï¸  ANÃLISE DE VALORES EXTREMOS\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    df_extremos = df_indicadores.filter(\n",
    "        (spark_abs(col('endividamento_geral')) > 100) |\n",
    "        (spark_abs(col('rentabilidade_pl')) > 200) |\n",
    "        (spark_abs(col('rentabilidade_ativo')) > 100)\n",
    "    )\n",
    "    \n",
    "    total_extremos = df_extremos.count()\n",
    "    print(f\"Empresas com valores extremos: {total_extremos:,} ({total_extremos/total_aptas*100:.2f}%)\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # TOP EMPRESAS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nðŸ† TOP 20 EMPRESAS (MENOR ERRO NA EQUAÃ‡ÃƒO)\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    df_indicadores.select(\n",
    "        'cnpj',\n",
    "        'total_contas',\n",
    "        'equacao_contabil_perc',\n",
    "        spark_round('ativo', 2).alias('ativo'),\n",
    "        spark_round('patrimonio_liquido', 2).alias('pl'),\n",
    "        spark_round('resultado', 2).alias('resultado'),\n",
    "        'rentabilidade_pl',\n",
    "        'endividamento_geral'\n",
    "    ).orderBy(col('equacao_contabil_perc').asc()) \\\n",
    "     .limit(20) \\\n",
    "     .show(truncate=False)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # RESUMO\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"ðŸ“Š RESUMO FINAL - SCRIPT 2\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nâœ… Total de empresas aptas: {total_aptas:,}\")\n",
    "    print(f\"âœ… Indicadores calculados: {len(indicadores_calculados)}\")\n",
    "    print(f\"âœ… Tabela: {DATABASE}.ecd_ml_empresas_aptas_indicadores\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Indicadores disponÃ­veis:\")\n",
    "    for i, ind in enumerate(indicadores_calculados, 1):\n",
    "        print(f\"   {i:2d}. {ind}\")\n",
    "    \n",
    "    print(f\"\\nðŸŽ¯ PrÃ³ximo passo: Execute Script 3 para Ã­ndices-padrÃ£o\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… SCRIPT 2 CONCLUÃDO!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2c967902-fbcb-48cd-9ae0-ae634a0e7b9a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“Š CÃLCULO DE ÃNDICES-PADRÃƒO POR PORTE (MÃ‰TODO MATARAZZO)\n",
      "================================================================================\n",
      "\n",
      "ðŸ“š Metodologia: Matarazzo (1998)\n",
      "\n",
      "âš™ï¸  ConfiguraÃ§Ãµes:\n",
      "   - Pequeno: atÃ© R$ 1,000,000\n",
      "   - MÃ©dio: atÃ© R$ 10,000,000\n",
      "   - Grande: acima de R$ 10,000,000\n",
      "   - MÃ­nimo para decis: 30\n",
      "\n",
      "ðŸ“¥ Carregando...\n",
      "âœ… 38 empresas\n",
      "\n",
      "ðŸ¢ Classificando por porte...\n",
      "\n",
      "ðŸ“Š DistribuiÃ§Ã£o:\n",
      "+-------+-----+\n",
      "|  porte|count|\n",
      "+-------+-----+\n",
      "| GRANDE|    2|\n",
      "|  MEDIO|   18|\n",
      "|PEQUENO|   18|\n",
      "+-------+-----+\n",
      "\n",
      "\n",
      "âš ï¸  Nenhum porte tem 30+ empresas!\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# SCRIPT 3: CÃLCULO DE ÃNDICES-PADRÃƒO POR PORTE\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ðŸ“Š CÃLCULO DE ÃNDICES-PADRÃƒO POR PORTE (MÃ‰TODO MATARAZZO)\")\n",
    "print(\"=\" * 80)\n",
    "print(\"\\nðŸ“š Metodologia: Matarazzo (1998)\")\n",
    "\n",
    "from pyspark.sql.functions import (\n",
    "    col, when, percentile_approx, count, lit,\n",
    "    round as spark_round, avg\n",
    ")\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import udf\n",
    "import pandas as pd\n",
    "\n",
    "DATABASE = 'neac'\n",
    "ANO_REFERENCIA = 2024\n",
    "\n",
    "# Faixas de porte\n",
    "PORTE_PEQUENO_MAX = 1_000_000\n",
    "PORTE_MEDIO_MAX = 10_000_000\n",
    "\n",
    "MIN_EMPRESAS_DECIS = 30\n",
    "\n",
    "print(f\"\\nâš™ï¸  ConfiguraÃ§Ãµes:\")\n",
    "print(f\"   - Pequeno: atÃ© R$ {PORTE_PEQUENO_MAX:,.0f}\")\n",
    "print(f\"   - MÃ©dio: atÃ© R$ {PORTE_MEDIO_MAX:,.0f}\")\n",
    "print(f\"   - Grande: acima de R$ {PORTE_MEDIO_MAX:,.0f}\")\n",
    "print(f\"   - MÃ­nimo para decis: {MIN_EMPRESAS_DECIS}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CARREGAR\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“¥ Carregando...\")\n",
    "\n",
    "df = spark.table(f\"{DATABASE}.ecd_ml_empresas_aptas_indicadores\")\n",
    "total_empresas = df.count()\n",
    "\n",
    "print(f\"âœ… {total_empresas:,} empresas\")\n",
    "\n",
    "if total_empresas == 0:\n",
    "    print(\"\\nâš ï¸  Execute Script 2 primeiro!\")\n",
    "    raise SystemExit\n",
    "\n",
    "# ================================================================================\n",
    "# CLASSIFICAR POR PORTE\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ¢ Classificando por porte...\")\n",
    "\n",
    "df_com_porte = df.withColumn(\n",
    "    'porte',\n",
    "    when(col('ativo') <= PORTE_PEQUENO_MAX, 'PEQUENO')\n",
    "    .when(col('ativo') <= PORTE_MEDIO_MAX, 'MEDIO')\n",
    "    .otherwise('GRANDE')\n",
    ")\n",
    "\n",
    "print(f\"\\nðŸ“Š DistribuiÃ§Ã£o:\")\n",
    "dist_porte = df_com_porte.groupBy('porte').count().orderBy('porte')\n",
    "dist_porte.show()\n",
    "\n",
    "dist_dict = {row['porte']: row['count'] for row in dist_porte.collect()}\n",
    "portes_validos = [porte for porte, qtd in dist_dict.items() if qtd >= MIN_EMPRESAS_DECIS]\n",
    "\n",
    "if len(portes_validos) == 0:\n",
    "    print(f\"\\nâš ï¸  Nenhum porte tem {MIN_EMPRESAS_DECIS}+ empresas!\")\n",
    "    raise SystemExit\n",
    "\n",
    "print(f\"\\nâœ… Portes vÃ¡lidos: {portes_validos}\")\n",
    "\n",
    "# ================================================================================\n",
    "# INDICADORES\n",
    "# ================================================================================\n",
    "\n",
    "indicadores_config = {\n",
    "    'liquidez_corrente': True,\n",
    "    'liquidez_geral': True,\n",
    "    'endividamento_geral': False,\n",
    "    'participacao_capital_terceiros': False,\n",
    "    'composicao_endividamento': False,\n",
    "    'margem_liquida': True,\n",
    "    'rentabilidade_ativo': True,\n",
    "    'rentabilidade_pl': True,\n",
    "    'giro_ativo': True\n",
    "}\n",
    "\n",
    "indicadores = list(indicadores_config.keys())\n",
    "\n",
    "print(f\"\\nðŸ“‹ Indicadores: {len(indicadores)}\")\n",
    "for ind, maior_melhor in indicadores_config.items():\n",
    "    sentido = \"â†‘\" if maior_melhor else \"â†“\"\n",
    "    print(f\"   {sentido} {ind}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CALCULAR DECIS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“ CALCULANDO DECIS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "decis_por_porte = {}\n",
    "\n",
    "for porte in portes_validos:\n",
    "    print(f\"\\nðŸ¢ {porte}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    df_porte = df_com_porte.filter(col('porte') == porte)\n",
    "    count_porte = df_porte.count()\n",
    "    \n",
    "    print(f\"   Empresas: {count_porte:,}\")\n",
    "    \n",
    "    decis_porte = {}\n",
    "    \n",
    "    for indicador in indicadores:\n",
    "        percentis_list = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]\n",
    "        \n",
    "        result = df_porte.select(\n",
    "            percentile_approx(indicador, percentis_list, 10000).alias('decis')\n",
    "        ).collect()\n",
    "        \n",
    "        if result and result[0]['decis']:\n",
    "            decis_porte[indicador] = result[0]['decis']\n",
    "            print(f\"   âœ… {indicador}\")\n",
    "    \n",
    "    decis_por_porte[porte] = decis_porte\n",
    "\n",
    "print(f\"\\nâœ… Decis calculados para {len(decis_por_porte)} portes\")\n",
    "\n",
    "# ================================================================================\n",
    "# EXIBIR TABELAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“‹ TABELAS DE ÃNDICES-PADRÃƒO\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "for porte in portes_validos:\n",
    "    if porte not in decis_por_porte:\n",
    "        continue\n",
    "        \n",
    "    decis_porte = decis_por_porte[porte]\n",
    "    \n",
    "    print(f\"\\nðŸ¢ PORTE: {porte}\")\n",
    "    print(\"-\" * 120)\n",
    "    print(f\"{'Indicador':<35} {'1Âº':>8} {'2Âº':>8} {'3Âº':>8} {'4Âº':>8} {'5Âº':>8} {'6Âº':>8} {'7Âº':>8} {'8Âº':>8} {'9Âº':>8}\")\n",
    "    print(\"-\" * 120)\n",
    "    \n",
    "    for indicador in indicadores:\n",
    "        if indicador in decis_porte:\n",
    "            decis = decis_porte[indicador]\n",
    "            valores_str = [f\"{v:8.2f}\" if v is not None else \"     N/A\" for v in decis]\n",
    "            print(f\"{indicador:<35} {' '.join(valores_str)}\")\n",
    "\n",
    "# ================================================================================\n",
    "# CLASSIFICAR EMPRESAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸŽ¯ CLASSIFICANDO EMPRESAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "def classificar_valor_nos_decis(valor, decis, quanto_maior_melhor):\n",
    "    if valor is None or decis is None or len(decis) < 9:\n",
    "        return \"NAO_CLASSIFICADO\"\n",
    "    \n",
    "    posicao = 10\n",
    "    for i, limite_decil in enumerate(decis):\n",
    "        if limite_decil is not None and valor <= limite_decil:\n",
    "            posicao = i + 1\n",
    "            break\n",
    "    \n",
    "    if quanto_maior_melhor:\n",
    "        if posicao <= 2:\n",
    "            return \"PESSIMO\"\n",
    "        elif posicao <= 3:\n",
    "            return \"DEFICIENTE\"\n",
    "        elif posicao == 4:\n",
    "            return \"FRACO\"\n",
    "        elif posicao == 5:\n",
    "            return \"RAZOAVEL\"\n",
    "        elif posicao in [6, 7]:\n",
    "            return \"SATISFATORIO\"\n",
    "        elif posicao == 8:\n",
    "            return \"BOM\"\n",
    "        else:\n",
    "            return \"OTIMO\"\n",
    "    else:\n",
    "        if posicao >= 9:\n",
    "            return \"PESSIMO\"\n",
    "        elif posicao == 8:\n",
    "            return \"DEFICIENTE\"\n",
    "        elif posicao == 7:\n",
    "            return \"FRACO\"\n",
    "        elif posicao == 6:\n",
    "            return \"RAZOAVEL\"\n",
    "        elif posicao in [4, 5]:\n",
    "            return \"SATISFATORIO\"\n",
    "        elif posicao == 3:\n",
    "            return \"BOM\"\n",
    "        else:\n",
    "            return \"OTIMO\"\n",
    "\n",
    "dfs_classificados = []\n",
    "\n",
    "for porte in portes_validos:\n",
    "    if porte not in decis_por_porte:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Classificando: {porte}\")\n",
    "    \n",
    "    df_porte = df_com_porte.filter(col('porte') == porte)\n",
    "    decis_porte = decis_por_porte[porte]\n",
    "    \n",
    "    for indicador in indicadores:\n",
    "        if indicador not in decis_porte:\n",
    "            continue\n",
    "        \n",
    "        decis = decis_porte[indicador]\n",
    "        quanto_maior_melhor = indicadores_config[indicador]\n",
    "        \n",
    "        @udf(returnType=StringType())\n",
    "        def classificar_udf(valor):\n",
    "            if valor is None:\n",
    "                return \"NAO_CLASSIFICADO\"\n",
    "            return classificar_valor_nos_decis(float(valor), decis, quanto_maior_melhor)\n",
    "        \n",
    "        nome_coluna_conceito = f'{indicador}_conceito'\n",
    "        df_porte = df_porte.withColumn(nome_coluna_conceito, classificar_udf(col(indicador)))\n",
    "    \n",
    "    dfs_classificados.append(df_porte)\n",
    "\n",
    "if len(dfs_classificados) > 0:\n",
    "    df_final_classificado = dfs_classificados[0]\n",
    "    for df_adicional in dfs_classificados[1:]:\n",
    "        df_final_classificado = df_final_classificado.union(df_adicional)\n",
    "    \n",
    "    print(f\"\\nâœ… Total classificadas: {df_final_classificado.count():,}\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SALVAR\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Salvando...\")\n",
    "    \n",
    "    df_final_classificado.createOrReplaceTempView(\"temp_empresas_classificadas\")\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_empresas_indices_padrao\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {DATABASE}.ecd_ml_empresas_indices_padrao\n",
    "        STORED AS PARQUET\n",
    "        AS SELECT * FROM temp_empresas_classificadas\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"âœ… Salvo: {DATABASE}.ecd_ml_empresas_indices_padrao\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # EXEMPLOS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nðŸ“Š EXEMPLOS\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for porte in portes_validos[:1]:\n",
    "        print(f\"\\nðŸ¢ PORTE: {porte}\")\n",
    "        \n",
    "        df_exemplo = df_final_classificado.filter(col('porte') == porte)\n",
    "        \n",
    "        colunas_exemplo = ['cnpj', 'ativo']\n",
    "        for ind in ['liquidez_corrente', 'rentabilidade_pl', 'endividamento_geral']:\n",
    "            if f'{ind}_conceito' in df_exemplo.columns:\n",
    "                colunas_exemplo.extend([ind, f'{ind}_conceito'])\n",
    "        \n",
    "        df_exemplo.select(colunas_exemplo).limit(10).show(truncate=False)\n",
    "    \n",
    "    # ============================================================================\n",
    "    # SALVAR DECIS\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\nðŸ’¾ Salvando tabela de decis...\")\n",
    "    \n",
    "    decis_data = []\n",
    "    for porte, decis_porte in decis_por_porte.items():\n",
    "        for indicador, decis in decis_porte.items():\n",
    "            decis_data.append({\n",
    "                'porte': porte,\n",
    "                'indicador': indicador,\n",
    "                'decil_1': decis[0] if len(decis) > 0 else None,\n",
    "                'decil_2': decis[1] if len(decis) > 1 else None,\n",
    "                'decil_3': decis[2] if len(decis) > 2 else None,\n",
    "                'decil_4': decis[3] if len(decis) > 3 else None,\n",
    "                'decil_5': decis[4] if len(decis) > 4 else None,\n",
    "                'decil_6': decis[5] if len(decis) > 5 else None,\n",
    "                'decil_7': decis[6] if len(decis) > 6 else None,\n",
    "                'decil_8': decis[7] if len(decis) > 7 else None,\n",
    "                'decil_9': decis[8] if len(decis) > 8 else None\n",
    "            })\n",
    "    \n",
    "    df_decis_pandas = pd.DataFrame(decis_data)\n",
    "    df_decis_spark = spark.createDataFrame(df_decis_pandas)\n",
    "    \n",
    "    df_decis_spark.createOrReplaceTempView(\"temp_decis\")\n",
    "    \n",
    "    spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_indices_padrao_decis\")\n",
    "    \n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE TABLE {DATABASE}.ecd_ml_indices_padrao_decis\n",
    "        STORED AS PARQUET\n",
    "        AS SELECT * FROM temp_decis\n",
    "    \"\"\")\n",
    "    \n",
    "    print(f\"âœ… Decis salvos: {DATABASE}.ecd_ml_indices_padrao_decis\")\n",
    "    \n",
    "    # ============================================================================\n",
    "    # RESUMO\n",
    "    # ============================================================================\n",
    "    \n",
    "    print(f\"\\n\" + \"=\" * 80)\n",
    "    print(\"ðŸ“Š RESUMO FINAL - SCRIPT 3\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    print(f\"\\nâœ… Empresas classificadas: {df_final_classificado.count():,}\")\n",
    "    print(f\"âœ… Portes analisados: {len(portes_validos)}\")\n",
    "    print(f\"âœ… Indicadores: {len(indicadores)}\")\n",
    "    \n",
    "    print(f\"\\nðŸ“‹ Tabelas:\")\n",
    "    print(f\"   1. {DATABASE}.ecd_ml_empresas_indices_padrao\")\n",
    "    print(f\"   2. {DATABASE}.ecd_ml_indices_padrao_decis\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… SCRIPT 3 CONCLUÃDO!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c910f-65b2-4584-9ad1-c2661bb3c309",
   "metadata": {},
   "source": [
    "### COBERTURA DO MODELO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bd2c8cfd-6409-4e76-89d0-d1da565d28b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” DIAGNÃ“STICO: COBERTURA DAS CLASSIFICAÃ‡Ã•ES ML\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š 1. COMPARANDO TOTAL DE CONTAS\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de registros no ECD (BP):        144,602,899\n",
      "Total de contas ÃšNICAS (id_ecd+conta): 80,600,825\n",
      "Total de prediÃ§Ãµes do modelo:          144,602,905\n",
      "DIFERENÃ‡A (contas sem prediÃ§Ã£o):       -64,002,080\n",
      "COBERTURA:                             179.41%\n",
      "\n",
      "ðŸ“Š 2. ANALISANDO CONTAS NÃƒO CLASSIFICADAS\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contas SEM classificaÃ§Ã£o ML: 0\n",
      "\n",
      "ðŸ“Š 3. INVESTIGANDO CAUSA DA BAIXA COBERTURA\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empresas no ECD (BP):           384,493\n",
      "Empresas com prediÃ§Ãµes:         384,493\n",
      "Cobertura de empresas:          100.00%\n",
      "\n",
      "ðŸ“Š CaracterÃ­sticas das contas CLASSIFICADAS:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+--------+--------------------+\n",
      "|    total|empresas|tamanho_medio_codigo|\n",
      "+---------+--------+--------------------+\n",
      "|144602905|  384493|   9.999866786908603|\n",
      "+---------+--------+--------------------+\n",
      "\n",
      "\n",
      "ðŸ“Š CaracterÃ­sticas das contas NÃƒO CLASSIFICADAS (amostra):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+\n",
      "|total|empresas|\n",
      "+-----+--------+\n",
      "|    0|       0|\n",
      "+-----+--------+\n",
      "\n",
      "\n",
      "ðŸ“Š 4. VERIFICANDO SE HÃ FILTROS APLICADOS\n",
      "--------------------------------------------------------------------------------\n",
      "Verificando prediÃ§Ãµes:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+------+---------------+\n",
      "|empresas| cnpjs|total_predicoes|\n",
      "+--------+------+---------------+\n",
      "|  384493|384493|      144602905|\n",
      "+--------+------+---------------+\n",
      "\n",
      "Verificando dados ECD:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3040:====================================================> (28 + 1) / 29]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------------+---------------+\n",
      "|empresas|contas_diferentes|total_registros|\n",
      "+--------+-----------------+---------------+\n",
      "|  384493|         11382673|      144602899|\n",
      "+--------+-----------------+---------------+\n",
      "\n",
      "\n",
      "ðŸ“Š 5. VERIFICANDO SE PREDIÃ‡Ã•ES SÃƒO DE AMOSTRA\n",
      "--------------------------------------------------------------------------------\n",
      "Total no dataset ML: 1,644,053\n",
      "Total no conjunto de TESTE: 246,359\n",
      "\n",
      "âš ï¸  DESCOBERTA:\n",
      "   O modelo foi aplicado APENAS no conjunto de TESTE!\n",
      "   Isso representa 0.31% do total\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# DIAGNÃ“STICO COMPLETO - VERSÃƒO CORRIGIDA\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ðŸ” DIAGNÃ“STICO: COBERTURA DAS CLASSIFICAÃ‡Ã•ES ML\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import col, count, sum as spark_sum, when, abs as spark_abs\n",
    "\n",
    "DATABASE = 'neac'\n",
    "DATABASE_ECD = 'usr_sat_ecd'\n",
    "ANO_REFERENCIA = 2024\n",
    "\n",
    "# ================================================================================\n",
    "# 1. COMPARAR TOTAL DE CONTAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š 1. COMPARANDO TOTAL DE CONTAS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Total de contas no ECD (BalanÃ§o)\n",
    "total_ecd_bp = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total\n",
    "    FROM {DATABASE_ECD}.ecd_rj100_balanco_patrimonial bp\n",
    "\"\"\").collect()[0]['total']\n",
    "\n",
    "# Total de prediÃ§Ãµes\n",
    "total_pred = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(*) as total\n",
    "    FROM {DATABASE}.ecd_ml_predictions_ALL\n",
    "\"\"\").collect()[0]['total']\n",
    "\n",
    "# Total ÃšNICO\n",
    "total_ecd_unicas = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT bp.id_ecd, bp.cod_agl) as total\n",
    "    FROM {DATABASE_ECD}.ecd_rj100_balanco_patrimonial bp\n",
    "\"\"\").collect()[0]['total']\n",
    "\n",
    "print(f\"Total de registros no ECD (BP):        {total_ecd_bp:,}\")\n",
    "print(f\"Total de contas ÃšNICAS (id_ecd+conta): {total_ecd_unicas:,}\")\n",
    "print(f\"Total de prediÃ§Ãµes do modelo:          {total_pred:,}\")\n",
    "print(f\"DIFERENÃ‡A (contas sem prediÃ§Ã£o):       {total_ecd_unicas - total_pred:,}\")\n",
    "print(f\"COBERTURA:                             {total_pred/total_ecd_unicas*100:.2f}%\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. ANALISAR CONTAS NÃƒO CLASSIFICADAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š 2. ANALISANDO CONTAS NÃƒO CLASSIFICADAS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "df_nao_classificadas = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        bp.id_ecd,\n",
    "        bp.cod_agl as cd_conta,\n",
    "        bp.descr_cod_agl as descr_conta,\n",
    "        bp.vl_cta_fin as vl_conta_final,\n",
    "        bp.ind_dc_cta_fin as ind_dc_final,\n",
    "        bp.ind_grp_bal,\n",
    "        pc.cd_natureza,\n",
    "        pc.tp_conta\n",
    "    FROM {DATABASE_ECD}.ecd_rj100_balanco_patrimonial bp\n",
    "    INNER JOIN {DATABASE_ECD}.ecd_ri050_plano_contas pc\n",
    "        ON bp.id_ecd = pc.id_ecd\n",
    "        AND bp.cod_agl = pc.cd_conta_anl\n",
    "    LEFT JOIN {DATABASE}.ecd_ml_predictions_ALL pred\n",
    "        ON bp.id_ecd = pred.id_ecd\n",
    "        AND bp.cod_agl = pred.cd_conta\n",
    "    WHERE pred.id_ecd IS NULL\n",
    "\"\"\")\n",
    "\n",
    "total_nao_class = df_nao_classificadas.count()\n",
    "print(f\"Contas SEM classificaÃ§Ã£o ML: {total_nao_class:,}\")\n",
    "\n",
    "if total_nao_class > 0:\n",
    "    # Calcular valor total - CORRIGIDO\n",
    "    df_nao_class_com_valor = df_nao_classificadas.withColumn(\n",
    "        'vl_ajustado',\n",
    "        when(col('ind_dc_final') == 'D', col('vl_conta_final'))\n",
    "        .when(col('ind_dc_final') == 'C', -col('vl_conta_final'))\n",
    "        .otherwise(col('vl_conta_final'))\n",
    "    )\n",
    "    \n",
    "    # CORREÃ‡ÃƒO: usar Python abs(), nÃ£o spark abs()\n",
    "    valor_total_resultado = df_nao_class_com_valor.select(\n",
    "        spark_sum(spark_abs(col('vl_ajustado'))).alias('total')\n",
    "    ).collect()[0]\n",
    "    \n",
    "    valor_total_nao_class = valor_total_resultado['total'] if valor_total_resultado['total'] else 0\n",
    "    \n",
    "    print(f\"Valor total (abs) nÃ£o classificado: R$ {float(valor_total_nao_class):,.2f}\")\n",
    "    \n",
    "    # Amostra\n",
    "    print(f\"\\nðŸ“‹ Amostra de contas NÃƒO classificadas:\")\n",
    "    df_nao_classificadas.select(\n",
    "        'cd_conta', 'descr_conta', 'vl_conta_final', 'ind_grp_bal', 'cd_natureza', 'tp_conta'\n",
    "    ).limit(20).show(truncate=False)\n",
    "    \n",
    "    # DistribuiÃ§Ã£o por tipo\n",
    "    print(f\"\\nðŸ“Š DistribuiÃ§Ã£o por tipo de conta:\")\n",
    "    df_nao_classificadas.groupBy('tp_conta').count().orderBy(col('count').desc()).show()\n",
    "    \n",
    "    # DistribuiÃ§Ã£o por grupo\n",
    "    print(f\"\\nðŸ“Š DistribuiÃ§Ã£o por grupo do balanÃ§o:\")\n",
    "    df_nao_classificadas.groupBy('ind_grp_bal').count().show()\n",
    "\n",
    "# ================================================================================\n",
    "# 3. INVESTIGAR POR QUE TÃƒO POUCAS CONTAS FORAM CLASSIFICADAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š 3. INVESTIGANDO CAUSA DA BAIXA COBERTURA\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Ver quais id_ecd foram classificados\n",
    "empresas_classificadas = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT id_ecd) as total\n",
    "    FROM {DATABASE}.ecd_ml_predictions_ALL\n",
    "\"\"\").collect()[0]['total']\n",
    "\n",
    "empresas_total_ecd = spark.sql(f\"\"\"\n",
    "    SELECT COUNT(DISTINCT id_ecd) as total\n",
    "    FROM {DATABASE_ECD}.ecd_rj100_balanco_patrimonial\n",
    "\"\"\").collect()[0]['total']\n",
    "\n",
    "print(f\"Empresas no ECD (BP):           {empresas_total_ecd:,}\")\n",
    "print(f\"Empresas com prediÃ§Ãµes:         {empresas_classificadas:,}\")\n",
    "print(f\"Cobertura de empresas:          {empresas_classificadas/empresas_total_ecd*100:.2f}%\")\n",
    "\n",
    "# Ver caracterÃ­sticas das contas classificadas vs nÃ£o classificadas\n",
    "print(f\"\\nðŸ“Š CaracterÃ­sticas das contas CLASSIFICADAS:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(*) as total,\n",
    "        COUNT(DISTINCT id_ecd) as empresas,\n",
    "        AVG(LENGTH(cd_conta)) as tamanho_medio_codigo\n",
    "    FROM {DATABASE}.ecd_ml_predictions_ALL\n",
    "\"\"\").show()\n",
    "\n",
    "print(f\"\\nðŸ“Š CaracterÃ­sticas das contas NÃƒO CLASSIFICADAS (amostra):\")\n",
    "df_nao_classificadas.select(\n",
    "    count('*').alias('total'),\n",
    "    count(col('id_ecd').isNotNull()).alias('empresas')\n",
    ").show()\n",
    "\n",
    "# ================================================================================\n",
    "# 4. COMPARAR FILTROS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š 4. VERIFICANDO SE HÃ FILTROS APLICADOS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Verificar se o modelo foi treinado apenas em SC 2024\n",
    "print(f\"Verificando prediÃ§Ãµes:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT id_ecd) as empresas,\n",
    "        COUNT(DISTINCT cnpj) as cnpjs,\n",
    "        COUNT(*) as total_predicoes\n",
    "    FROM {DATABASE}.ecd_ml_predictions_ALL\n",
    "\"\"\").show()\n",
    "\n",
    "# Verificar dados ECD\n",
    "print(f\"Verificando dados ECD:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        COUNT(DISTINCT id_ecd) as empresas,\n",
    "        COUNT(DISTINCT cod_agl) as contas_diferentes,\n",
    "        COUNT(*) as total_registros\n",
    "    FROM {DATABASE_ECD}.ecd_rj100_balanco_patrimonial\n",
    "\"\"\").show()\n",
    "\n",
    "# ================================================================================\n",
    "# 5. VERIFICAR SE PREDIÃ‡Ã•ES SÃƒO APENAS DE UMA AMOSTRA\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š 5. VERIFICANDO SE PREDIÃ‡Ã•ES SÃƒO DE AMOSTRA\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Ver se hÃ¡ tabela de treino/teste\n",
    "try:\n",
    "    total_dataset = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as total\n",
    "        FROM {DATABASE}.ecd_ml_dataset\n",
    "    \"\"\").collect()[0]['total']\n",
    "    \n",
    "    print(f\"Total no dataset ML: {total_dataset:,}\")\n",
    "    \n",
    "    total_test = spark.sql(f\"\"\"\n",
    "        SELECT COUNT(*) as total\n",
    "        FROM {DATABASE}.ecd_ml_test\n",
    "    \"\"\").collect()[0]['total']\n",
    "    \n",
    "    print(f\"Total no conjunto de TESTE: {total_test:,}\")\n",
    "    \n",
    "    print(f\"\\nâš ï¸  DESCOBERTA:\")\n",
    "    print(f\"   O modelo foi aplicado APENAS no conjunto de TESTE!\")\n",
    "    print(f\"   Isso representa {total_test/total_ecd_unicas*100:.2f}% do total\")\n",
    "    \n",
    "except:\n",
    "    print(\"NÃ£o encontrou tabela de dataset/test\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0aa4255-48f3-4893-85d2-499171919032",
   "metadata": {},
   "source": [
    "### CLASSIFICAR CONTAS SINTÃ‰TICAS POR HERANÃ‡A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b178603c-2b7a-41f3-9a63-dc4cd47987d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ CLASSIFICANDO CONTAS SINTÃ‰TICAS POR HERANÃ‡A\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š 1. IDENTIFICANDO CONTAS SINTÃ‰TICAS\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contas SINTÃ‰TICAS: 51,198,730\n",
      "Contas ANALÃTICAS: 457,254,458\n",
      "\n",
      "ðŸ”§ 2. CLASSIFICANDO SINTÃ‰TICAS POR HERANÃ‡A\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… SintÃ©ticas classificadas por heranÃ§a: 8,401,167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3126:=================================================>    (12 + 1) / 13]0]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Salvo em: neac.ecd_ml_sinteticas_por_heranca\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# SOLUÃ‡ÃƒO: CLASSIFICAR CONTAS SINTÃ‰TICAS POR HERANÃ‡A\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ðŸ”§ CLASSIFICANDO CONTAS SINTÃ‰TICAS POR HERANÃ‡A\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import col, when, first, collect_list\n",
    "\n",
    "DATABASE = 'neac'\n",
    "DATABASE_ECD = 'usr_sat_ecd'\n",
    "\n",
    "# ================================================================================\n",
    "# 1. IDENTIFICAR CONTAS SINTÃ‰TICAS vs ANALÃTICAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š 1. IDENTIFICANDO CONTAS SINTÃ‰TICAS\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Carregar plano de contas\n",
    "df_plano = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        id_ecd,\n",
    "        cd_conta_anl,\n",
    "        cd_conta_sint,\n",
    "        nm_conta_anl,\n",
    "        tp_conta,\n",
    "        cd_natureza,\n",
    "        nivel\n",
    "    FROM {DATABASE_ECD}.ecd_ri050_plano_contas\n",
    "\"\"\")\n",
    "\n",
    "# Contas sintÃ©ticas sÃ£o aquelas que tÃªm cd_conta_sint preenchido\n",
    "# E geralmente tp_conta = 'S'\n",
    "df_sinteticas = df_plano.filter(col('tp_conta') == 'S')\n",
    "df_analiticas = df_plano.filter(col('tp_conta') == 'A')\n",
    "\n",
    "total_sint = df_sinteticas.count()\n",
    "total_anal = df_analiticas.count()\n",
    "\n",
    "print(f\"Contas SINTÃ‰TICAS: {total_sint:,}\")\n",
    "print(f\"Contas ANALÃTICAS: {total_anal:,}\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. CLASSIFICAR SINTÃ‰TICAS PELA MAIORIA DAS FILHAS\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ”§ 2. CLASSIFICANDO SINTÃ‰TICAS POR HERANÃ‡A\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Carregar prediÃ§Ãµes existentes\n",
    "df_pred = spark.table(f\"{DATABASE}.ecd_ml_predictions_ALL\")\n",
    "\n",
    "# Join: plano de contas + prediÃ§Ãµes\n",
    "df_plano_com_pred = df_plano.alias('pc').join(\n",
    "    df_pred.alias('pred'),\n",
    "    (col('pc.id_ecd') == col('pred.id_ecd')) & \n",
    "    (col('pc.cd_conta_anl') == col('pred.cd_conta')),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Para cada conta sintÃ©tica, pegar classificaÃ§Ã£o das filhas\n",
    "classificacao_sinteticas = spark.sql(f\"\"\"\n",
    "    WITH filhas_classificadas AS (\n",
    "        SELECT \n",
    "            pc_sint.id_ecd,\n",
    "            pc_sint.cd_conta_anl as conta_sintetica,\n",
    "            pred.classificacao_nivel2,\n",
    "            COUNT(*) as qtd\n",
    "        FROM {DATABASE_ECD}.ecd_ri050_plano_contas pc_anal\n",
    "        INNER JOIN {DATABASE_ECD}.ecd_ri050_plano_contas pc_sint\n",
    "            ON pc_anal.id_ecd = pc_sint.id_ecd\n",
    "            AND pc_anal.cd_conta_sint = pc_sint.cd_conta_anl\n",
    "        INNER JOIN {DATABASE}.ecd_ml_predictions_ALL pred\n",
    "            ON pc_anal.id_ecd = pred.id_ecd\n",
    "            AND pc_anal.cd_conta_anl = pred.cd_conta\n",
    "        WHERE pc_sint.tp_conta = 'S'\n",
    "        GROUP BY pc_sint.id_ecd, pc_sint.cd_conta_anl, pred.classificacao_nivel2\n",
    "    ),\n",
    "    classificacao_majoritaria AS (\n",
    "        SELECT \n",
    "            id_ecd,\n",
    "            conta_sintetica,\n",
    "            FIRST(classificacao_nivel2) as classificacao_herdada,\n",
    "            SUM(qtd) as total_filhas\n",
    "        FROM filhas_classificadas\n",
    "        GROUP BY id_ecd, conta_sintetica\n",
    "        ORDER BY SUM(qtd) DESC\n",
    "    )\n",
    "    SELECT * FROM classificacao_majoritaria\n",
    "\"\"\")\n",
    "\n",
    "total_sint_classificadas = classificacao_sinteticas.count()\n",
    "print(f\"âœ… SintÃ©ticas classificadas por heranÃ§a: {total_sint_classificadas:,}\")\n",
    "\n",
    "# Salvar\n",
    "classificacao_sinteticas.createOrReplaceTempView(\"temp_sinteticas_classificadas\")\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_sinteticas_por_heranca\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {DATABASE}.ecd_ml_sinteticas_por_heranca\n",
    "    STORED AS PARQUET\n",
    "    AS SELECT * FROM temp_sinteticas_classificadas\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Salvo em: {DATABASE}.ecd_ml_sinteticas_por_heranca\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5d4341-e32e-42d6-916c-ea4cba6e5e0f",
   "metadata": {},
   "source": [
    "### CLASSIFICAR CONTAS POR FALLBACK POR NATUREZA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "36cfd628-f074-4def-b93c-00d25789cc73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ CLASSIFICAÃ‡ÃƒO FALLBACK PARA CONTAS NÃƒO CLASSIFICADAS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š Aplicando classificaÃ§Ã£o fallback por cd_natureza...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Contas classificadas por fallback: 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3145:===================================================>(199 + 1) / 200]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Salvo em: neac.ecd_ml_fallback_classificacoes\n",
      "\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# SOLUÃ‡ÃƒO: CLASSIFICAÃ‡ÃƒO FALLBACK POR cd_natureza\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ðŸ”§ CLASSIFICAÃ‡ÃƒO FALLBACK PARA CONTAS NÃƒO CLASSIFICADAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import col, when, substring\n",
    "\n",
    "DATABASE = 'neac'\n",
    "DATABASE_ECD = 'usr_sat_ecd'\n",
    "\n",
    "# ================================================================================\n",
    "# MAPEAR cd_natureza â†’ ClassificaÃ§Ã£o\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š Aplicando classificaÃ§Ã£o fallback por cd_natureza...\")\n",
    "\n",
    "# Carregar contas nÃ£o classificadas\n",
    "df_nao_class = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        bp.id_ecd,\n",
    "        bp.cod_agl as cd_conta,\n",
    "        bp.descr_cod_agl as descr_conta,\n",
    "        bp.vl_cta_fin,\n",
    "        bp.ind_dc_cta_fin,\n",
    "        bp.ind_grp_bal,\n",
    "        pc.cd_natureza,\n",
    "        pc.tp_conta\n",
    "    FROM {DATABASE_ECD}.ecd_rj100_balanco_patrimonial bp\n",
    "    INNER JOIN {DATABASE_ECD}.ecd_ri050_plano_contas pc\n",
    "        ON bp.id_ecd = pc.id_ecd\n",
    "        AND bp.cod_agl = pc.cd_conta_anl\n",
    "    LEFT JOIN {DATABASE}.ecd_ml_predictions_ALL pred\n",
    "        ON bp.id_ecd = pred.id_ecd\n",
    "        AND bp.cod_agl = pred.cd_conta\n",
    "    WHERE pred.id_ecd IS NULL\n",
    "\"\"\")\n",
    "\n",
    "# Aplicar regras de fallback baseadas em cd_natureza\n",
    "df_fallback = df_nao_class.withColumn(\n",
    "    'classificacao_fallback',\n",
    "    # ATIVO\n",
    "    when(col('cd_natureza').startswith('1.01'), 'ATIVO_CIRCULANTE')\n",
    "    .when(col('cd_natureza').startswith('1.02'), 'ATIVO_NAO_CIRCULANTE')\n",
    "    .when(col('cd_natureza').startswith('1.'), 'ATIVO')\n",
    "    # PASSIVO\n",
    "    .when(col('cd_natureza').startswith('2.01'), 'PASSIVO_CIRCULANTE')\n",
    "    .when(col('cd_natureza').startswith('2.02'), 'PASSIVO_NAO_CIRCULANTE')\n",
    "    # PATRIMÃ”NIO LÃQUIDO\n",
    "    .when(col('cd_natureza').startswith('2.03'), 'PATRIMONIO_LIQUIDO')\n",
    "    .when(col('cd_natureza').startswith('2.3'), 'PATRIMONIO_LIQUIDO')\n",
    "    # Passivo genÃ©rico\n",
    "    .when(col('cd_natureza').startswith('2.'), 'PASSIVO')\n",
    "    # Baseado em ind_grp_bal se cd_natureza nÃ£o for claro\n",
    "    .when((col('ind_grp_bal') == 'A'), 'ATIVO')\n",
    "    .when((col('ind_grp_bal') == 'P'), 'PASSIVO')\n",
    "    .otherwise('NAO_CLASSIFICADO')\n",
    ")\n",
    "\n",
    "total_fallback = df_fallback.filter(col('classificacao_fallback') != 'NAO_CLASSIFICADO').count()\n",
    "print(f\"âœ… Contas classificadas por fallback: {total_fallback:,}\")\n",
    "\n",
    "# Salvar\n",
    "df_fallback.createOrReplaceTempView(\"temp_fallback\")\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {DATABASE}.ecd_ml_fallback_classificacoes\")\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE {DATABASE}.ecd_ml_fallback_classificacoes\n",
    "    STORED AS PARQUET\n",
    "    AS SELECT * FROM temp_fallback\n",
    "\"\"\")\n",
    "\n",
    "print(f\"âœ… Salvo em: {DATABASE}.ecd_ml_fallback_classificacoes\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be01eed3-f74d-49be-b09e-f33249b9decd",
   "metadata": {},
   "source": [
    "### RECALCULAR EQUAÃ‡Ã•ES COM TODAS AS CLASSIFICAÃ‡Ã•ES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "5b03bf66-9e53-46e7-8475-c8c3e77eb81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ RECALCULANDO EQUAÃ‡ÃƒO CONTÃBIL COM CLASSIFICAÃ‡Ã•ES EXPANDIDAS\n",
      "================================================================================\n",
      "\n",
      "ðŸ“Š 1. CONSOLIDANDO CLASSIFICAÃ‡Ã•ES\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3166:====================================================> (65 + 2) / 67]]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cobertura ANTES: 0.05%\n",
      "Cobertura DEPOIS: 100.00%\n",
      "Melhoria: +99.95 pontos percentuais\n",
      "\n",
      "âœ… Execute o Script 1 novamente com as classificaÃ§Ãµes expandidas!\n",
      "================================================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# SOLUÃ‡ÃƒO: RECALCULAR EQUAÃ‡ÃƒO CONTÃBIL COM CLASSIFICAÃ‡Ã•ES COMPLETAS\n",
    "# ================================================================================\n",
    "\n",
    "print(\"ðŸ”§ RECALCULANDO EQUAÃ‡ÃƒO CONTÃBIL COM CLASSIFICAÃ‡Ã•ES EXPANDIDAS\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "from pyspark.sql.functions import col, when, coalesce, sum as spark_sum, abs as spark_abs\n",
    "\n",
    "DATABASE = 'neac'\n",
    "DATABASE_ECD = 'usr_sat_ecd'\n",
    "\n",
    "# ================================================================================\n",
    "# 1. CONSOLIDAR TODAS AS CLASSIFICAÃ‡Ã•ES\n",
    "# ================================================================================\n",
    "\n",
    "print(f\"\\nðŸ“Š 1. CONSOLIDANDO CLASSIFICAÃ‡Ã•ES\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# UniÃ£o de todas as fontes de classificaÃ§Ã£o\n",
    "df_class_completa = spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        bp.id_ecd,\n",
    "        bp.cod_agl as cd_conta,\n",
    "        bp.vl_cta_fin,\n",
    "        bp.ind_dc_cta_fin,\n",
    "        COALESCE(\n",
    "            pred.classificacao_nivel2,      -- 1Âª prioridade: ML\n",
    "            sint.classificacao_herdada,     -- 2Âª prioridade: HeranÃ§a\n",
    "            fb.classificacao_fallback       -- 3Âª prioridade: Fallback\n",
    "        ) as classificacao_final\n",
    "    FROM {DATABASE_ECD}.ecd_rj100_balanco_patrimonial bp\n",
    "    LEFT JOIN {DATABASE}.ecd_ml_predictions_ALL pred\n",
    "        ON bp.id_ecd = pred.id_ecd AND bp.cod_agl = pred.cd_conta\n",
    "    LEFT JOIN {DATABASE}.ecd_ml_sinteticas_por_heranca sint\n",
    "        ON bp.id_ecd = sint.id_ecd AND bp.cod_agl = sint.conta_sintetica\n",
    "    LEFT JOIN {DATABASE}.ecd_ml_fallback_classificacoes fb\n",
    "        ON bp.id_ecd = fb.id_ecd AND bp.cod_agl = fb.cd_conta\n",
    "\"\"\")\n",
    "\n",
    "# Contar cobertura\n",
    "cobertura = df_class_completa.filter(col('classificacao_final').isNotNull()).count()\n",
    "total = df_class_completa.count()\n",
    "\n",
    "print(f\"Cobertura ANTES: {246359/total*100:.2f}%\")\n",
    "print(f\"Cobertura DEPOIS: {cobertura/total*100:.2f}%\")\n",
    "print(f\"Melhoria: +{(cobertura-246359)/total*100:.2f} pontos percentuais\")\n",
    "\n",
    "# ================================================================================\n",
    "# 2. RECALCULAR BALANÃ‡OCOM CLASSIFICAÃ‡ÃƒO COMPLETA\n",
    "# ================================================================================\n",
    "\n",
    "# Aplicar mesmo processo do Script 1, mas com classificaÃ§Ã£o_final\n",
    "# [CÃ“DIGO DO SCRIPT 1 AQUI, usando classificacao_final]\n",
    "\n",
    "print(\"\\nâœ… Execute o Script 1 novamente com as classificaÃ§Ãµes expandidas!\")\n",
    "print(\"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Data Pipeline)",
   "language": "python",
   "name": "conda_data_pipeline"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
