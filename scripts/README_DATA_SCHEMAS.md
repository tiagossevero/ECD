# Scripts para Gera√ß√£o de Data Schemas

Este diret√≥rio cont√©m scripts para gerar automaticamente os data schemas de todas as tabelas do projeto ECD.

## üìÅ Arquivos

### 1. `generate_data_schemas.py` ‚≠ê RECOMENDADO

**Script principal que executa tudo automaticamente.**

- Conecta ao Spark
- Executa `DESCRIBE FORMATTED` em todas as tabelas
- Executa `SELECT * FROM ... LIMIT 10` em todas as tabelas
- Salva resultados em arquivos organizados por categoria

**Sa√≠da:**
```
data-schemas/
‚îú‚îÄ‚îÄ INDEX.md (√≠ndice de todas as tabelas)
‚îú‚îÄ‚îÄ ORIGINAIS_RI/
‚îÇ   ‚îú‚îÄ‚îÄ usr_sat_ecd_ecd_ri050_plano_contas_DESCRIBE.txt
‚îÇ   ‚îú‚îÄ‚îÄ usr_sat_ecd_ecd_ri050_plano_contas_SAMPLE.txt
‚îÇ   ‚îú‚îÄ‚îÄ usr_sat_ecd_ecd_ri050_plano_contas_SAMPLE.csv
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îú‚îÄ‚îÄ PRODUCAO/
‚îÇ   ‚îú‚îÄ‚îÄ teste_ecd_balanco_patrimonial_DESCRIBE.txt
‚îÇ   ‚îú‚îÄ‚îÄ teste_ecd_balanco_patrimonial_SAMPLE.txt
‚îÇ   ‚îú‚îÄ‚îÄ teste_ecd_balanco_patrimonial_SAMPLE.csv
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ ... (outras categorias)
```

### 2. `generate_sql_commands.py`

**Gera apenas os comandos SQL, sem executar.**

√ötil se voc√™ quiser:
- Revisar os comandos antes de executar
- Executar manualmente em outro ambiente
- Customizar quais tabelas processar

**Sa√≠da:**
```
sql-commands/
‚îú‚îÄ‚îÄ INDEX.md (guia de uso)
‚îú‚îÄ‚îÄ ALL_COMMANDS.sql (todos os comandos)
‚îú‚îÄ‚îÄ ORIGINAIS_RI.sql
‚îú‚îÄ‚îÄ PRODUCAO.sql
‚îú‚îÄ‚îÄ ML_DATASET.sql
‚îî‚îÄ‚îÄ ... (um arquivo por categoria)
```

## üöÄ Como Usar

### Op√ß√£o 1: Executar Script Automatizado (Recomendado)

```bash
cd /home/user/ECD
python scripts/generate_data_schemas.py
```

**O que acontece:**
1. ‚úì Conecta ao Spark (perfil `efd_t2`)
2. ‚úì Processa 52 tabelas em 11 categorias
3. ‚úì Gera 156 arquivos (3 por tabela: DESCRIBE, SAMPLE.txt, SAMPLE.csv)
4. ‚úì Cria √≠ndice naveg√°vel
5. ‚úì Exibe relat√≥rio de progresso

**Tempo estimado:** 10-20 minutos (dependendo do tamanho das tabelas)

### Op√ß√£o 2: Gerar Apenas os Comandos SQL

```bash
cd /home/user/ECD
python scripts/generate_sql_commands.py
```

Depois execute manualmente:

```bash
# Todos os comandos
spark-sql -f sql-commands/ALL_COMMANDS.sql > resultados.txt

# Ou por categoria
spark-sql -f sql-commands/PRODUCAO.sql > producao.txt
```

### Op√ß√£o 3: Executar em Notebook Jupyter

Copie o c√≥digo para um notebook:

```python
# Carregar o script
exec(open('/home/user/ECD/scripts/generate_data_schemas.py').read())
```

### Op√ß√£o 4: Executar Apenas Categorias Espec√≠ficas

Edite `generate_data_schemas.py` e comente as categorias que n√£o quer processar:

```python
TABELAS = {
    "PRODUCAO": [  # ‚Üê Apenas esta categoria
        "teste.ecd_contas_classificadas_producao",
        "teste.ecd_balanco_patrimonial",
        "teste.ecd_dre",
        "teste.ecd_indicadores_financeiros",
    ],

    # "ML_DATASET": [  # ‚Üê Comentadas
    #     ...
    # ],
}
```

## üìä Tabelas Processadas

### Resumo por Categoria

| Categoria | Tabelas | Descri√ß√£o |
|-----------|---------|-----------|
| **ORIGINAIS_RI** | 4 | Tabelas de Registro de Informa√ß√µes (RI) |
| **ORIGINAIS_RJ** | 2 | Tabelas de Demonstra√ß√µes (RJ) |
| **ORIGINAIS_PROCESSADAS** | 2 | Tabelas I (saldos consolidados) |
| **PRODUCAO** | 4 | Pipeline principal (BP, DRE, Indicadores) |
| **STREAMLIT** | 4 | Tabelas usadas pela aplica√ß√£o web |
| **ML_DATASET** | 4 | Datasets para Machine Learning |
| **ML_PREDICOES** | 4 | Predi√ß√µes dos modelos ML |
| **ML_METRICAS** | 4 | M√©tricas de performance ML |
| **ML_EMPRESAS** | 4 | An√°lise por empresa (ML) |
| **ML_ANALISE** | 6 | An√°lises avan√ßadas ML |
| **INDICADORES** | 5 | Indicadores financeiros e an√°lises |
| **VALIDACAO** | 9 | Valida√ß√£o e controle de qualidade |
| **TOTAL** | **52** | |

### Tabelas Priorit√°rias (Essenciais)

Se voc√™ tem tempo limitado, comece por estas:

**FASE 1 - Dados Originais (8 tabelas):**
```
usr_sat_ecd.ecd_ri050_plano_contas
usr_sat_ecd.ecd_ri155_detalhe_saldos_periodicos
teste.ecd_i150  ‚Üê PRINCIPAL
teste.ecd_i200
```

**FASE 2 - Produ√ß√£o (4 tabelas):**
```
teste.ecd_contas_classificadas_producao
teste.ecd_balanco_patrimonial  ‚Üê Particionado por ano
teste.ecd_dre  ‚Üê Particionado por ano
teste.ecd_indicadores_financeiros  ‚Üê Particionado por ano
```

**FASE 3 - Aplica√ß√£o (4 tabelas):**
```
teste.ecd_empresas_cadastro
teste.ecd_plano_contas
```

## üîß Configura√ß√£o

### Requisitos

- PySpark configurado
- Acesso ao banco de dados (schemas `usr_sat_ecd` e `teste`)
- Perfil Spark: `efd_t2`
- Bibliotecas: pandas, numpy

### Ajustar Ambiente

Se seus paths forem diferentes, edite no in√≠cio do script:

```python
# Ajustar estes paths
sys.path.append("/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/poc")
sys.path.append("/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/plugins")
sys.path.append("/home/tsevero/notebooks/SAT_BIG_DATA/data-pipeline/batch/dags")
```

### Ajustar Perfil Spark

Se usar outro perfil:

```python
# Trocar 'efd_t2' pelo seu perfil
session = get_session(profile='seu_perfil_aqui')
```

## üìù Formato dos Arquivos Gerados

### DESCRIBE FORMATTED

```
col_name              data_type            comment
----------------------------------------------------
cnpj_empresa          string               NULL
ano                   int                  NULL
trimestre             int                  NULL
...

# Partition Information
# col_name            data_type            comment
ano                   int                  NULL
```

### SAMPLE (TXT)

```
  cnpj_empresa   ano  trimestre  ...
0  12345678000190  2023  4       ...
1  98765432000100  2023  4       ...
...
```

### SAMPLE (CSV)

```csv
cnpj_empresa,ano,trimestre,...
12345678000190,2023,4,...
98765432000100,2023,4,...
```

## ‚ö†Ô∏è Avisos Importantes

### Tabelas Particionadas

Algumas tabelas s√£o particionadas por `ano`:
- `teste.ecd_balanco_patrimonial`
- `teste.ecd_dre`
- `teste.ecd_indicadores_financeiros`

O DESCRIBE FORMATTED mostrar√° as informa√ß√µes de particionamento.

### Tabelas que Podem N√£o Existir

Algumas tabelas podem n√£o existir se os notebooks correspondentes n√£o foram executados:
- Tabelas ML (se modelo n√£o foi treinado)
- Tabelas de valida√ß√£o (se an√°lise n√£o foi feita)

**O script pula automaticamente tabelas inexistentes.**

### Tabelas Grandes

Algumas tabelas podem ter milh√µes de registros:
- `usr_sat_ecd.ecd_ri155_detalhe_saldos_periodicos`
- `teste.ecd_i150`
- `teste.ecd_ml_dataset`

**O LIMIT 10 garante que apenas 10 linhas sejam retornadas.**

## üêõ Troubleshooting

### Erro: "Table not found"

**Causa:** Tabela n√£o existe ainda

**Solu√ß√£o:** Normal para tabelas ML/Valida√ß√£o. Execute os notebooks correspondentes primeiro.

### Erro: "Permission denied"

**Causa:** Sem acesso ao schema

**Solu√ß√£o:** Verifique permiss√µes no Spark/Hive

### Erro: "Java heap space"

**Causa:** Tabela muito grande

**Solu√ß√£o:** O script usa LIMIT 10, mas se ainda der erro:
```python
# Aumentar mem√≥ria do Spark
spark_builder.config("spark.driver.memory", "8g")
```

### Script muito lento

**Causa:** Muitas tabelas ou tabelas grandes

**Solu√ß√£o:** Processe por categoria:
1. Comente categorias desnecess√°rias
2. Execute em paralelo (v√°rias inst√¢ncias do script)

## üìö Refer√™ncias

- **Notebook exemplo:** `/home/user/ECD/notebooks/ECD-Exemplo.ipynb`
- **Pipeline produ√ß√£o:** `/home/user/ECD/main.py`
- **README principal:** `/home/user/ECD/README.md`

## ü§ù Contribuindo

Para adicionar novas tabelas ao script:

1. Edite `TABELAS` em `generate_data_schemas.py`
2. Adicione na categoria apropriada
3. Execute o script
4. Commit os novos data schemas

## üìû Suporte

Em caso de d√∫vidas:
1. Verifique logs do script
2. Teste comando SQL manualmente
3. Consulte documenta√ß√£o do PySpark

---

**√öltima atualiza√ß√£o:** 2025-11-17
